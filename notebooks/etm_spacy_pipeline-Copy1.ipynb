{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93de24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Tuple, List\n",
    "\n",
    "import spacy\n",
    "import multiprocessing\n",
    "#from gensim import \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "# explore the topics discussed in the reviews and compare how they have evolved over time\n",
    "# We will define a series of functions to achieve that objective\n",
    "from gensim.models import phrases\n",
    "from smart_open import smart_open\n",
    "from gensim.utils import simple_preprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cfed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import smart_open\n",
    "from gensim.utils import simple_preprocess\n",
    "import os\n",
    "#dictionary = Dictionary(\n",
    "#   simple_preprocess(line, deacc =True) for line in open('../data/pitchfork/dict.txt'))\n",
    "dictionary = Dictionary.load('../data/pitchfork/dict.pkl')\n",
    "bows = gensim.corpora.MmCorpus('../data/pitchfork/corpus.mm')\n",
    "docs = pickle.load(open('../data/pitchfork/docs.pkl','rb'))\n",
    "\n",
    "#dictionary = Dictionary.load_from_text('/media/data_files/github/Neural_Topic_Models/data/zhddline/dict.txt')\n",
    "#bows = gensim.corpora.MmCorpus('/media/data_files/github/Neural_Topic_Models/data/zhddline/corpus.mm')\n",
    "#docs = pickle.load(open('/media/data_files/github/Neural_Topic_Models/data/zhddline/docs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31406fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_bows_train, x_bows_test = train_test_split(bows, test_size=0.1, random_state=192)\n",
    "x_tokens_train, x_tokens_test = train_test_split(docs, test_size=0.1, random_state=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b972b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of training size is 18782 and tokes are 18782 len of test size is 2087 and tokens are 2087)\n"
     ]
    }
   ],
   "source": [
    "#item = list(zip(*bows[0]))\n",
    "#torch.tensor(list(item[1])).float()\n",
    "print(f'len of training size is {len(x_bows_train)} and tokes are {len(x_tokens_train)} '\n",
    "      f'len of test size is {len(x_bows_test)} and tokens are {len(x_tokens_test)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b304c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "\n",
    "class Data_Processing(object):\n",
    "    def __init__(self, docs, bows, vocab):\n",
    "        self.docs = docs\n",
    "        self.bows = bows\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        bow = torch.zeros(len(self.vocab))\n",
    "        # create token and frequency\n",
    "        item = list(zip(*self.bows[idx])) # bow = [[token_id1,token_id2,...],[freq1,freq2,...]]\n",
    "        # create\n",
    "        bow[list(item[0])] = torch.tensor(list(item[1])).float()\n",
    "        txt = self.docs[idx]\n",
    "        #print(f'shape of bow before being put together in data loader {bow.shape} {type(bow)}')\n",
    "        return txt, bow\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "    \n",
    "    def collate_fn(self,batch_data):\n",
    "        texts,bows = list(zip(*batch_data))\n",
    "        #print(f'what happens with collate function {torch.stack(bows,dim=0)}, {torch.stack(bows,dim=0).shape}')\n",
    "        return texts,torch.stack(bows,dim=0)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs:\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ddedffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "# create a class to process the traininga and test data\n",
    "training_data = Data_Processing(x_tokens_train, x_bows_train, dictionary)\n",
    "test_data = Data_Processing(x_tokens_test, x_bows_test, dictionary)\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4,\n",
    "                                        collate_fn=training_data.collate_fn),\n",
    "                    'test': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4,\n",
    "                                        collate_fn=test_data.collate_fn)}\n",
    "dataset_sizes = {'train':len(training_data)}\n",
    "example = next(iter(dataloaders_dict.get('train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9295729a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17756])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df0ce0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom torch.utils.data import DataLoader\\nimport torch\\nimport torch.nn.functional as F\\nfrom torch import nn\\n\\n\\nclass Data_Processing(object):\\n    def __init__(self, docs, bows, vocab):\\n        self.docs = docs\\n        self.bows = bows\\n        self.vocab = vocab\\n    \\n# iter method to get each element at the time and tokenize it using bert        \\n    def __getitem__(self, index):\\n        # create an empty torch object to store\\n        #expand_array(vocab,)\\n        bow = np.zeros(len(self.vocab))\\n        item = list(zip(*self.bows[index])) # bow = [[token_id1,token_id2,...],[freq1,freq2,...]]\\n        bow[list(item[0])] = list(item[1])\\n        bow = torch.tensor(bow).float()\\n        #bow = torch.stack(bow,dim=0)\\n        txt = self.docs[index]\\n        return txt, bow\\n        #return {'text':txt, 'bows':bow}\\n  \\n    def __len__(self):\\n        return len(self.docs)\\n    \\n    def collate_fn1(self, batch_data):\\n        texts, bows = list(zip(*batch_data))\\n        return texts, torch.stack(bows,dim=0)\\n\\nbatch_size = 512\\n\\n# create a class to process the traininga and test data\\ntraining_data = Data_Processing(x_tokens_train, x_bows_train, dictionary)\\ntest_data = Data_Processing(x_tokens_test, x_bows_test, dictionary)\\n\\n# use the dataloaders class to load the data\\ndataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4,\\n                                        collate_fn=training_data.collate_fn1),\\n                    'test': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4,\\n                                        collate_fn=test_data.collate_fn1)}\\ndataset_sizes = {'train':len(training_data)}\\nexample = next(iter(dataloaders_dict.get('train')))\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Data_Processing(object):\n",
    "    def __init__(self, docs, bows, vocab):\n",
    "        self.docs = docs\n",
    "        self.bows = bows\n",
    "        self.vocab = vocab\n",
    "    \n",
    "# iter method to get each element at the time and tokenize it using bert        \n",
    "    def __getitem__(self, index):\n",
    "        # create an empty torch object to store\n",
    "        #expand_array(vocab,)\n",
    "        bow = np.zeros(len(self.vocab))\n",
    "        item = list(zip(*self.bows[index])) # bow = [[token_id1,token_id2,...],[freq1,freq2,...]]\n",
    "        bow[list(item[0])] = list(item[1])\n",
    "        bow = torch.tensor(bow).float()\n",
    "        #bow = torch.stack(bow,dim=0)\n",
    "        txt = self.docs[index]\n",
    "        return txt, bow\n",
    "        #return {'text':txt, 'bows':bow}\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "    \n",
    "    def collate_fn1(self, batch_data):\n",
    "        texts, bows = list(zip(*batch_data))\n",
    "        return texts, torch.stack(bows,dim=0)\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# create a class to process the traininga and test data\n",
    "training_data = Data_Processing(x_tokens_train, x_bows_train, dictionary)\n",
    "test_data = Data_Processing(x_tokens_test, x_bows_test, dictionary)\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4,\n",
    "                                        collate_fn=training_data.collate_fn1),\n",
    "                    'test': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4,\n",
    "                                        collate_fn=test_data.collate_fn1)}\n",
    "dataset_sizes = {'train':len(training_data)}\n",
    "example = next(iter(dataloaders_dict.get('train')))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3d6472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 17756])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c749162b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport torch\\nimport torch.nn.functional as F\\nfrom torch import nn\\n\\nclass ETM(nn.Module):\\n    def __init__(self, dropout=0.0,\\n                 num_topics = 20, vocab_size = 2000, t_hidden_size = 800, emb_dim = 300,  \\n                 embeddings=None, train_embeddings=True):\\n        super(ETM, self).__init__()\\n        \\n        self.emb_dim = emb_dim\\n        self.vocab_size = vocab_size\\n        self.num_topics = num_topics\\n        self.t_hidden_size = t_hidden_size\\n        \\n        # add the layers that map vocab to first hidden size\\n        # then add layers from hidden to t_hidden_size\\n        self.encoder = nn.ModuleDict({\\n                'enc_1': nn.Linear(vocab_size, t_hidden_size),\\n                'enc_2': nn.Linear(t_hidden_size, emb_dim)})\\n        # define mu emb_size to num_topics\\n        self.fc_mu = nn.Linear(emb_dim,num_topics)\\n        # define logvar emb_sim to num_topics\\n        self.fc_logvar = nn.Linear(emb_dim,num_topics)\\n        \\n        # decoder part, define layers from num_topics to hidden size\\n        # define layer from hidden_size to vocab size\\n        #self.decoder = nn.ModuleDict({\\n        #        'dec_1': nn.Linear(num_topics, t_hidden_size),\\n        #        'dec_2': nn.Linear(t_hidden_size, vocab_size)})\\n        \\n        #self.latent_dim = num_topics\\n        # dropout probability\\n        self.dropout = nn.Dropout(p=dropout)\\n        self.fc1 = nn.Linear(num_topics,num_topics)\\n        \\n        if train_embeddings:\\n            # embeddings layer\\n            self.rho = nn.Linear(emb_dim,vocab_size)\\n        else:\\n            pass\\n            # add possibility of using pretrained embeddings\\n            \\n        # embedding for topics\\n        self.alpha = nn.Linear(emb_dim,num_topics)\\n        self.decoder = None\\n        \\n\\n    def encode(self, x):\\n        hid = x\\n        for i,layer in self.encoder.items():\\n            hid = F.relu(self.dropout(layer(hid)))\\n        mu, log_var = self.fc_mu(hid), self.fc_logvar(hid)\\n        return mu, log_var\\n\\n    def inference(self,x):\\n        mu, log_var = self.encode(x)\\n        theta = torch.softmax(x,dim=1)\\n        return theta\\n    \\n    def reparameterize(self, mu, log_var):\\n        std = torch.exp(log_var/2)\\n        eps = torch.randn_like(std)\\n        z = mu + eps * std\\n        return z\\n\\n    def decode(self, z):\\n        hid = z\\n        for i,(_,layer) in enumerate(self.decoder.items()):\\n            hid = layer(hid)\\n            if i<len(self.decoder)-1:\\n                hid = F.relu(self.dropout(hid))\\n        return hid\\n    \\n    def decode1(self,z):\\n        wght_dec = self.alpha(self.rho.weight) #[K,V]\\n        #print(f'weihjt ro {wght_dec}')\\n        beta = F.softmax(wght_dec,dim=0).transpose(1,0)\\n        #print(f'beta shape {beta.shape}and beta {beta}')\\n        print(f'logits are z {z.shape} and beta {beta.shape}')\\n        res = torch.mm(z,beta)\\n        logits = torch.log(res+1e-6)\\n        return logits\\n    \\n    def forward(self, x, collate_fn=None):\\n        mu, log_var = self.encode(x)\\n        _theta = self.reparameterize(mu, log_var)\\n        _theta = self.fc1(_theta) \\n        if collate_fn!=None:\\n            theta = collate_fn(_theta)\\n        else:\\n            theta = _theta\\n        x_reconst = self.decode1(theta)\\n        return x_reconst, mu, log_var\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the model\n",
    "'''\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class ETM(nn.Module):\n",
    "    def __init__(self, dropout=0.0,\n",
    "                 num_topics = 20, vocab_size = 2000, t_hidden_size = 800, emb_dim = 300,  \n",
    "                 embeddings=None, train_embeddings=True):\n",
    "        super(ETM, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_topics = num_topics\n",
    "        self.t_hidden_size = t_hidden_size\n",
    "        \n",
    "        # add the layers that map vocab to first hidden size\n",
    "        # then add layers from hidden to t_hidden_size\n",
    "        self.encoder = nn.ModuleDict({\n",
    "                'enc_1': nn.Linear(vocab_size, t_hidden_size),\n",
    "                'enc_2': nn.Linear(t_hidden_size, emb_dim)})\n",
    "        # define mu emb_size to num_topics\n",
    "        self.fc_mu = nn.Linear(emb_dim,num_topics)\n",
    "        # define logvar emb_sim to num_topics\n",
    "        self.fc_logvar = nn.Linear(emb_dim,num_topics)\n",
    "        \n",
    "        # decoder part, define layers from num_topics to hidden size\n",
    "        # define layer from hidden_size to vocab size\n",
    "        #self.decoder = nn.ModuleDict({\n",
    "        #        'dec_1': nn.Linear(num_topics, t_hidden_size),\n",
    "        #        'dec_2': nn.Linear(t_hidden_size, vocab_size)})\n",
    "        \n",
    "        #self.latent_dim = num_topics\n",
    "        # dropout probability\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc1 = nn.Linear(num_topics,num_topics)\n",
    "        \n",
    "        if train_embeddings:\n",
    "            # embeddings layer\n",
    "            self.rho = nn.Linear(emb_dim,vocab_size)\n",
    "        else:\n",
    "            pass\n",
    "            # add possibility of using pretrained embeddings\n",
    "            \n",
    "        # embedding for topics\n",
    "        self.alpha = nn.Linear(emb_dim,num_topics)\n",
    "        self.decoder = None\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        hid = x\n",
    "        for i,layer in self.encoder.items():\n",
    "            hid = F.relu(self.dropout(layer(hid)))\n",
    "        mu, log_var = self.fc_mu(hid), self.fc_logvar(hid)\n",
    "        return mu, log_var\n",
    "\n",
    "    def inference(self,x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        theta = torch.softmax(x,dim=1)\n",
    "        return theta\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        hid = z\n",
    "        for i,(_,layer) in enumerate(self.decoder.items()):\n",
    "            hid = layer(hid)\n",
    "            if i<len(self.decoder)-1:\n",
    "                hid = F.relu(self.dropout(hid))\n",
    "        return hid\n",
    "    \n",
    "    def decode1(self,z):\n",
    "        wght_dec = self.alpha(self.rho.weight) #[K,V]\n",
    "        #print(f'weihjt ro {wght_dec}')\n",
    "        beta = F.softmax(wght_dec,dim=0).transpose(1,0)\n",
    "        #print(f'beta shape {beta.shape}and beta {beta}')\n",
    "        print(f'logits are z {z.shape} and beta {beta.shape}')\n",
    "        res = torch.mm(z,beta)\n",
    "        logits = torch.log(res+1e-6)\n",
    "        return logits\n",
    "    \n",
    "    def forward(self, x, collate_fn=None):\n",
    "        mu, log_var = self.encode(x)\n",
    "        _theta = self.reparameterize(mu, log_var)\n",
    "        _theta = self.fc1(_theta) \n",
    "        if collate_fn!=None:\n",
    "            theta = collate_fn(_theta)\n",
    "        else:\n",
    "            theta = _theta\n",
    "        x_reconst = self.decode1(theta)\n",
    "        return x_reconst, mu, log_var\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c9475b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encode_dims=[2000,1024,512,20],decode_dims=[20,1024,2000],dropout=0.0):\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.ModuleDict({\n",
    "            f'enc_{i}':nn.Linear(encode_dims[i],encode_dims[i+1]) \n",
    "            for i in range(len(encode_dims)-2)\n",
    "        })\n",
    "        self.fc_mu = nn.Linear(encode_dims[-2],encode_dims[-1])\n",
    "        self.fc_logvar = nn.Linear(encode_dims[-2],encode_dims[-1])\n",
    "\n",
    "        self.decoder = nn.ModuleDict({\n",
    "            f'dec_{i}':nn.Linear(decode_dims[i],decode_dims[i+1])\n",
    "            for i in range(len(decode_dims)-1)\n",
    "        })\n",
    "        self.latent_dim = encode_dims[-1]\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc1 = nn.Linear(encode_dims[-1],encode_dims[-1])\n",
    "        \n",
    "        \n",
    "    def encode(self, x):\n",
    "        hid = x\n",
    "        for i,layer in self.encoder.items():\n",
    "            hid = F.relu(self.dropout(layer(hid)))\n",
    "        mu, log_var = self.fc_mu(hid), self.fc_logvar(hid)\n",
    "        return mu, log_var\n",
    "\n",
    "    def inference(self,x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        theta = torch.softmax(x,dim=1)\n",
    "        return theta\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        #print('we are using original decoder of vae')\n",
    "        hid = z\n",
    "        for i,(_,layer) in enumerate(self.decoder.items()):\n",
    "            hid = layer(hid)\n",
    "            if i<len(self.decoder)-1:\n",
    "                hid = F.relu(self.dropout(hid))\n",
    "        return hid\n",
    "    \n",
    "    def forward(self, x, collate_fn=None):\n",
    "        #print(f'inputs of encoder {x.shape}')\n",
    "        # mu and log_var\n",
    "        mu, log_var = self.encode(x)\n",
    "        #print(f'shape of the outputs {mu.shape, log_var.shape}')\n",
    "        _theta = self.reparameterize(mu, log_var)\n",
    "        #print(f'shape of theta reparametized {_theta.shape}')\n",
    "        _theta = self.fc1(_theta) \n",
    "        #print(f'shape of theta after FC1 {_theta.shape}')\n",
    "        if collate_fn!=None:\n",
    "            #print(f'we are collating and shape is {_theta.shape}')\n",
    "            theta = collate_fn(_theta)\n",
    "            #print(f'theta collated {theta.shape}')\n",
    "        else:\n",
    "            theta = _theta\n",
    "        x_reconst = self.decode(theta)\n",
    "        #print(f'output of theta from decoder {x_reconst.shape}')\n",
    "        #time.sleep(5)\n",
    "\n",
    "        return x_reconst, mu, log_var\n",
    "\n",
    "class EVAE(VAE):\n",
    "    def __init__(self, encode_dims=[2000,1024,512,20],decode_dims=[20,1024,2000],dropout=0.0,emb_dim=300):\n",
    "        super(EVAE,self).__init__(encode_dims=encode_dims,decode_dims=decode_dims,dropout=dropout)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vocab_size = encode_dims[0]\n",
    "        self.n_topic = encode_dims[-1]\n",
    "        self.rho = nn.Linear(emb_dim,self.vocab_size)\n",
    "        self.alpha = nn.Linear(emb_dim,self.n_topic)\n",
    "        self.decoder = None\n",
    "\n",
    "    def decode(self,z):\n",
    "        #print('we are here enforcing the decoder we redefing')\n",
    "        wght_dec = self.alpha(self.rho.weight) #[K,V]\n",
    "        #print(f'we are here defining the weights of rho {wght_dec.shape}')\n",
    "        beta = F.softmax(wght_dec,dim=0).transpose(1,0)\n",
    "        #print(f'we are now on beta and the shape is {beta.shape}')\n",
    "        res = torch.mm(z,beta)\n",
    "        #print(f'res is the matmul of z which is {res.shape}')\n",
    "        logits = torch.log(res+1e-6)\n",
    "        #print(f'logits of the log of res are {logits.shape}')\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bd6fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = EVAE(encode_dims=[len(dictionary),1024,512,20],decode_dims=[20,512,len(dictionary)],\n",
    "#             dropout=0.0,emb_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b7fdf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41703b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now prepare a bunch of auxiliary functions to evaluate the consistency of the model\n",
    "import numpy as np\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "def calc_topic_diversity(topic_words):\n",
    "    '''topic_words is in the form of [[w11,w12,...],[w21,w22,...]]'''\n",
    "    vocab = set(sum(topic_words,[]))\n",
    "    n_total = len(topic_words) * len(topic_words[0])\n",
    "    topic_div = len(vocab) / n_total\n",
    "    return topic_div\n",
    "\n",
    "def calc_topic_coherence(topic_words,docs,dictionary,emb_path=None,taskname=None,sents4emb=None,calc4each=False):\n",
    "    # emb_path: path of the pretrained word2vec weights, in text format.\n",
    "    # sents4emb: list/generator of tokenized sentences.\n",
    "    # Computing the C_V score\n",
    "    cv_coherence_model = CoherenceModel(topics=topic_words,texts=docs,dictionary=dictionary,coherence='c_v')\n",
    "    cv_per_topic = cv_coherence_model.get_coherence_per_topic() if calc4each else None\n",
    "    cv_score = cv_coherence_model.get_coherence()\n",
    "    \n",
    "    # Computing the C_W2V score\n",
    "    try:\n",
    "        w2v_model_path = os.path.join(os.getcwd(),'data',f'{taskname}','w2v_weight_kv.txt')\n",
    "        # Priority order: 1) user's embed file; 2) standard path embed file; 3) train from scratch then store.\n",
    "        if emb_path!=None and os.path.exists(emb_path):\n",
    "            keyed_vectors = gensim.models.KeyedVectors.load_word2vec_format(emb_path,binary=False)\n",
    "        elif os.path.exists(w2v_model_path):\n",
    "            keyed_vectors = gensim.models.KeyedVectors.load_word2vec_format(w2v_model_path,binary=False)\n",
    "        elif sents4emb!=None:\n",
    "            print('Training a word2vec model 20 epochs to evaluate topic coherence, this may take a few minutes ...')\n",
    "            w2v_model = gensim.models.Word2Vec(sents4emb,size=300,min_count=1,workers=6,iter=20)\n",
    "            keyed_vectors = w2v_model.wv\n",
    "            keyed_vectors.save_word2vec_format(w2v_model_path,binary=False)\n",
    "        else:\n",
    "            raise Exception(\"C_w2v score isn't available for the missing of training corpus (sents4emb=None).\")\n",
    "            \n",
    "        w2v_coherence_model = CoherenceModel(topics=topic_words,texts=docs,dictionary=dictionary,coherence='c_w2v',keyed_vectors=keyed_vectors)\n",
    "\n",
    "        w2v_per_topic = w2v_coherence_model.get_coherence_per_topic() if calc4each else None\n",
    "        w2v_score = w2v_coherence_model.get_coherence()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        #In case of OOV Error\n",
    "        w2v_per_topic = [None for _ in range(len(topic_words))]\n",
    "        w2v_score = None\n",
    "    \n",
    "    # Computing the C_UCI score\n",
    "    c_uci_coherence_model = CoherenceModel(topics=topic_words,texts=docs,dictionary=dictionary,coherence='c_uci')\n",
    "    c_uci_per_topic = c_uci_coherence_model.get_coherence_per_topic() if calc4each else None\n",
    "    c_uci_score = c_uci_coherence_model.get_coherence()\n",
    "    \n",
    "    \n",
    "    # Computing the C_NPMI score\n",
    "    c_npmi_coherence_model = CoherenceModel(topics=topic_words,texts=docs,dictionary=dictionary,coherence='c_npmi')\n",
    "    c_npmi_per_topic = c_npmi_coherence_model.get_coherence_per_topic() if calc4each else None\n",
    "    c_npmi_score = c_npmi_coherence_model.get_coherence()\n",
    "    return (cv_score,w2v_score,c_uci_score, c_npmi_score),(cv_per_topic,w2v_per_topic,c_uci_per_topic,c_npmi_per_topic)\n",
    "\n",
    "\n",
    "def evaluate_topic_quality(topic_words, test_data, taskname=None, calc4each=False):\n",
    "    \n",
    "    td_score = calc_topic_diversity(topic_words)\n",
    "    print(f'topic diversity:{td_score}')\n",
    "    \n",
    "    (c_v, c_w2v, c_uci, c_npmi),\\\n",
    "        (cv_per_topic, c_w2v_per_topic, c_uci_per_topic, c_npmi_per_topic) = \\\n",
    "        calc_topic_coherence(topic_words=topic_words, docs=test_data.docs, dictionary=test_data.dictionary,\n",
    "                             emb_path=None, taskname=taskname, sents4emb=test_data, calc4each=calc4each)\n",
    "    print('c_v:{}, c_w2v:{}, c_uci:{}, c_npmi:{}'.format(\n",
    "        c_v, c_w2v, c_uci, c_npmi))\n",
    "    scrs = {'c_v':cv_per_topic,'c_w2v':c_w2v_per_topic,'c_uci':c_uci_per_topic,'c_npmi':c_npmi_per_topic}\n",
    "    if calc4each:\n",
    "        for scr_name,scr_per_topic in scrs.items():\n",
    "            print(f'{scr_name}:')\n",
    "            for t_idx, (score, twords) in enumerate(zip(scr_per_topic, topic_words)):\n",
    "                print(f'topic.{t_idx+1:>03d}: {score} {twords}')\n",
    "    \n",
    "    mimno_tc = mimno_topic_coherence(topic_words, test_data.docs)\n",
    "    print('mimno topic coherence:{}'.format(mimno_tc))\n",
    "    if calc4each:\n",
    "        return (c_v, c_w2v, c_uci, c_npmi, mimno_tc, td_score), (cv_per_topic, c_w2v_per_topic, c_uci_per_topic, c_npmi_per_topic)\n",
    "    else:\n",
    "        return c_v, c_w2v, c_uci, c_npmi, mimno_tc, td_score\n",
    "    \n",
    "def inference_by_bow(model,doc_bow):\n",
    "    # doc_bow: torch.tensor [vocab_size]; optional: np.array [vocab_size]\n",
    "    if isinstance(doc_bow,np.ndarray):\n",
    "        doc_bow = torch.from_numpy(doc_bow)\n",
    "        doc_bow = doc_bow.reshape(-1,bow_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            mu,log_var = model.encode(doc_bow)\n",
    "            mu = self.vae.fc1(mu) \n",
    "            theta = F.softmax(mu,dim=1)\n",
    "            return theta.detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "def inference(model, doc_tokenized, dictionary,normalize=True):\n",
    "    doc_bow = torch.zeros(1,len(dictionary))\n",
    "    for token in doc_tokenized:\n",
    "        try:\n",
    "            idx = dictionary.token2id[token]\n",
    "            doc_bow[0][idx] += 1.0\n",
    "        except:\n",
    "            print(f'{token} not in the vocabulary.')\n",
    "        doc_bow = doc_bow.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu,log_var = model.encode(doc_bow)\n",
    "            mu = model.fc1(mu)\n",
    "            if normalize:\n",
    "                theta = F.softmax(mu,dim=1)\n",
    "            return theta.detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "def get_embed(model,train_data, num=1000):\n",
    "    model.eval()\n",
    "    data_loader = DataLoader(train_data, batch_size=512,\n",
    "                             shuffle=False, num_workers=4, collate_fn=train_data.collate_fn)\n",
    "    embed_lst = []\n",
    "    txt_lst = []\n",
    "    cnt = 0\n",
    "    for data_batch in data_loader:\n",
    "        txts, bows = data_batch\n",
    "        embed = inference_by_bow(bows)\n",
    "        embed_lst.append(embed)\n",
    "        txt_lst.append(txts)\n",
    "        cnt += embed.shape[0]\n",
    "        if cnt>=num:\n",
    "            break\n",
    "        embed_lst = np.concatenate(embed_lst,axis=0)[:num]\n",
    "        txt_lst = np.concatenate(txt_lst,axis=0)[:num]\n",
    "        return txt_lst, embed_lst\n",
    "\n",
    "def get_topic_word_dist(model,normalize=True):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        idxes = torch.eye(model.n_topic).to(device)\n",
    "        print(f'shape of idxes is {idxes.shape}')\n",
    "        word_dist = model.decode(idxes)  # word_dist: [n_topic, vocab.size]\n",
    "        if normalize:\n",
    "            word_dist = F.softmax(word_dist,dim=1)\n",
    "        return word_dist.detach().cpu().numpy()\n",
    "    \n",
    "def show_topic_words(model,topic_id=None,topK=15, id2token= None, dictionary=None):\n",
    "        topic_words = []\n",
    "        topic_idx = []\n",
    "        vals_ = []\n",
    "        with torch.no_grad():\n",
    "            idxes = torch.eye(model.n_topic).to(device)\n",
    "            print(f'shape of idxes is {idxes.shape}')\n",
    "            word_dist = model.decode(idxes)\n",
    "            word_dist_logs= torch.softmax(word_dist,dim=1)\n",
    "            word_out = word_dist_logs.detach().cpu().numpy()\n",
    "            print(torch.argmax(word_dist_logs, dim = 1))\n",
    "            vals,indices = torch.topk(word_dist_logs,topK,dim=1)\n",
    "            print(f'shape of vals is {vals.shape} shape of index is {indices.shape}')\n",
    "            vals = vals.cpu().tolist()\n",
    "            vals_.append(vals)\n",
    "            indices = indices.cpu().tolist()\n",
    "            topic_idx.append(indices)\n",
    "            if id2token==None and dictionary!=None:\n",
    "                id2token = {v:k for k,v in dictionary.token2id.items()}\n",
    "            if topic_id==None:\n",
    "                print('iterating over topics')\n",
    "                for i in range(model.n_topic):\n",
    "                    #print(i, indices, vals)\n",
    "                    topic_words.append([id2token[idx] for idx in indices[i]])\n",
    "                    #topic_idx.append(indices)\n",
    "            else:\n",
    "                topic_words.append([id2token[idx] for idx in indices[topic_id]])\n",
    "        return topic_words\n",
    "\n",
    "def load_model(model):\n",
    "    model.load_state_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5170dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ETM(num_topics = 20, vocab_size = len(dictionary), t_hidden_size = 1024, emb_dim = 300,  \n",
    "#          embeddings=None, train_embeddings=True, dropout=0.1)\n",
    "\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a1e98da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d3bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a61af7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport wandb\\nwandb.init(entity=\"jlealtru\", project=\"ETM_runs_p\")\\n\\n\\n# WandB – Config is a variable that holds and saves hyperparameters and inputs\\nconfig = wandb.config          # Initialize config\\nconfig.batch_size = batch_size          # input batch size for training (default: 64)\\nconfig.epochs = 400             # number of epochs to train (default: 10)\\nconfig.lr = lr               # learning rate (default: 0.01)\\nconfig.no_cuda = False         # disables CUDA training\\nconfig.seed = 42               # random seed (default: 42)\\nconfig.log_interval = 20     # how many batches to wait before logging training status\\nconfig\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import wandb\n",
    "wandb.init(entity=\"jlealtru\", project=\"ETM_runs_p\")\n",
    "\n",
    "\n",
    "# WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
    "config = wandb.config          # Initialize config\n",
    "config.batch_size = batch_size          # input batch size for training (default: 64)\n",
    "config.epochs = 400             # number of epochs to train (default: 10)\n",
    "config.lr = lr               # learning rate (default: 0.01)\n",
    "config.no_cuda = False         # disables CUDA training\n",
    "config.seed = 42               # random seed (default: 42)\n",
    "config.log_interval = 20     # how many batches to wait before logging training status\n",
    "config\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8db2505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef training(model = None, epochs = 100, optimizer = None, vocab = None):\\n        \"\"\"\\n        Training the mothefucking model\\n        \"\"\"\\n        acc_loss = 0\\n        acc_kl_theta_loss = 0\\n        cnt = 0\\n        trainloss_lst, valloss_lst = [], []\\n        recloss_lst, klloss_lst = [],[]\\n        for i in range(epochs):\\n            model.train()\\n            optimizer.zero_grad()\\n            #model.zero_grad()\\n            epochloss_lst = []\\n            for index, (bows) in enumerate(dataloaders_dict[\\'train\\']):\\n                bows = bows[1].to(device)\\n                bows_recon,mus,log_vars = model(bows,lambda x:torch.softmax(x,dim=1))\\n                logsoftmax = torch.log_softmax(bows_recon,dim=1)\\n                #bows_recon,mus,log_vars  = model(bows)\\n                #print(bows_recon.shape)\\n                #rint(bows_recon[0])\\n                #model(bows, lambda x:torch.softmax(x,dim=1))\\n                #logsoftmax = torch.log_softmax(bows_recon, lambda x:torch.softmax(x,dim=1))\\n                rec_loss = -1.0 * torch.sum(bows*logsoftmax)\\n                \\n                kl_div = -0.5 * torch.sum(1+log_vars-mus.pow(2)-log_vars.exp())\\n                \\n                loss = rec_loss + kl_div * 1.0\\n                \\n                loss.backward()\\n                optimizer.step()\\n\\n                trainloss_lst.append(loss.item()/len(bows))\\n                epochloss_lst.append(loss.item()/len(bows))\\n                \\n                if index % 20==0:\\n                    print(f\\'Epoch {(i+1):>3d}\\tIter {(i+1):>4d}\\tLoss:{loss.item()/len(bows):<.7f}\\tRec Loss:{rec_loss.item()/len(bows):<.7f}\\tKL Div:{kl_div.item()/len(bows):<.7f}\\')\\n                    #wandb.log({\"Epoch\": i,\"Train Loss\": loss.item()/len(bows),\"rec_loss\": rec_loss.item()/len(bows)})\\n            \\n            if (i+1)%10==0:\\n                save_name = f\\'../results/ETM_models/ETM_tp{model.num_topics}_{time.strftime(\"%Y-%m-%d-%H-%M\", time.localtime())}_ep{i}_hd{model.t_hidden_size}.ckpt\\'\\n                checkpoint = {\\n                    \"net\": model.state_dict(),\\n                    \"optimizer\": optimizer.state_dict(),\\n                    \"epoch\": i,\\n                    \"param\": {\\n                        \"bow_dim\": len(vocab),\\n                        \"n_topic\": 25,\\n                        \"emb_dim\": 300,\\n                        \\'hidden_t\\': 1024\\n                    }\\n                }\\n                torch.save(checkpoint,save_name)\\n            #if (i+1)%20==0:\\n            #    with torch.no_grad():\\n            #        model.eval()\\n            #        for i in range(20):\\n            #            show_topic_words(model,topic_id=i,topK=15, dictionary=vocab)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def training(model = None, epochs = 100, optimizer = None, vocab = None):\n",
    "        \"\"\"\n",
    "        Training the mothefucking model\n",
    "        \"\"\"\n",
    "        acc_loss = 0\n",
    "        acc_kl_theta_loss = 0\n",
    "        cnt = 0\n",
    "        trainloss_lst, valloss_lst = [], []\n",
    "        recloss_lst, klloss_lst = [],[]\n",
    "        for i in range(epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            #model.zero_grad()\n",
    "            epochloss_lst = []\n",
    "            for index, (bows) in enumerate(dataloaders_dict['train']):\n",
    "                bows = bows[1].to(device)\n",
    "                bows_recon,mus,log_vars = model(bows,lambda x:torch.softmax(x,dim=1))\n",
    "                logsoftmax = torch.log_softmax(bows_recon,dim=1)\n",
    "                #bows_recon,mus,log_vars  = model(bows)\n",
    "                #print(bows_recon.shape)\n",
    "                #rint(bows_recon[0])\n",
    "                #model(bows, lambda x:torch.softmax(x,dim=1))\n",
    "                #logsoftmax = torch.log_softmax(bows_recon, lambda x:torch.softmax(x,dim=1))\n",
    "                rec_loss = -1.0 * torch.sum(bows*logsoftmax)\n",
    "                \n",
    "                kl_div = -0.5 * torch.sum(1+log_vars-mus.pow(2)-log_vars.exp())\n",
    "                \n",
    "                loss = rec_loss + kl_div * 1.0\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                trainloss_lst.append(loss.item()/len(bows))\n",
    "                epochloss_lst.append(loss.item()/len(bows))\n",
    "                \n",
    "                if index % 20==0:\n",
    "                    print(f'Epoch {(i+1):>3d}\\tIter {(i+1):>4d}\\tLoss:{loss.item()/len(bows):<.7f}\\tRec Loss:{rec_loss.item()/len(bows):<.7f}\\tKL Div:{kl_div.item()/len(bows):<.7f}')\n",
    "                    #wandb.log({\"Epoch\": i,\"Train Loss\": loss.item()/len(bows),\"rec_loss\": rec_loss.item()/len(bows)})\n",
    "            \n",
    "            if (i+1)%10==0:\n",
    "                save_name = f'../results/ETM_models/ETM_tp{model.num_topics}_{time.strftime(\"%Y-%m-%d-%H-%M\", time.localtime())}_ep{i}_hd{model.t_hidden_size}.ckpt'\n",
    "                checkpoint = {\n",
    "                    \"net\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"epoch\": i,\n",
    "                    \"param\": {\n",
    "                        \"bow_dim\": len(vocab),\n",
    "                        \"n_topic\": 25,\n",
    "                        \"emb_dim\": 300,\n",
    "                        'hidden_t': 1024\n",
    "                    }\n",
    "                }\n",
    "                torch.save(checkpoint,save_name)\n",
    "            #if (i+1)%20==0:\n",
    "            #    with torch.no_grad():\n",
    "            #        model.eval()\n",
    "            #        for i in range(20):\n",
    "            #            show_topic_words(model,topic_id=i,topK=15, dictionary=vocab)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02734499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAE(\n",
      "  (encoder): ModuleDict(\n",
      "    (enc_0): Linear(in_features=17756, out_features=1024, bias=True)\n",
      "    (enc_1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (fc_logvar): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (decoder): None\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (fc1): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (rho): Linear(in_features=300, out_features=17756, bias=True)\n",
      "  (alpha): Linear(in_features=300, out_features=20, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_dim = len(dictionary)\n",
    "n_topic = 20\n",
    "emb_dim = 300\n",
    "model = EVAE(encode_dims=[bow_dim,1024,512,n_topic],decode_dims=[n_topic,512,bow_dim],dropout=0.0,emb_dim=emb_dim)\n",
    "print(model)\n",
    "from torch.optim import Adam\n",
    "lr = 1e-3\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68b081dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjlealtru\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.15<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">wise-cosmos-50</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jlealtru/ETM_runs_p\" target=\"_blank\">https://wandb.ai/jlealtru/ETM_runs_p</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jlealtru/ETM_runs_p/runs/1q5u1ydl\" target=\"_blank\">https://wandb.ai/jlealtru/ETM_runs_p/runs/1q5u1ydl</a><br/>\n",
       "                Run data is saved locally in <code>/media/data_files/github/website_tutorials/notebooks/wandb/run-20211020_135413-1q5u1ydl</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1q5u1ydl)</h1><iframe src=\"https://wandb.ai/jlealtru/ETM_runs_p/runs/1q5u1ydl\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fdecf6fcb50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(entity=\"jlealtru\", project=\"ETM_runs_p\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "281607d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,batch_size=256,\n",
    "          learning_rate=2e-3,test_data=None,\n",
    "          num_epochs=200,is_evaluate=False,log_every=10,beta=1.0, \n",
    "          criterion='cross_entropy',ckpt=None):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    #self.id2token = {v:k for k,v in train_data.dictionary.token2id.items()}\n",
    "    #data_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,\n",
    "    #                         num_workers=4,collate_fn=train_data.collate_fn)\n",
    "\n",
    "    data_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4,\n",
    "                                        collate_fn=training_data.collate_fn)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    if ckpt:\n",
    "        self.load_model(ckpt[\"net\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        start_epoch = ckpt[\"epoch\"] + 1\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainloss_lst, valloss_lst = [], []\n",
    "    recloss_lst, klloss_lst = [],[]\n",
    "    c_v_lst, c_w2v_lst, c_uci_lst, c_npmi_lst, mimno_tc_lst, td_lst = [], [], [], [], [], []\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        epochloss_lst = []\n",
    "        for iter_,data in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            txts,bows = data\n",
    "            bows = bows.to(device)\n",
    "            #print(f'we now are in training and the shape of bows is {bows.shape}')\n",
    "            \n",
    "            bows_recon,mus,log_vars = model(bows, lambda x:torch.softmax(x,dim=1))\n",
    "            #print(f'we now look at output of vae with lambda x shape of bows_recons is {bows_recon.shape} mus {mus.shape} and log vars {log_vars.shape}')\n",
    "            #print(bows_recon[0][0:10])\n",
    "\n",
    "            #print(f'bows recon shape is {bows_recon.shape}')\n",
    "            #print(f'sum of torch {torch.sum(bows_recon, dim = 1)}')\n",
    "            #print(f'one bow is {bows_recon[0]}')\n",
    "            #bows_recon = torch.softmax(bows_recon,dim=1)\n",
    "            \n",
    "            #print(model(bows,lambda x:torch.softmax(x,dim=1)))\n",
    "            #print(model(bows)[2].shape)\n",
    "            #bows_recon,mus,log_vars = model(bows)\n",
    "            #print(bows_recon[1].shape)\n",
    "            if criterion=='cross_entropy':\n",
    "                logsoftmax = torch.log_softmax(bows_recon,dim=1)\n",
    "                #print(f'softmax shape is {logsoftmax.shape}')\n",
    "                #print(f'softmax is {logsoftmax[0]}')\n",
    "                #print(f'softmax is {logsoftmax[0]}')\n",
    "                rec_loss = -1.0 * torch.sum(bows*logsoftmax)\n",
    "                #print(f'rec_loss is {rec_loss}')\n",
    "                #print(f'mat mul is  {torch.sum(bows*logsoftmax)}')\n",
    "                #print(f'mat mul is  {torch.sum(bows*logsoftmax).shape}')\n",
    "            elif criterion=='bce_softmax':\n",
    "                rec_loss = F.binary_cross_entropy(torch.softmax(bows_recon,dim=1),bows,reduction='sum')\n",
    "            elif criterion=='bce_sigmoid':\n",
    "                rec_loss = F.binary_cross_entropy(torch.sigmoid(bows_recon),bows,reduction='sum')\n",
    "\n",
    "            kl_div = -0.5 * torch.sum(1+log_vars-mus.pow(2)-log_vars.exp())\n",
    "\n",
    "            loss = rec_loss + kl_div * beta\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            trainloss_lst.append(loss.item()/len(bows))\n",
    "            epochloss_lst.append(loss.item()/len(bows))\n",
    "            #print(iter_, type(epoch))\n",
    "            if (iter_+1) % 6==0:\n",
    "                print(f'Epoch {(epoch+1):>3d}\\tIter {(iter_+1):>4d}\\tLoss:{loss.item()/len(bows):<.7f}\\tRec Loss:{rec_loss.item()/len(bows):<.7f}\\tKL Div:{kl_div.item()/len(bows):<.7f}')\n",
    "                wandb.log({\"Epoch\": iter_+1,\n",
    "                           \"Train Loss\": loss.item()/len(bows),\"rec_loss\": rec_loss.item()/len(bows),\n",
    "                          \"lr\": learning_rate,\n",
    "                          \"optimizer\": 'Adam'})\n",
    "        \n",
    "        if (epoch+1) % 40==0:    \n",
    "            print(f'Epoch {(epoch+1):>3d}\\tLoss:{sum(epochloss_lst)/len(epochloss_lst):<.7f}')\n",
    "            print('\\n'.join([str(lst) for lst in show_topic_words(model,\n",
    "                                                                  topic_id=None,\n",
    "                                                                  topK=15, dictionary=dictionary)]))\n",
    "            top_ = show_topic_words(model,topic_id=None,topK=15, dictionary=dictionary)\n",
    "            print(calc_topic_diversity(show_topic_words(model,topic_id=None,topK=15, dictionary=dictionary)))\n",
    "            print('calculating topic dist')\n",
    "            size = get_topic_word_dist(model,normalize=True)\n",
    "            for i in np.argsort(size[0], axis = 0)[::-1][:15]:\n",
    "                print(dictionary[i])\n",
    "            c_uci_coherence_model = CoherenceModel(topics=top_,texts=x_tokens_test,dictionary=dictionary,coherence='c_uci')\n",
    "            c_npmi_coherence_model = CoherenceModel(topics=top_,texts=x_tokens_test,dictionary=dictionary,coherence='c_npmi')\n",
    "            c_cv_coherence_model = CoherenceModel(topics=top_,texts=x_tokens_test,dictionary=dictionary,coherence='c_v')\n",
    "            uci = c_uci_coherence_model.get_coherence()\n",
    "            print(f'coherence c_uci {uci}')\n",
    "            print(f'coherence c_npmi {c_npmi_coherence_model.get_coherence()}')\n",
    "            print(f'coherence c_cv {c_cv_coherence_model.get_coherence()}')\n",
    "            wandb.log({\"topic_diversity\": calc_topic_diversity(show_topic_words(model,topic_id=None,topK=15, dictionary=dictionary)), \n",
    "                       \"c_uci\": uci,\"c_npmi\": c_npmi_coherence_model.get_coherence(), \n",
    "                       \"c_cv\": c_cv_coherence_model.get_coherence(),  \"rec_loss\": rec_loss.item()/len(bows),\n",
    "                      \"epoch_loss\":sum(epochloss_lst)/len(epochloss_lst)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4328a77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlealtru/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1\tIter    6\tLoss:2435.6679688\tRec Loss:2435.6616211\tKL Div:0.0063035\n",
      "Epoch   1\tIter   12\tLoss:2364.9091797\tRec Loss:2364.8942871\tKL Div:0.0148828\n",
      "Epoch   1\tIter   18\tLoss:2432.1032715\tRec Loss:2432.0595703\tKL Div:0.0436287\n",
      "Epoch   2\tIter    6\tLoss:2450.7968750\tRec Loss:2450.6955566\tKL Div:0.1012335\n",
      "Epoch   2\tIter   12\tLoss:2359.8413086\tRec Loss:2359.6572266\tKL Div:0.1841140\n",
      "Epoch   2\tIter   18\tLoss:2358.7370605\tRec Loss:2358.3994141\tKL Div:0.3375591\n",
      "Epoch   3\tIter    6\tLoss:2285.2260742\tRec Loss:2284.5649414\tKL Div:0.6611623\n",
      "Epoch   3\tIter   12\tLoss:2291.8762207\tRec Loss:2290.6474609\tKL Div:1.2288318\n",
      "Epoch   3\tIter   18\tLoss:2284.7360840\tRec Loss:2282.8945312\tKL Div:1.8415115\n",
      "Epoch   4\tIter    6\tLoss:2221.6940918\tRec Loss:2219.4140625\tKL Div:2.2801003\n",
      "Epoch   4\tIter   12\tLoss:2207.5747070\tRec Loss:2205.4396973\tKL Div:2.1349330\n",
      "Epoch   4\tIter   18\tLoss:2211.2480469\tRec Loss:2209.6733398\tKL Div:1.5748149\n",
      "Epoch   5\tIter    6\tLoss:2194.7678223\tRec Loss:2193.8752441\tKL Div:0.8925794\n",
      "Epoch   5\tIter   12\tLoss:2158.6616211\tRec Loss:2157.8823242\tKL Div:0.7791944\n",
      "Epoch   5\tIter   18\tLoss:2187.7683105\tRec Loss:2187.1855469\tKL Div:0.5826939\n",
      "Epoch   6\tIter    6\tLoss:2175.0712891\tRec Loss:2174.6557617\tKL Div:0.4154631\n",
      "Epoch   6\tIter   12\tLoss:2231.0808105\tRec Loss:2230.6337891\tKL Div:0.4469084\n",
      "Epoch   6\tIter   18\tLoss:2196.8134766\tRec Loss:2196.3730469\tKL Div:0.4403284\n",
      "Epoch   7\tIter    6\tLoss:2170.0705566\tRec Loss:2169.6472168\tKL Div:0.4232778\n",
      "Epoch   7\tIter   12\tLoss:2175.5273438\tRec Loss:2175.1467285\tKL Div:0.3806634\n",
      "Epoch   7\tIter   18\tLoss:2161.0686035\tRec Loss:2160.6523438\tKL Div:0.4163387\n",
      "Epoch   8\tIter    6\tLoss:2145.6743164\tRec Loss:2145.3088379\tKL Div:0.3655048\n",
      "Epoch   8\tIter   12\tLoss:2170.9892578\tRec Loss:2170.6083984\tKL Div:0.3808786\n",
      "Epoch   8\tIter   18\tLoss:2128.0664062\tRec Loss:2127.6811523\tKL Div:0.3852386\n",
      "Epoch   9\tIter    6\tLoss:2154.0903320\tRec Loss:2153.6687012\tKL Div:0.4216483\n",
      "Epoch   9\tIter   12\tLoss:2168.1425781\tRec Loss:2167.7167969\tKL Div:0.4257453\n",
      "Epoch   9\tIter   18\tLoss:2214.0627441\tRec Loss:2213.6611328\tKL Div:0.4015670\n",
      "Epoch  10\tIter    6\tLoss:2167.1501465\tRec Loss:2166.7255859\tKL Div:0.4245254\n",
      "Epoch  10\tIter   12\tLoss:2178.8710938\tRec Loss:2178.4191895\tKL Div:0.4518512\n",
      "Epoch  10\tIter   18\tLoss:2221.2529297\tRec Loss:2220.8437500\tKL Div:0.4091426\n",
      "Epoch  11\tIter    6\tLoss:2239.4672852\tRec Loss:2238.9487305\tKL Div:0.5184389\n",
      "Epoch  11\tIter   12\tLoss:2156.2749023\tRec Loss:2155.7993164\tKL Div:0.4756032\n",
      "Epoch  11\tIter   18\tLoss:2108.2819824\tRec Loss:2107.8566895\tKL Div:0.4254123\n",
      "Epoch  12\tIter    6\tLoss:2150.0693359\tRec Loss:2149.6320801\tKL Div:0.4373667\n",
      "Epoch  12\tIter   12\tLoss:2176.9035645\tRec Loss:2176.4777832\tKL Div:0.4257562\n",
      "Epoch  12\tIter   18\tLoss:2184.1979980\tRec Loss:2183.7421875\tKL Div:0.4557592\n",
      "Epoch  13\tIter    6\tLoss:2105.2309570\tRec Loss:2104.7751465\tKL Div:0.4558410\n",
      "Epoch  13\tIter   12\tLoss:2137.2565918\tRec Loss:2136.8041992\tKL Div:0.4524538\n",
      "Epoch  13\tIter   18\tLoss:2226.5014648\tRec Loss:2226.0585938\tKL Div:0.4428685\n",
      "Epoch  14\tIter    6\tLoss:2191.8466797\tRec Loss:2191.3237305\tKL Div:0.5230198\n",
      "Epoch  14\tIter   12\tLoss:2142.7775879\tRec Loss:2142.3491211\tKL Div:0.4285429\n",
      "Epoch  14\tIter   18\tLoss:2149.7519531\tRec Loss:2149.2475586\tKL Div:0.5042952\n",
      "Epoch  15\tIter    6\tLoss:2172.0979004\tRec Loss:2171.5795898\tKL Div:0.5183179\n",
      "Epoch  15\tIter   12\tLoss:2199.2766113\tRec Loss:2198.7741699\tKL Div:0.5024164\n",
      "Epoch  15\tIter   18\tLoss:2147.8254395\tRec Loss:2147.3769531\tKL Div:0.4485846\n",
      "Epoch  16\tIter    6\tLoss:2194.3103027\tRec Loss:2193.7722168\tKL Div:0.5381834\n",
      "Epoch  16\tIter   12\tLoss:2163.9665527\tRec Loss:2163.4418945\tKL Div:0.5245409\n",
      "Epoch  16\tIter   18\tLoss:2191.5925293\tRec Loss:2191.1132812\tKL Div:0.4791484\n",
      "Epoch  17\tIter    6\tLoss:2159.3210449\tRec Loss:2158.8312988\tKL Div:0.4897386\n",
      "Epoch  17\tIter   12\tLoss:2165.1994629\tRec Loss:2164.6479492\tKL Div:0.5516009\n",
      "Epoch  17\tIter   18\tLoss:2186.8120117\tRec Loss:2186.2773438\tKL Div:0.5347455\n",
      "Epoch  18\tIter    6\tLoss:2176.7075195\tRec Loss:2176.1269531\tKL Div:0.5804863\n",
      "Epoch  18\tIter   12\tLoss:2169.2692871\tRec Loss:2168.8085938\tKL Div:0.4607977\n",
      "Epoch  18\tIter   18\tLoss:2160.2761230\tRec Loss:2159.7602539\tKL Div:0.5158901\n",
      "Epoch  19\tIter    6\tLoss:2184.7353516\tRec Loss:2184.1579590\tKL Div:0.5773895\n",
      "Epoch  19\tIter   12\tLoss:2159.4582520\tRec Loss:2158.9304199\tKL Div:0.5277633\n",
      "Epoch  19\tIter   18\tLoss:2222.1284180\tRec Loss:2221.5566406\tKL Div:0.5716913\n",
      "Epoch  20\tIter    6\tLoss:2148.5893555\tRec Loss:2147.9577637\tKL Div:0.6316954\n",
      "Epoch  20\tIter   12\tLoss:2142.8247070\tRec Loss:2142.3137207\tKL Div:0.5109626\n",
      "Epoch  20\tIter   18\tLoss:2193.3039551\tRec Loss:2192.6879883\tKL Div:0.6158606\n",
      "Epoch  21\tIter    6\tLoss:2245.7316895\tRec Loss:2245.1147461\tKL Div:0.6168937\n",
      "Epoch  21\tIter   12\tLoss:2212.3962402\tRec Loss:2211.7768555\tKL Div:0.6192812\n",
      "Epoch  21\tIter   18\tLoss:2183.2661133\tRec Loss:2182.6520996\tKL Div:0.6140150\n",
      "Epoch  22\tIter    6\tLoss:2209.2490234\tRec Loss:2208.6633301\tKL Div:0.5857533\n",
      "Epoch  22\tIter   12\tLoss:2224.6635742\tRec Loss:2224.0073242\tKL Div:0.6562297\n",
      "Epoch  22\tIter   18\tLoss:2233.5136719\tRec Loss:2232.9033203\tKL Div:0.6104315\n",
      "Epoch  23\tIter    6\tLoss:2190.1770020\tRec Loss:2189.6223145\tKL Div:0.5546688\n",
      "Epoch  23\tIter   12\tLoss:2169.8322754\tRec Loss:2169.0292969\tKL Div:0.8029542\n",
      "Epoch  23\tIter   18\tLoss:2181.9572754\tRec Loss:2181.2998047\tKL Div:0.6574847\n",
      "Epoch  24\tIter    6\tLoss:2162.2009277\tRec Loss:2161.5493164\tKL Div:0.6516039\n",
      "Epoch  24\tIter   12\tLoss:2182.5993652\tRec Loss:2181.8520508\tKL Div:0.7472811\n",
      "Epoch  24\tIter   18\tLoss:2171.9526367\tRec Loss:2171.2624512\tKL Div:0.6901270\n",
      "Epoch  25\tIter    6\tLoss:2144.4245605\tRec Loss:2143.6342773\tKL Div:0.7903150\n",
      "Epoch  25\tIter   12\tLoss:2161.7163086\tRec Loss:2160.9914551\tKL Div:0.7249331\n",
      "Epoch  25\tIter   18\tLoss:2208.3344727\tRec Loss:2207.4633789\tKL Div:0.8711818\n",
      "Epoch  26\tIter    6\tLoss:2228.6584473\tRec Loss:2227.8239746\tKL Div:0.8343638\n",
      "Epoch  26\tIter   12\tLoss:2162.4904785\tRec Loss:2161.7348633\tKL Div:0.7555307\n",
      "Epoch  26\tIter   18\tLoss:2192.8989258\tRec Loss:2192.0024414\tKL Div:0.8964508\n",
      "Epoch  27\tIter    6\tLoss:2203.7287598\tRec Loss:2202.6435547\tKL Div:1.0852469\n",
      "Epoch  27\tIter   12\tLoss:2177.2607422\tRec Loss:2176.2753906\tKL Div:0.9854670\n",
      "Epoch  27\tIter   18\tLoss:2196.1318359\tRec Loss:2195.0864258\tKL Div:1.0454700\n",
      "Epoch  28\tIter    6\tLoss:2186.4580078\tRec Loss:2185.2519531\tKL Div:1.2061214\n",
      "Epoch  28\tIter   12\tLoss:2120.2939453\tRec Loss:2119.2158203\tKL Div:1.0781749\n",
      "Epoch  28\tIter   18\tLoss:2181.9267578\tRec Loss:2180.6708984\tKL Div:1.2559092\n",
      "Epoch  29\tIter    6\tLoss:2200.4948730\tRec Loss:2198.9741211\tKL Div:1.5207036\n",
      "Epoch  29\tIter   12\tLoss:2177.1237793\tRec Loss:2175.3896484\tKL Div:1.7342032\n",
      "Epoch  29\tIter   18\tLoss:2195.0307617\tRec Loss:2193.3278809\tKL Div:1.7029253\n",
      "Epoch  30\tIter    6\tLoss:2154.6655273\tRec Loss:2152.9035645\tKL Div:1.7619672\n",
      "Epoch  30\tIter   12\tLoss:2158.1486816\tRec Loss:2156.6103516\tKL Div:1.5382187\n",
      "Epoch  30\tIter   18\tLoss:2187.0261230\tRec Loss:2185.3554688\tKL Div:1.6706781\n",
      "Epoch  31\tIter    6\tLoss:2170.7741699\tRec Loss:2168.8437500\tKL Div:1.9303885\n",
      "Epoch  31\tIter   12\tLoss:2151.6772461\tRec Loss:2149.6367188\tKL Div:2.0406365\n",
      "Epoch  31\tIter   18\tLoss:2194.9687500\tRec Loss:2192.8664551\tKL Div:2.1022735\n",
      "Epoch  32\tIter    6\tLoss:2182.2023926\tRec Loss:2180.0556641\tKL Div:2.1467018\n",
      "Epoch  32\tIter   12\tLoss:2156.5253906\tRec Loss:2154.4243164\tKL Div:2.1009760\n",
      "Epoch  32\tIter   18\tLoss:2192.1062012\tRec Loss:2189.9660645\tKL Div:2.1400223\n",
      "Epoch  33\tIter    6\tLoss:2181.0629883\tRec Loss:2179.2102051\tKL Div:1.8528008\n",
      "Epoch  33\tIter   12\tLoss:2143.7143555\tRec Loss:2141.7534180\tKL Div:1.9609381\n",
      "Epoch  33\tIter   18\tLoss:2131.2214355\tRec Loss:2129.1220703\tKL Div:2.0992866\n",
      "Epoch  34\tIter    6\tLoss:2194.7448730\tRec Loss:2192.9785156\tKL Div:1.7662551\n",
      "Epoch  34\tIter   12\tLoss:2159.7475586\tRec Loss:2157.8173828\tKL Div:1.9301274\n",
      "Epoch  34\tIter   18\tLoss:2137.1062012\tRec Loss:2135.0224609\tKL Div:2.0837245\n",
      "Epoch  35\tIter    6\tLoss:2155.8386230\tRec Loss:2153.9370117\tKL Div:1.9016094\n",
      "Epoch  35\tIter   12\tLoss:2198.7944336\tRec Loss:2196.8032227\tKL Div:1.9911819\n",
      "Epoch  35\tIter   18\tLoss:2135.3154297\tRec Loss:2133.0402832\tKL Div:2.2752414\n",
      "Epoch  36\tIter    6\tLoss:2178.1120605\tRec Loss:2175.6484375\tKL Div:2.4636607\n",
      "Epoch  36\tIter   12\tLoss:2151.5534668\tRec Loss:2149.2597656\tKL Div:2.2936356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  36\tIter   18\tLoss:2183.5251465\tRec Loss:2181.3105469\tKL Div:2.2145827\n",
      "Epoch  37\tIter    6\tLoss:2239.6206055\tRec Loss:2237.4179688\tKL Div:2.2026343\n",
      "Epoch  37\tIter   12\tLoss:2153.5051270\tRec Loss:2151.2211914\tKL Div:2.2838852\n",
      "Epoch  37\tIter   18\tLoss:2124.1706543\tRec Loss:2121.7749023\tKL Div:2.3956654\n",
      "Epoch  38\tIter    6\tLoss:2180.9082031\tRec Loss:2178.4326172\tKL Div:2.4755754\n",
      "Epoch  38\tIter   12\tLoss:2183.3378906\tRec Loss:2180.8061523\tKL Div:2.5317082\n",
      "Epoch  38\tIter   18\tLoss:2106.7446289\tRec Loss:2104.2968750\tKL Div:2.4478559\n",
      "Epoch  39\tIter    6\tLoss:2155.3757324\tRec Loss:2152.7719727\tKL Div:2.6038165\n",
      "Epoch  39\tIter   12\tLoss:2115.5036621\tRec Loss:2113.1035156\tKL Div:2.4001021\n",
      "Epoch  39\tIter   18\tLoss:2173.7990723\tRec Loss:2171.3398438\tKL Div:2.4592397\n",
      "Epoch  40\tIter    6\tLoss:2171.4216309\tRec Loss:2168.7556152\tKL Div:2.6661184\n",
      "Epoch  40\tIter   12\tLoss:2165.2583008\tRec Loss:2162.4218750\tKL Div:2.8364954\n",
      "Epoch  40\tIter   18\tLoss:2147.6325684\tRec Loss:2145.1354980\tKL Div:2.4971211\n",
      "Epoch  40\tLoss:2167.5240421\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238,\n",
      "        13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['\\xa0', 'love', 'line', 'beat', 'live', 'lyric', 'world', 'turn', 'debut', 'melody', 'close', 'year', 'early', 'group', 'set']\n",
      "['\\xa0', 'love', 'lyric', 'line', 'live', 'beat', 'world', 'turn', 'year', 'debut', 'big', 'group', 'close', 'early', 'day']\n",
      "['\\xa0', 'love', 'lyric', 'line', 'live', 'sing', 'life', 'big', 'turn', 'day', 'leave', 'year', 'debut', 'man', 'world']\n",
      "['\\xa0', 'love', 'line', 'lyric', 'live', 'beat', 'world', 'year', 'turn', 'group', 'close', 'melody', 'debut', 'early', 'big']\n",
      "['\\xa0', 'electronic', 'piece', 'beat', 'synth', 'bass', 'melody', 'drum', 'sense', 'noise', 'set', 'mix', 'drone', 'label', 'dance']\n",
      "['\\xa0', 'love', 'line', 'lyric', 'beat', 'live', 'world', 'turn', 'year', 'debut', 'close', 'group', 'set', 'melody', 'leave']\n",
      "['\\xa0', 'love', 'line', 'lyric', 'beat', 'live', 'world', 'year', 'turn', 'melody', 'group', 'debut', 'close', 'set', 'big']\n",
      "['\\xa0', 'love', 'line', 'lyric', 'beat', 'live', 'year', 'world', 'turn', 'big', 'group', 'close', 'debut', 'melody', 'title']\n",
      "['\\xa0', 'love', 'lyric', 'line', 'live', 'beat', 'world', 'turn', 'close', 'year', 'group', 'big', 'debut', 'melody', 'early']\n",
      "['\\xa0', 'love', 'lyric', 'line', 'live', 'year', 'beat', 'debut', 'world', 'turn', 'big', 'early', 'close', 'life', 'day']\n",
      "['\\xa0', 'love', 'line', 'lyric', 'beat', 'live', 'melody', 'world', 'close', 'turn', 'group', 'early', 'year', 'big', 'debut']\n",
      "['\\xa0', 'love', 'line', 'lyric', 'melody', 'live', 'group', 'world', 'beat', 'turn', 'close', 'debut', 'year', 'big', 'early']\n",
      "['\\xa0', 'love', 'melody', 'lyric', 'line', 'close', 'live', 'beat', 'group', 'turn', 'world', 'debut', 'set', 'year', 'early']\n",
      "['\\xa0', 'love', 'line', 'lyric', 'beat', 'live', 'world', 'year', 'turn', 'close', 'early', 'group', 'debut', 'big', 'melody']\n",
      "['\\xa0', 'melody', 'love', 'drum', 'line', 'beat', 'group', 'close', 'set', 'kind', 'live', 'turn', 'early', 'world', 'ep']\n",
      "['\\xa0', 'rap', 'love', 'life', 'man', 'year', 'beat', 'rapper', 'people', 'live', 'world', 'big', 'line', 'day', 'write']\n",
      "['\\xa0', 'love', 'line', 'beat', 'melody', 'live', 'group', 'turn', 'lyric', 'world', 'close', 'set', 'debut', 'year', 'kind']\n",
      "['\\xa0', 'love', 'line', 'lyric', 'live', 'beat', 'world', 'turn', 'debut', 'melody', 'year', 'big', 'group', 'close', 'early']\n",
      "['\\xa0', 'love', 'line', 'lyric', 'world', 'live', 'beat', 'turn', 'year', 'big', 'close', 'debut', 'group', 'early', 'leave']\n",
      "['\\xa0', 'love', 'lyric', 'line', 'live', 'world', 'beat', 'turn', 'big', 'debut', 'year', 'close', 'group', 'set', 'life']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238,\n",
      "        13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238,\n",
      "        13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.13\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      " \n",
      "love\n",
      "line\n",
      "beat\n",
      "live\n",
      "lyric\n",
      "world\n",
      "turn\n",
      "debut\n",
      "melody\n",
      "close\n",
      "year\n",
      "early\n",
      "group\n",
      "set\n",
      "coherence c_uci -0.13994571102730458\n",
      "coherence c_npmi -0.01382391070018373\n",
      "coherence c_cv 0.2655290560583131\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238,\n",
      "        13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238, 13238],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch  41\tIter    6\tLoss:2124.9936523\tRec Loss:2122.2392578\tKL Div:2.7543721\n",
      "Epoch  41\tIter   12\tLoss:2122.0043945\tRec Loss:2119.4042969\tKL Div:2.6000333\n",
      "Epoch  41\tIter   18\tLoss:2159.8442383\tRec Loss:2157.0361328\tKL Div:2.8081603\n",
      "Epoch  42\tIter    6\tLoss:2171.9575195\tRec Loss:2169.3154297\tKL Div:2.6421213\n",
      "Epoch  42\tIter   12\tLoss:2147.5351562\tRec Loss:2144.5019531\tKL Div:3.0332379\n",
      "Epoch  42\tIter   18\tLoss:2125.5441895\tRec Loss:2122.7456055\tKL Div:2.7984719\n",
      "Epoch  43\tIter    6\tLoss:2161.5637207\tRec Loss:2158.5822754\tKL Div:2.9815478\n",
      "Epoch  43\tIter   12\tLoss:2133.1083984\tRec Loss:2130.2163086\tKL Div:2.8921018\n",
      "Epoch  43\tIter   18\tLoss:2177.1477051\tRec Loss:2174.3620605\tKL Div:2.7856474\n",
      "Epoch  44\tIter    6\tLoss:2156.1843262\tRec Loss:2153.1809082\tKL Div:3.0034752\n",
      "Epoch  44\tIter   12\tLoss:2186.8515625\tRec Loss:2183.9291992\tKL Div:2.9223866\n",
      "Epoch  44\tIter   18\tLoss:2146.6274414\tRec Loss:2143.6616211\tKL Div:2.9658346\n",
      "Epoch  45\tIter    6\tLoss:2187.8364258\tRec Loss:2184.6787109\tKL Div:3.1576629\n",
      "Epoch  45\tIter   12\tLoss:2167.9299316\tRec Loss:2164.7373047\tKL Div:3.1925287\n",
      "Epoch  45\tIter   18\tLoss:2209.8371582\tRec Loss:2206.5747070\tKL Div:3.2625537\n",
      "Epoch  46\tIter    6\tLoss:2171.8295898\tRec Loss:2168.4050293\tKL Div:3.4246368\n",
      "Epoch  46\tIter   12\tLoss:2186.0527344\tRec Loss:2182.6816406\tKL Div:3.3710589\n",
      "Epoch  46\tIter   18\tLoss:2157.5715332\tRec Loss:2154.3657227\tKL Div:3.2058778\n",
      "Epoch  47\tIter    6\tLoss:2147.6801758\tRec Loss:2144.4633789\tKL Div:3.2168236\n",
      "Epoch  47\tIter   12\tLoss:2167.1169434\tRec Loss:2163.7868652\tKL Div:3.3301716\n",
      "Epoch  47\tIter   18\tLoss:2163.8911133\tRec Loss:2160.6538086\tKL Div:3.2372546\n",
      "Epoch  48\tIter    6\tLoss:2151.7314453\tRec Loss:2148.6579590\tKL Div:3.0734384\n",
      "Epoch  48\tIter   12\tLoss:2125.1169434\tRec Loss:2121.6181641\tKL Div:3.4988837\n",
      "Epoch  48\tIter   18\tLoss:2202.1389160\tRec Loss:2198.7177734\tKL Div:3.4210482\n",
      "Epoch  49\tIter    6\tLoss:2091.3010254\tRec Loss:2087.7541504\tKL Div:3.5467584\n",
      "Epoch  49\tIter   12\tLoss:2161.6777344\tRec Loss:2158.2685547\tKL Div:3.4092679\n",
      "Epoch  49\tIter   18\tLoss:2123.4301758\tRec Loss:2119.7727051\tKL Div:3.6574993\n",
      "Epoch  50\tIter    6\tLoss:2171.5788574\tRec Loss:2167.7993164\tKL Div:3.7795370\n",
      "Epoch  50\tIter   12\tLoss:2150.9677734\tRec Loss:2147.7885742\tKL Div:3.1790891\n",
      "Epoch  50\tIter   18\tLoss:2125.5847168\tRec Loss:2121.9196777\tKL Div:3.6649330\n",
      "Epoch  51\tIter    6\tLoss:2106.0876465\tRec Loss:2102.5637207\tKL Div:3.5239210\n",
      "Epoch  51\tIter   12\tLoss:2133.0439453\tRec Loss:2129.5932617\tKL Div:3.4506431\n",
      "Epoch  51\tIter   18\tLoss:2150.6071777\tRec Loss:2146.9133301\tKL Div:3.6938596\n",
      "Epoch  52\tIter    6\tLoss:2138.5632324\tRec Loss:2134.7858887\tKL Div:3.7773805\n",
      "Epoch  52\tIter   12\tLoss:2197.5122070\tRec Loss:2194.0798340\tKL Div:3.4324853\n",
      "Epoch  52\tIter   18\tLoss:2149.8112793\tRec Loss:2146.0761719\tKL Div:3.7350397\n",
      "Epoch  53\tIter    6\tLoss:2153.0305176\tRec Loss:2149.2993164\tKL Div:3.7311029\n",
      "Epoch  53\tIter   12\tLoss:2155.9384766\tRec Loss:2152.3544922\tKL Div:3.5839329\n",
      "Epoch  53\tIter   18\tLoss:2169.3837891\tRec Loss:2165.6752930\tKL Div:3.7085457\n",
      "Epoch  54\tIter    6\tLoss:2166.9230957\tRec Loss:2163.2924805\tKL Div:3.6306496\n",
      "Epoch  54\tIter   12\tLoss:2141.8356934\tRec Loss:2138.0874023\tKL Div:3.7482080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  54\tIter   18\tLoss:2167.5783691\tRec Loss:2163.7221680\tKL Div:3.8562150\n",
      "Epoch  55\tIter    6\tLoss:2179.6130371\tRec Loss:2175.9311523\tKL Div:3.6818950\n",
      "Epoch  55\tIter   12\tLoss:2181.6918945\tRec Loss:2177.6726074\tKL Div:4.0193920\n",
      "Epoch  55\tIter   18\tLoss:2159.2434082\tRec Loss:2155.7500000\tKL Div:3.4935269\n",
      "Epoch  56\tIter    6\tLoss:2158.6513672\tRec Loss:2154.7773438\tKL Div:3.8739166\n",
      "Epoch  56\tIter   12\tLoss:2179.3840332\tRec Loss:2175.5737305\tKL Div:3.8102641\n",
      "Epoch  56\tIter   18\tLoss:2166.4609375\tRec Loss:2162.7321777\tKL Div:3.7287478\n",
      "Epoch  57\tIter    6\tLoss:2169.1457520\tRec Loss:2165.3583984\tKL Div:3.7874174\n",
      "Epoch  57\tIter   12\tLoss:2196.3056641\tRec Loss:2192.3466797\tKL Div:3.9590349\n",
      "Epoch  57\tIter   18\tLoss:2112.6333008\tRec Loss:2109.0705566\tKL Div:3.5627234\n",
      "Epoch  58\tIter    6\tLoss:2175.6884766\tRec Loss:2171.9020996\tKL Div:3.7863455\n",
      "Epoch  58\tIter   12\tLoss:2135.4011230\tRec Loss:2131.6784668\tKL Div:3.7225504\n",
      "Epoch  58\tIter   18\tLoss:2125.6176758\tRec Loss:2121.9086914\tKL Div:3.7090657\n",
      "Epoch  59\tIter    6\tLoss:2181.6613770\tRec Loss:2177.6318359\tKL Div:4.0295601\n",
      "Epoch  59\tIter   12\tLoss:2180.4099121\tRec Loss:2176.6423340\tKL Div:3.7676282\n",
      "Epoch  59\tIter   18\tLoss:2159.0219727\tRec Loss:2155.1770020\tKL Div:3.8449326\n",
      "Epoch  60\tIter    6\tLoss:2187.3518066\tRec Loss:2183.3388672\tKL Div:4.0130038\n",
      "Epoch  60\tIter   12\tLoss:2161.3818359\tRec Loss:2157.5556641\tKL Div:3.8261745\n",
      "Epoch  60\tIter   18\tLoss:2138.4785156\tRec Loss:2134.5593262\tKL Div:3.9192207\n",
      "Epoch  61\tIter    6\tLoss:2185.1000977\tRec Loss:2180.9853516\tKL Div:4.1147165\n",
      "Epoch  61\tIter   12\tLoss:2169.4858398\tRec Loss:2165.4389648\tKL Div:4.0469356\n",
      "Epoch  61\tIter   18\tLoss:2158.7019043\tRec Loss:2154.8571777\tKL Div:3.8448324\n",
      "Epoch  62\tIter    6\tLoss:2214.4648438\tRec Loss:2210.4262695\tKL Div:4.0386281\n",
      "Epoch  62\tIter   12\tLoss:2161.8681641\tRec Loss:2157.6826172\tKL Div:4.1854625\n",
      "Epoch  62\tIter   18\tLoss:2152.6362305\tRec Loss:2148.7968750\tKL Div:3.8392458\n",
      "Epoch  63\tIter    6\tLoss:2171.8999023\tRec Loss:2167.6235352\tKL Div:4.2762785\n",
      "Epoch  63\tIter   12\tLoss:2123.6181641\tRec Loss:2119.6665039\tKL Div:3.9517670\n",
      "Epoch  63\tIter   18\tLoss:2180.3581543\tRec Loss:2176.2629395\tKL Div:4.0952611\n",
      "Epoch  64\tIter    6\tLoss:2173.9265137\tRec Loss:2169.8974609\tKL Div:4.0291028\n",
      "Epoch  64\tIter   12\tLoss:2139.3908691\tRec Loss:2135.1328125\tKL Div:4.2580824\n",
      "Epoch  64\tIter   18\tLoss:2165.8337402\tRec Loss:2161.9174805\tKL Div:3.9162598\n",
      "Epoch  65\tIter    6\tLoss:2169.7287598\tRec Loss:2165.6821289\tKL Div:4.0466948\n",
      "Epoch  65\tIter   12\tLoss:2176.2956543\tRec Loss:2172.1555176\tKL Div:4.1402564\n",
      "Epoch  65\tIter   18\tLoss:2154.6987305\tRec Loss:2150.6469727\tKL Div:4.0516381\n",
      "Epoch  66\tIter    6\tLoss:2153.4882812\tRec Loss:2149.1665039\tKL Div:4.3217564\n",
      "Epoch  66\tIter   12\tLoss:2151.3356934\tRec Loss:2147.0888672\tKL Div:4.2467775\n",
      "Epoch  66\tIter   18\tLoss:2163.6337891\tRec Loss:2159.4394531\tKL Div:4.1942587\n",
      "Epoch  67\tIter    6\tLoss:2124.5961914\tRec Loss:2120.3017578\tKL Div:4.2943954\n",
      "Epoch  67\tIter   12\tLoss:2143.8469238\tRec Loss:2139.5798340\tKL Div:4.2670460\n",
      "Epoch  67\tIter   18\tLoss:2202.4367676\tRec Loss:2197.9179688\tKL Div:4.5188532\n",
      "Epoch  68\tIter    6\tLoss:2160.6660156\tRec Loss:2156.3020020\tKL Div:4.3640366\n",
      "Epoch  68\tIter   12\tLoss:2168.6604004\tRec Loss:2164.4340820\tKL Div:4.2263799\n",
      "Epoch  68\tIter   18\tLoss:2154.4006348\tRec Loss:2150.1127930\tKL Div:4.2877259\n",
      "Epoch  69\tIter    6\tLoss:2137.9833984\tRec Loss:2133.6386719\tKL Div:4.3448014\n",
      "Epoch  69\tIter   12\tLoss:2173.3242188\tRec Loss:2168.7470703\tKL Div:4.5772610\n",
      "Epoch  69\tIter   18\tLoss:2185.9875488\tRec Loss:2181.7138672\tKL Div:4.2736311\n",
      "Epoch  70\tIter    6\tLoss:2164.4548340\tRec Loss:2159.9033203\tKL Div:4.5514517\n",
      "Epoch  70\tIter   12\tLoss:2142.3999023\tRec Loss:2137.9406738\tKL Div:4.4591179\n",
      "Epoch  70\tIter   18\tLoss:2130.6623535\tRec Loss:2126.2275391\tKL Div:4.4347839\n",
      "Epoch  71\tIter    6\tLoss:2202.9924316\tRec Loss:2198.6547852\tKL Div:4.3377333\n",
      "Epoch  71\tIter   12\tLoss:2126.5961914\tRec Loss:2121.9780273\tKL Div:4.6180534\n",
      "Epoch  71\tIter   18\tLoss:2165.0380859\tRec Loss:2160.6403809\tKL Div:4.3977251\n",
      "Epoch  72\tIter    6\tLoss:2163.6342773\tRec Loss:2159.1489258\tKL Div:4.4854145\n",
      "Epoch  72\tIter   12\tLoss:2138.1950684\tRec Loss:2133.5395508\tKL Div:4.6556106\n",
      "Epoch  72\tIter   18\tLoss:2192.2023926\tRec Loss:2187.6069336\tKL Div:4.5955696\n",
      "Epoch  73\tIter    6\tLoss:2157.4824219\tRec Loss:2152.7978516\tKL Div:4.6844492\n",
      "Epoch  73\tIter   12\tLoss:2111.2563477\tRec Loss:2106.7067871\tKL Div:4.5496783\n",
      "Epoch  73\tIter   18\tLoss:2094.5688477\tRec Loss:2090.0991211\tKL Div:4.4697385\n",
      "Epoch  74\tIter    6\tLoss:2178.3020020\tRec Loss:2173.5148926\tKL Div:4.7871652\n",
      "Epoch  74\tIter   12\tLoss:2158.3747559\tRec Loss:2153.6396484\tKL Div:4.7351847\n",
      "Epoch  74\tIter   18\tLoss:2240.3256836\tRec Loss:2235.6210938\tKL Div:4.7045450\n",
      "Epoch  75\tIter    6\tLoss:2165.4262695\tRec Loss:2160.5566406\tKL Div:4.8696537\n",
      "Epoch  75\tIter   12\tLoss:2140.2253418\tRec Loss:2135.5727539\tKL Div:4.6525884\n",
      "Epoch  75\tIter   18\tLoss:2154.9514160\tRec Loss:2150.3828125\tKL Div:4.5686579\n",
      "Epoch  76\tIter    6\tLoss:2166.4997559\tRec Loss:2161.4465332\tKL Div:5.0531597\n",
      "Epoch  76\tIter   12\tLoss:2194.3940430\tRec Loss:2189.6347656\tKL Div:4.7592993\n",
      "Epoch  76\tIter   18\tLoss:2131.2556152\tRec Loss:2126.5541992\tKL Div:4.7013588\n",
      "Epoch  77\tIter    6\tLoss:2187.1999512\tRec Loss:2182.2207031\tKL Div:4.9793363\n",
      "Epoch  77\tIter   12\tLoss:2150.6684570\tRec Loss:2146.0561523\tKL Div:4.6123409\n",
      "Epoch  77\tIter   18\tLoss:2167.3962402\tRec Loss:2162.4626465\tKL Div:4.9335494\n",
      "Epoch  78\tIter    6\tLoss:2182.0061035\tRec Loss:2177.0747070\tKL Div:4.9314871\n",
      "Epoch  78\tIter   12\tLoss:2133.6269531\tRec Loss:2128.8999023\tKL Div:4.7269592\n",
      "Epoch  78\tIter   18\tLoss:2134.9716797\tRec Loss:2129.8576660\tKL Div:5.1140070\n",
      "Epoch  79\tIter    6\tLoss:2128.5878906\tRec Loss:2123.6562500\tKL Div:4.9315214\n",
      "Epoch  79\tIter   12\tLoss:2169.8120117\tRec Loss:2164.7895508\tKL Div:5.0224457\n",
      "Epoch  79\tIter   18\tLoss:2200.5756836\tRec Loss:2195.4841309\tKL Div:5.0914831\n",
      "Epoch  80\tIter    6\tLoss:2136.6137695\tRec Loss:2131.6062012\tKL Div:5.0076709\n",
      "Epoch  80\tIter   12\tLoss:2124.4582520\tRec Loss:2119.5087891\tKL Div:4.9494519\n",
      "Epoch  80\tIter   18\tLoss:2125.2006836\tRec Loss:2120.0598145\tKL Div:5.1408510\n",
      "Epoch  80\tLoss:2151.7954537\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([13238,    97,    95, 13238,   526, 13238, 13238, 13238, 13238,    97,\n",
      "        13238,    97, 13238, 13238,   751,  1295, 13238, 13238,   131, 13238],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['\\xa0', 'love', 'line', 'world', 'turn', 'debut', 'kind', 'year', 'close', 'title', 'point', 'early', 'melody', 'beat', 'life']\n",
      "['love', '\\xa0', 'world', 'line', 'turn', 'lyric', 'debut', 'year', 'life', 'title', 'day', 'kind', 'close', 'big', 'leave']\n",
      "['live', 'version', 'cover', 'set', 'disc', 'love', 'single', 'record', 'include', 'man', 'people', 'early', 'group', 'hit', 'studio']\n",
      "['\\xa0', 'love', 'line', 'world', 'turn', 'year', 'close', 'life', 'kind', 'debut', 'leave', 'title', 'early', 'melody', 'live']\n",
      "['electronic', 'beat', 'dance', 'synth', 'mix', 'house', 'bass', 'label', 'artist', 'producer', '\\xa0', 'set', 'piece', 'techno', 'sample']\n",
      "['\\xa0', 'love', 'world', 'line', 'turn', 'year', 'kind', 'debut', 'close', 'life', 'title', 'single', 'leave', 'point', 'big']\n",
      "['\\xa0', 'love', 'synth', 'world', 'line', 'artist', 'kind', 'single', 'debut', 'life', 'ep', 'year', 'turn', 'dance', 'sense']\n",
      "['\\xa0', 'love', 'line', 'world', 'year', 'turn', 'title', 'kind', 'debut', 'life', 'close', 'big', 'leave', 'lyric', 'point']\n",
      "['\\xa0', 'love', 'line', 'world', 'turn', 'kind', 'year', 'close', 'debut', 'life', 'big', 'title', 'point', 'early', 'leave']\n",
      "['love', '\\xa0', 'lyric', 'line', 'world', 'life', 'debut', 'turn', 'year', 'title', 'day', 'close', 'big', 'kind', 'man']\n",
      "['\\xa0', 'love', 'world', 'line', 'turn', 'kind', 'melody', 'close', 'year', 'title', 'early', 'point', 'live', 'debut', 'sense']\n",
      "['love', '\\xa0', 'lyric', 'sing', 'life', 'write', 'line', 'girl', 'chorus', 'people', 'big', 'word', 'world', 'indie', 'day']\n",
      "['\\xa0', 'melody', 'lyric', 'sing', 'love', 'line', 'debut', 'close', 'chorus', 'title', 'word', 'kind', 'sense', 'leave', 'turn']\n",
      "['\\xa0', 'love', 'world', 'turn', 'year', 'line', 'kind', 'early', 'beat', 'single', 'live', 'set', 'life', 'title', 'close']\n",
      "['piece', 'noise', 'drone', 'drum', 'group', 'melody', 'close', 'tone', 'bass', 'metal', 'solo', 'piano', 'build', 'note', 'instrumental']\n",
      "['rap', 'beat', 'hip_hop', 'rapper', 'year', 'production', 'producer', 'verse', 'mixtape', 'artist', 'life', 'big', 'hit', 'love', 'line']\n",
      "['\\xa0', 'love', 'line', 'world', 'melody', 'turn', 'kind', 'close', 'debut', 'beat', 'sense', 'year', 'title', 'point', 'synth']\n",
      "['\\xa0', 'love', 'line', 'world', 'turn', 'debut', 'year', 'kind', 'life', 'close', 'title', 'melody', 'big', 'lyric', 'leave']\n",
      "['punk', 'band', 'riff', 'lyric', 'man', 'love', 'metal', 'line', 'cover', 'day', 'turn', 'world', 'debut', 'title', 'blue']\n",
      "['\\xa0', 'love', 'world', 'line', 'turn', 'life', 'year', 'debut', 'kind', 'close', 'big', 'title', 'single', 'leave', 'people']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([13238,    97,    95, 13238,   526, 13238, 13238, 13238, 13238,    97,\n",
      "        13238,    97, 13238, 13238,   751,  1295, 13238, 13238,   131, 13238],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([13238,    97,    95, 13238,   526, 13238, 13238, 13238, 13238,    97,\n",
      "        13238,    97, 13238, 13238,   751,  1295, 13238, 13238,   131, 13238],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.24\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      " \n",
      "love\n",
      "line\n",
      "world\n",
      "turn\n",
      "debut\n",
      "kind\n",
      "year\n",
      "close\n",
      "title\n",
      "point\n",
      "early\n",
      "melody\n",
      "beat\n",
      "life\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence c_uci -0.06046248307067775\n",
      "coherence c_npmi -0.0033205613730262775\n",
      "coherence c_cv 0.3027161108544133\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([13238,    97,    95, 13238,   526, 13238, 13238, 13238, 13238,    97,\n",
      "        13238,    97, 13238, 13238,   751,  1295, 13238, 13238,   131, 13238],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch  81\tIter    6\tLoss:2141.6706543\tRec Loss:2136.5075684\tKL Div:5.1631513\n",
      "Epoch  81\tIter   12\tLoss:2116.7944336\tRec Loss:2111.9721680\tKL Div:4.8223066\n",
      "Epoch  81\tIter   18\tLoss:2130.6413574\tRec Loss:2125.5720215\tKL Div:5.0693836\n",
      "Epoch  82\tIter    6\tLoss:2171.9548340\tRec Loss:2166.8662109\tKL Div:5.0887318\n",
      "Epoch  82\tIter   12\tLoss:2137.0268555\tRec Loss:2132.0505371\tKL Div:4.9763222\n",
      "Epoch  82\tIter   18\tLoss:2175.9208984\tRec Loss:2170.5456543\tKL Div:5.3751926\n",
      "Epoch  83\tIter    6\tLoss:2127.7221680\tRec Loss:2122.6359863\tKL Div:5.0862350\n",
      "Epoch  83\tIter   12\tLoss:2165.8784180\tRec Loss:2160.8666992\tKL Div:5.0116129\n",
      "Epoch  83\tIter   18\tLoss:2146.5979004\tRec Loss:2141.3210449\tKL Div:5.2769580\n",
      "Epoch  84\tIter    6\tLoss:2167.5593262\tRec Loss:2162.3676758\tKL Div:5.1916332\n",
      "Epoch  84\tIter   12\tLoss:2159.2919922\tRec Loss:2154.0908203\tKL Div:5.2011590\n",
      "Epoch  84\tIter   18\tLoss:2168.6062012\tRec Loss:2163.4267578\tKL Div:5.1794620\n",
      "Epoch  85\tIter    6\tLoss:2169.9909668\tRec Loss:2164.7451172\tKL Div:5.2458019\n",
      "Epoch  85\tIter   12\tLoss:2185.1650391\tRec Loss:2180.0280762\tKL Div:5.1370616\n",
      "Epoch  85\tIter   18\tLoss:2167.0078125\tRec Loss:2161.5966797\tKL Div:5.4110913\n",
      "Epoch  86\tIter    6\tLoss:2164.8188477\tRec Loss:2159.5270996\tKL Div:5.2918324\n",
      "Epoch  86\tIter   12\tLoss:2119.0795898\tRec Loss:2113.9409180\tKL Div:5.1386375\n",
      "Epoch  86\tIter   18\tLoss:2161.8100586\tRec Loss:2156.5400391\tKL Div:5.2699809\n",
      "Epoch  87\tIter    6\tLoss:2117.7111816\tRec Loss:2112.4738770\tKL Div:5.2373562\n",
      "Epoch  87\tIter   12\tLoss:2154.7561035\tRec Loss:2149.3618164\tKL Div:5.3943682\n",
      "Epoch  87\tIter   18\tLoss:2122.7963867\tRec Loss:2117.6699219\tKL Div:5.1263914\n",
      "Epoch  88\tIter    6\tLoss:2144.0632324\tRec Loss:2138.5161133\tKL Div:5.5472240\n",
      "Epoch  88\tIter   12\tLoss:2100.6760254\tRec Loss:2095.3120117\tKL Div:5.3639221\n",
      "Epoch  88\tIter   18\tLoss:2188.9519043\tRec Loss:2183.6003418\tKL Div:5.3514585\n",
      "Epoch  89\tIter    6\tLoss:2135.5725098\tRec Loss:2130.0424805\tKL Div:5.5301161\n",
      "Epoch  89\tIter   12\tLoss:2132.3784180\tRec Loss:2127.1911621\tKL Div:5.1873031\n",
      "Epoch  89\tIter   18\tLoss:2140.4484863\tRec Loss:2134.9165039\tKL Div:5.5320554\n",
      "Epoch  90\tIter    6\tLoss:2130.0146484\tRec Loss:2124.5805664\tKL Div:5.4340439\n",
      "Epoch  90\tIter   12\tLoss:2152.5175781\tRec Loss:2147.0627441\tKL Div:5.4548116\n",
      "Epoch  90\tIter   18\tLoss:2188.1115723\tRec Loss:2182.4682617\tKL Div:5.6434193\n",
      "Epoch  91\tIter    6\tLoss:2179.2824707\tRec Loss:2173.5766602\tKL Div:5.7057991\n",
      "Epoch  91\tIter   12\tLoss:2175.5119629\tRec Loss:2170.1340332\tKL Div:5.3778501\n",
      "Epoch  91\tIter   18\tLoss:2172.7983398\tRec Loss:2167.2182617\tKL Div:5.5800238\n",
      "Epoch  92\tIter    6\tLoss:2164.9377441\tRec Loss:2159.4970703\tKL Div:5.4406581\n",
      "Epoch  92\tIter   12\tLoss:2116.4665527\tRec Loss:2111.0593262\tKL Div:5.4072161\n",
      "Epoch  92\tIter   18\tLoss:2142.6687012\tRec Loss:2137.0488281\tKL Div:5.6197724\n",
      "Epoch  93\tIter    6\tLoss:2124.1240234\tRec Loss:2118.5559082\tKL Div:5.5680246\n",
      "Epoch  93\tIter   12\tLoss:2175.1064453\tRec Loss:2169.5922852\tKL Div:5.5141373\n",
      "Epoch  93\tIter   18\tLoss:2169.1665039\tRec Loss:2163.6591797\tKL Div:5.5073962\n",
      "Epoch  94\tIter    6\tLoss:2119.3425293\tRec Loss:2113.7314453\tKL Div:5.6109791\n",
      "Epoch  94\tIter   12\tLoss:2160.9440918\tRec Loss:2155.4428711\tKL Div:5.5011511\n",
      "Epoch  94\tIter   18\tLoss:2125.3110352\tRec Loss:2119.7341309\tKL Div:5.5768356\n",
      "Epoch  95\tIter    6\tLoss:2183.3769531\tRec Loss:2177.5869141\tKL Div:5.7899914\n",
      "Epoch  95\tIter   12\tLoss:2111.5046387\tRec Loss:2106.0354004\tKL Div:5.4691272\n",
      "Epoch  95\tIter   18\tLoss:2152.4035645\tRec Loss:2146.6884766\tKL Div:5.7151418\n",
      "Epoch  96\tIter    6\tLoss:2132.8066406\tRec Loss:2127.0478516\tKL Div:5.7586808\n",
      "Epoch  96\tIter   12\tLoss:2190.3232422\tRec Loss:2184.6269531\tKL Div:5.6961918\n",
      "Epoch  96\tIter   18\tLoss:2131.7878418\tRec Loss:2126.0957031\tKL Div:5.6921773\n",
      "Epoch  97\tIter    6\tLoss:2163.8088379\tRec Loss:2158.0031738\tKL Div:5.8057117\n",
      "Epoch  97\tIter   12\tLoss:2099.1181641\tRec Loss:2093.5261230\tKL Div:5.5920467\n",
      "Epoch  97\tIter   18\tLoss:2158.1267090\tRec Loss:2152.2519531\tKL Div:5.8746347\n",
      "Epoch  98\tIter    6\tLoss:2167.4812012\tRec Loss:2161.6396484\tKL Div:5.8415256\n",
      "Epoch  98\tIter   12\tLoss:2136.0236816\tRec Loss:2130.3041992\tKL Div:5.7194986\n",
      "Epoch  98\tIter   18\tLoss:2115.7695312\tRec Loss:2110.0288086\tKL Div:5.7406402\n",
      "Epoch  99\tIter    6\tLoss:2149.9729004\tRec Loss:2144.2082520\tKL Div:5.7647581\n",
      "Epoch  99\tIter   12\tLoss:2175.9665527\tRec Loss:2170.1003418\tKL Div:5.8663006\n",
      "Epoch  99\tIter   18\tLoss:2150.1379395\tRec Loss:2144.2958984\tKL Div:5.8419814\n",
      "Epoch 100\tIter    6\tLoss:2161.9719238\tRec Loss:2156.0876465\tKL Div:5.8842168\n",
      "Epoch 100\tIter   12\tLoss:2145.7238770\tRec Loss:2139.9184570\tKL Div:5.8054733\n",
      "Epoch 100\tIter   18\tLoss:2167.6772461\tRec Loss:2161.7001953\tKL Div:5.9770222\n",
      "Epoch 101\tIter    6\tLoss:2125.0002441\tRec Loss:2119.1560059\tKL Div:5.8442869\n",
      "Epoch 101\tIter   12\tLoss:2132.4719238\tRec Loss:2126.6250000\tKL Div:5.8468399\n",
      "Epoch 101\tIter   18\tLoss:2132.7519531\tRec Loss:2126.6916504\tKL Div:6.0604072\n",
      "Epoch 102\tIter    6\tLoss:2169.7788086\tRec Loss:2163.7998047\tKL Div:5.9788857\n",
      "Epoch 102\tIter   12\tLoss:2127.2290039\tRec Loss:2121.3164062\tKL Div:5.9125767\n",
      "Epoch 102\tIter   18\tLoss:2127.1867676\tRec Loss:2121.1655273\tKL Div:6.0212612\n",
      "Epoch 103\tIter    6\tLoss:2154.5341797\tRec Loss:2148.4628906\tKL Div:6.0713587\n",
      "Epoch 103\tIter   12\tLoss:2194.2182617\tRec Loss:2188.2033691\tKL Div:6.0149755\n",
      "Epoch 103\tIter   18\tLoss:2188.2536621\tRec Loss:2181.9443359\tKL Div:6.3092461\n",
      "Epoch 104\tIter    6\tLoss:2198.4091797\tRec Loss:2192.3671875\tKL Div:6.0419083\n",
      "Epoch 104\tIter   12\tLoss:2150.5732422\tRec Loss:2144.3549805\tKL Div:6.2182503\n",
      "Epoch 104\tIter   18\tLoss:2156.0495605\tRec Loss:2150.0639648\tKL Div:5.9856300\n",
      "Epoch 105\tIter    6\tLoss:2167.2827148\tRec Loss:2161.2150879\tKL Div:6.0675240\n",
      "Epoch 105\tIter   12\tLoss:2178.0759277\tRec Loss:2171.8635254\tKL Div:6.2122917\n",
      "Epoch 105\tIter   18\tLoss:2119.8979492\tRec Loss:2113.9111328\tKL Div:5.9867029\n",
      "Epoch 106\tIter    6\tLoss:2163.5788574\tRec Loss:2157.4155273\tKL Div:6.1633196\n",
      "Epoch 106\tIter   12\tLoss:2137.4990234\tRec Loss:2131.4448242\tKL Div:6.0541005\n",
      "Epoch 106\tIter   18\tLoss:2180.9565430\tRec Loss:2174.7124023\tKL Div:6.2441607\n",
      "Epoch 107\tIter    6\tLoss:2118.1254883\tRec Loss:2111.9604492\tKL Div:6.1650653\n",
      "Epoch 107\tIter   12\tLoss:2158.1845703\tRec Loss:2151.9731445\tKL Div:6.2114658\n",
      "Epoch 107\tIter   18\tLoss:2145.4357910\tRec Loss:2139.1953125\tKL Div:6.2404642\n",
      "Epoch 108\tIter    6\tLoss:2163.5461426\tRec Loss:2157.2509766\tKL Div:6.2952466\n",
      "Epoch 108\tIter   12\tLoss:2179.5598145\tRec Loss:2173.3627930\tKL Div:6.1970978\n",
      "Epoch 108\tIter   18\tLoss:2111.0666504\tRec Loss:2104.7946777\tKL Div:6.2719183\n",
      "Epoch 109\tIter    6\tLoss:2160.9455566\tRec Loss:2154.7294922\tKL Div:6.2161517\n",
      "Epoch 109\tIter   12\tLoss:2138.6511230\tRec Loss:2132.4482422\tKL Div:6.2028294\n",
      "Epoch 109\tIter   18\tLoss:2081.0664062\tRec Loss:2074.8750000\tKL Div:6.1913071\n",
      "Epoch 110\tIter    6\tLoss:2122.7446289\tRec Loss:2116.5183105\tKL Div:6.2262573\n",
      "Epoch 110\tIter   12\tLoss:2177.0490723\tRec Loss:2170.7900391\tKL Div:6.2590389\n",
      "Epoch 110\tIter   18\tLoss:2134.9047852\tRec Loss:2128.7338867\tKL Div:6.1708040\n",
      "Epoch 111\tIter    6\tLoss:2142.0397949\tRec Loss:2135.6657715\tKL Div:6.3739309\n",
      "Epoch 111\tIter   12\tLoss:2114.2612305\tRec Loss:2108.0588379\tKL Div:6.2023363\n",
      "Epoch 111\tIter   18\tLoss:2163.4541016\tRec Loss:2156.9863281\tKL Div:6.4676533\n",
      "Epoch 112\tIter    6\tLoss:2128.7021484\tRec Loss:2122.3964844\tKL Div:6.3057275\n",
      "Epoch 112\tIter   12\tLoss:2173.9182129\tRec Loss:2167.6245117\tKL Div:6.2937417\n",
      "Epoch 112\tIter   18\tLoss:2124.0546875\tRec Loss:2117.7001953\tKL Div:6.3545136\n",
      "Epoch 113\tIter    6\tLoss:2178.4470215\tRec Loss:2172.0031738\tKL Div:6.4439254\n",
      "Epoch 113\tIter   12\tLoss:2114.2211914\tRec Loss:2107.9379883\tKL Div:6.2833252\n",
      "Epoch 113\tIter   18\tLoss:2133.8479004\tRec Loss:2127.4082031\tKL Div:6.4395905\n",
      "Epoch 114\tIter    6\tLoss:2132.0078125\tRec Loss:2125.6191406\tKL Div:6.3886509\n",
      "Epoch 114\tIter   12\tLoss:2143.2038574\tRec Loss:2136.9848633\tKL Div:6.2190857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114\tIter   18\tLoss:2154.8447266\tRec Loss:2148.2666016\tKL Div:6.5781770\n",
      "Epoch 115\tIter    6\tLoss:2124.6674805\tRec Loss:2118.1828613\tKL Div:6.4845467\n",
      "Epoch 115\tIter   12\tLoss:2180.0595703\tRec Loss:2173.4794922\tKL Div:6.5800009\n",
      "Epoch 115\tIter   18\tLoss:2156.0659180\tRec Loss:2149.5434570\tKL Div:6.5224910\n",
      "Epoch 116\tIter    6\tLoss:2155.2563477\tRec Loss:2148.7460938\tKL Div:6.5103731\n",
      "Epoch 116\tIter   12\tLoss:2160.2800293\tRec Loss:2153.5949707\tKL Div:6.6850667\n",
      "Epoch 116\tIter   18\tLoss:2126.1638184\tRec Loss:2119.5861816\tKL Div:6.5776100\n",
      "Epoch 117\tIter    6\tLoss:2166.2617188\tRec Loss:2159.7285156\tKL Div:6.5331497\n",
      "Epoch 117\tIter   12\tLoss:2167.4167480\tRec Loss:2160.8999023\tKL Div:6.5167627\n",
      "Epoch 117\tIter   18\tLoss:2159.1938477\tRec Loss:2152.5749512\tKL Div:6.6190128\n",
      "Epoch 118\tIter    6\tLoss:2174.8190918\tRec Loss:2168.1586914\tKL Div:6.6603022\n",
      "Epoch 118\tIter   12\tLoss:2152.8154297\tRec Loss:2146.3186035\tKL Div:6.4967880\n",
      "Epoch 118\tIter   18\tLoss:2162.5305176\tRec Loss:2155.9182129\tKL Div:6.6121869\n",
      "Epoch 119\tIter    6\tLoss:2133.3242188\tRec Loss:2126.6501465\tKL Div:6.6740699\n",
      "Epoch 119\tIter   12\tLoss:2134.7336426\tRec Loss:2128.2099609\tKL Div:6.5237017\n",
      "Epoch 119\tIter   18\tLoss:2165.4602051\tRec Loss:2158.9179688\tKL Div:6.5421901\n",
      "Epoch 120\tIter    6\tLoss:2122.8457031\tRec Loss:2116.1127930\tKL Div:6.7329874\n",
      "Epoch 120\tIter   12\tLoss:2106.3613281\tRec Loss:2099.7519531\tKL Div:6.6094885\n",
      "Epoch 120\tIter   18\tLoss:2090.0856934\tRec Loss:2083.5964355\tKL Div:6.4892683\n",
      "Epoch 120\tLoss:2141.8202477\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([   97,    97,    95,    97,   823,    97, 13238,    97,    97,    99,\n",
      "          573,    97,  1654,   235,   751,  1295,   433,    27,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['love', 'line', 'debut', 'melody', 'title', 'kind', 'turn', 'close', 'point', 'beat', 'ep', 'synth', 'bit', 'big', 'world']\n",
      "['love', 'lyric', 'debut', 'line', 'big', 'title', 'turn', 'kind', 'melody', 'point', 'chorus', 'close', 'group', 'single', 'hard']\n",
      "['live', 'version', 'disc', 'cover', 'set', 'include', 'record', 'original', 'recording', 'studio', 'early', 'group', 'reissue', 'love', 'single']\n",
      "['love', 'line', 'turn', 'title', 'melody', 'close', 'debut', 'kind', 'world', 'point', 'year', 'big', 'leave', 'group', 'bit']\n",
      "['beat', 'mix', 'dance', 'house', 'electronic', 'label', 'bass', 'synth', 'producer', 'techno', 'artist', 'sample', 'drum', 'rhythm', 'set']\n",
      "['love', 'line', 'debut', 'turn', 'title', 'kind', 'point', 'big', 'close', 'single', 'world', 'year', 'melody', 'leave', 'hard']\n",
      "['\\xa0', 'love', 'project', 'sing', 'artist', 'point', 'lyric', 'title', 'sense', 'debut', 'producer', 'dream', 'interview', 'ep', 'single']\n",
      "['love', 'line', 'debut', 'title', 'melody', 'turn', 'kind', 'big', 'close', 'point', 'lyric', 'year', 'leave', 'bit', 'single']\n",
      "['love', 'line', 'turn', 'kind', 'debut', 'close', 'title', 'world', 'big', 'point', 'year', 'leave', 'single', 'melody', 'hard']\n",
      "['lyric', 'love', 'band', 'indie', 'chorus', 'girl', 'debut', 'bad', 'indie_rock', 'pretty', 'big', 'cover', 'get', 'line', 'guy']\n",
      "['melody', 'line', 'love', 'kind', 'title', 'close', 'turn', 'debut', 'point', 'beat', 'world', 'big', 'bit', 'year', 'single']\n",
      "['love', 'life', 'sing', 'write', 'world', 'man', 'people', 'woman', 'lyric', 'young', 'line', 'word', 'day', 'girl', 'death']\n",
      "['folk', 'sing', 'lyric', 'melody', 'piano', 'line', 'arrangement', 'love', 'solo', 'close', 'string', 'record', 'word', 'acoustic', 'light']\n",
      "['dance', 'single', 'disco', 'bad', 'big', 'party', 'fun', 'people', 'beat', 'guy', 'love', 'hit', 'get', 'pretty', 'girl']\n",
      "['piece', 'drone', 'noise', 'electronic', 'tone', 'instrument', 'sense', 'space', 'piano', 'create', 'world', 'composition', 'jazz', 'note', 'group']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'production', 'year', 'verse', 'mixtape', 'artist', 'producer', 'life', 'line', 'feature', 'flow', 'black']\n",
      "['synth', 'melody', 'ep', 'electronic', 'debut', 'beat', 'love', 'production', 'kind', 'line', 'close', 'sense', 'bit', 'drum', 'duo']\n",
      "['chorus', 'love', 'debut', 'lyric', 'indie', 'melody', 'hook', 'line', 'ep', 'title', 'synth', 'kind', 'big', 'turn', 'sort']\n",
      "['punk', 'metal', 'riff', 'band', 'drum', 'noise', 'bass', 'hardcore', 'drummer', 'group', 'guitarist', 'heavy', 'black', 'live', 'scream']\n",
      "['love', 'line', 'debut', 'big', 'lyric', 'turn', 'title', 'kind', 'single', 'point', 'close', 'world', 'melody', 'hard', 'leave']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([   97,    97,    95,    97,   823,    97, 13238,    97,    97,    99,\n",
      "          573,    97,  1654,   235,   751,  1295,   433,    27,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([   97,    97,    95,    97,   823,    97, 13238,    97,    97,    99,\n",
      "          573,    97,  1654,   235,   751,  1295,   433,    27,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.36666666666666664\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "love\n",
      "line\n",
      "debut\n",
      "melody\n",
      "title\n",
      "kind\n",
      "turn\n",
      "close\n",
      "point\n",
      "beat\n",
      "ep\n",
      "synth\n",
      "bit\n",
      "big\n",
      "world\n",
      "coherence c_uci -0.00811589509518466\n",
      "coherence c_npmi 0.004365082967837731\n",
      "coherence c_cv 0.33720715120524314\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([   97,    97,    95,    97,   823,    97, 13238,    97,    97,    99,\n",
      "          573,    97,  1654,   235,   751,  1295,   433,    27,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 121\tIter    6\tLoss:2134.0832520\tRec Loss:2127.5439453\tKL Div:6.5392809\n",
      "Epoch 121\tIter   12\tLoss:2138.9472656\tRec Loss:2132.2834473\tKL Div:6.6638432\n",
      "Epoch 121\tIter   18\tLoss:2118.8908691\tRec Loss:2112.4248047\tKL Div:6.4659805\n",
      "Epoch 122\tIter    6\tLoss:2151.1364746\tRec Loss:2144.2160645\tKL Div:6.9203110\n",
      "Epoch 122\tIter   12\tLoss:2112.5334473\tRec Loss:2106.0324707\tKL Div:6.5010595\n",
      "Epoch 122\tIter   18\tLoss:2151.7661133\tRec Loss:2144.9028320\tKL Div:6.8632121\n",
      "Epoch 123\tIter    6\tLoss:2118.8923340\tRec Loss:2112.1181641\tKL Div:6.7742290\n",
      "Epoch 123\tIter   12\tLoss:2099.1728516\tRec Loss:2092.4897461\tKL Div:6.6830430\n",
      "Epoch 123\tIter   18\tLoss:2151.9631348\tRec Loss:2145.3540039\tKL Div:6.6091518\n",
      "Epoch 124\tIter    6\tLoss:2144.1464844\tRec Loss:2137.2661133\tKL Div:6.8802586\n",
      "Epoch 124\tIter   12\tLoss:2124.0505371\tRec Loss:2117.4377441\tKL Div:6.6127119\n",
      "Epoch 124\tIter   18\tLoss:2072.8945312\tRec Loss:2066.0971680\tKL Div:6.7972736\n",
      "Epoch 125\tIter    6\tLoss:2137.4433594\tRec Loss:2130.7607422\tKL Div:6.6827316\n",
      "Epoch 125\tIter   12\tLoss:2144.2163086\tRec Loss:2137.3549805\tKL Div:6.8614049\n",
      "Epoch 125\tIter   18\tLoss:2171.9628906\tRec Loss:2165.2204590\tKL Div:6.7423906\n",
      "Epoch 126\tIter    6\tLoss:2110.8027344\tRec Loss:2103.9289551\tKL Div:6.8737121\n",
      "Epoch 126\tIter   12\tLoss:2174.5881348\tRec Loss:2167.8891602\tKL Div:6.6989985\n",
      "Epoch 126\tIter   18\tLoss:2144.6330566\tRec Loss:2137.8354492\tKL Div:6.7975082\n",
      "Epoch 127\tIter    6\tLoss:2161.8266602\tRec Loss:2154.9233398\tKL Div:6.9033370\n",
      "Epoch 127\tIter   12\tLoss:2151.1623535\tRec Loss:2144.3120117\tKL Div:6.8502474\n",
      "Epoch 127\tIter   18\tLoss:2109.9060059\tRec Loss:2103.1230469\tKL Div:6.7830343\n",
      "Epoch 128\tIter    6\tLoss:2155.8137207\tRec Loss:2148.8452148\tKL Div:6.9685922\n",
      "Epoch 128\tIter   12\tLoss:2121.6474609\tRec Loss:2114.8417969\tKL Div:6.8057451\n",
      "Epoch 128\tIter   18\tLoss:2102.9772949\tRec Loss:2096.2045898\tKL Div:6.7726974\n",
      "Epoch 129\tIter    6\tLoss:2129.9714355\tRec Loss:2123.1010742\tKL Div:6.8704653\n",
      "Epoch 129\tIter   12\tLoss:2109.3073730\tRec Loss:2102.5078125\tKL Div:6.7995014\n",
      "Epoch 129\tIter   18\tLoss:2146.6145020\tRec Loss:2139.6821289\tKL Div:6.9324675\n",
      "Epoch 130\tIter    6\tLoss:2166.0605469\tRec Loss:2159.1506348\tKL Div:6.9098873\n",
      "Epoch 130\tIter   12\tLoss:2132.3293457\tRec Loss:2125.4707031\tKL Div:6.8587618\n",
      "Epoch 130\tIter   18\tLoss:2120.4733887\tRec Loss:2113.7456055\tKL Div:6.7278099\n",
      "Epoch 131\tIter    6\tLoss:2125.3173828\tRec Loss:2118.3647461\tKL Div:6.9525838\n",
      "Epoch 131\tIter   12\tLoss:2131.7946777\tRec Loss:2124.8493652\tKL Div:6.9453535\n",
      "Epoch 131\tIter   18\tLoss:2114.8220215\tRec Loss:2107.9362793\tKL Div:6.8857789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132\tIter    6\tLoss:2134.9926758\tRec Loss:2127.8708496\tKL Div:7.1217656\n",
      "Epoch 132\tIter   12\tLoss:2158.1760254\tRec Loss:2151.2363281\tKL Div:6.9396777\n",
      "Epoch 132\tIter   18\tLoss:2180.6794434\tRec Loss:2173.5791016\tKL Div:7.1002669\n",
      "Epoch 133\tIter    6\tLoss:2166.0009766\tRec Loss:2159.0112305\tKL Div:6.9897003\n",
      "Epoch 133\tIter   12\tLoss:2120.2563477\tRec Loss:2113.2812500\tKL Div:6.9749823\n",
      "Epoch 133\tIter   18\tLoss:2172.9958496\tRec Loss:2166.0368652\tKL Div:6.9590616\n",
      "Epoch 134\tIter    6\tLoss:2138.2238770\tRec Loss:2131.1909180\tKL Div:7.0329814\n",
      "Epoch 134\tIter   12\tLoss:2150.1018066\tRec Loss:2143.1923828\tKL Div:6.9094677\n",
      "Epoch 134\tIter   18\tLoss:2171.2746582\tRec Loss:2164.3547363\tKL Div:6.9199090\n",
      "Epoch 135\tIter    6\tLoss:2141.0227051\tRec Loss:2133.9626465\tKL Div:7.0599871\n",
      "Epoch 135\tIter   12\tLoss:2171.3229980\tRec Loss:2164.3767090\tKL Div:6.9461737\n",
      "Epoch 135\tIter   18\tLoss:2156.7836914\tRec Loss:2149.5854492\tKL Div:7.1982927\n",
      "Epoch 136\tIter    6\tLoss:2075.7988281\tRec Loss:2068.9057617\tKL Div:6.8931808\n",
      "Epoch 136\tIter   12\tLoss:2121.3461914\tRec Loss:2114.4492188\tKL Div:6.8969784\n",
      "Epoch 136\tIter   18\tLoss:2120.1020508\tRec Loss:2113.0312500\tKL Div:7.0708876\n",
      "Epoch 137\tIter    6\tLoss:2133.8937988\tRec Loss:2127.0383301\tKL Div:6.8555207\n",
      "Epoch 137\tIter   12\tLoss:2108.2141113\tRec Loss:2101.2341309\tKL Div:6.9799037\n",
      "Epoch 137\tIter   18\tLoss:2133.4567871\tRec Loss:2126.4423828\tKL Div:7.0145040\n",
      "Epoch 138\tIter    6\tLoss:2155.6589355\tRec Loss:2148.4384766\tKL Div:7.2203503\n",
      "Epoch 138\tIter   12\tLoss:2148.9780273\tRec Loss:2141.9609375\tKL Div:7.0169954\n",
      "Epoch 138\tIter   18\tLoss:2167.4277344\tRec Loss:2160.2419434\tKL Div:7.1857767\n",
      "Epoch 139\tIter    6\tLoss:2113.3457031\tRec Loss:2106.3627930\tKL Div:6.9828720\n",
      "Epoch 139\tIter   12\tLoss:2180.5832520\tRec Loss:2173.3703613\tKL Div:7.2129092\n",
      "Epoch 139\tIter   18\tLoss:2126.8908691\tRec Loss:2119.9204102\tKL Div:6.9703465\n",
      "Epoch 140\tIter    6\tLoss:2122.3557129\tRec Loss:2115.2248535\tKL Div:7.1309428\n",
      "Epoch 140\tIter   12\tLoss:2151.8398438\tRec Loss:2144.8666992\tKL Div:6.9730711\n",
      "Epoch 140\tIter   18\tLoss:2147.0524902\tRec Loss:2139.9184570\tKL Div:7.1341262\n",
      "Epoch 141\tIter    6\tLoss:2073.3283691\tRec Loss:2066.3840332\tKL Div:6.9442172\n",
      "Epoch 141\tIter   12\tLoss:2119.0908203\tRec Loss:2111.9389648\tKL Div:7.1517615\n",
      "Epoch 141\tIter   18\tLoss:2118.5073242\tRec Loss:2111.5644531\tKL Div:6.9427938\n",
      "Epoch 142\tIter    6\tLoss:2119.2854004\tRec Loss:2112.1040039\tKL Div:7.1814232\n",
      "Epoch 142\tIter   12\tLoss:2142.5053711\tRec Loss:2135.4116211\tKL Div:7.0936713\n",
      "Epoch 142\tIter   18\tLoss:2123.6298828\tRec Loss:2116.5971680\tKL Div:7.0326309\n",
      "Epoch 143\tIter    6\tLoss:2101.4025879\tRec Loss:2094.3840332\tKL Div:7.0184536\n",
      "Epoch 143\tIter   12\tLoss:2147.6010742\tRec Loss:2140.4951172\tKL Div:7.1059594\n",
      "Epoch 143\tIter   18\tLoss:2093.0434570\tRec Loss:2085.9479980\tKL Div:7.0955048\n",
      "Epoch 144\tIter    6\tLoss:2169.1423340\tRec Loss:2161.9555664\tKL Div:7.1867957\n",
      "Epoch 144\tIter   12\tLoss:2107.7766113\tRec Loss:2100.6540527\tKL Div:7.1225300\n",
      "Epoch 144\tIter   18\tLoss:2143.1831055\tRec Loss:2135.9697266\tKL Div:7.2134614\n",
      "Epoch 145\tIter    6\tLoss:2160.7521973\tRec Loss:2153.4360352\tKL Div:7.3160763\n",
      "Epoch 145\tIter   12\tLoss:2129.5332031\tRec Loss:2122.3718262\tKL Div:7.1614981\n",
      "Epoch 145\tIter   18\tLoss:2146.3303223\tRec Loss:2139.0529785\tKL Div:7.2772250\n",
      "Epoch 146\tIter    6\tLoss:2123.3500977\tRec Loss:2116.1059570\tKL Div:7.2441216\n",
      "Epoch 146\tIter   12\tLoss:2108.7863770\tRec Loss:2101.7512207\tKL Div:7.0351214\n",
      "Epoch 146\tIter   18\tLoss:2134.2214355\tRec Loss:2126.9467773\tKL Div:7.2747240\n",
      "Epoch 147\tIter    6\tLoss:2131.1779785\tRec Loss:2124.0820312\tKL Div:7.0958648\n",
      "Epoch 147\tIter   12\tLoss:2137.9406738\tRec Loss:2130.7453613\tKL Div:7.1952353\n",
      "Epoch 147\tIter   18\tLoss:2144.2541504\tRec Loss:2137.0168457\tKL Div:7.2373686\n",
      "Epoch 148\tIter    6\tLoss:2154.3666992\tRec Loss:2147.1289062\tKL Div:7.2377129\n",
      "Epoch 148\tIter   12\tLoss:2124.4238281\tRec Loss:2117.1579590\tKL Div:7.2657914\n",
      "Epoch 148\tIter   18\tLoss:2140.7700195\tRec Loss:2133.5351562\tKL Div:7.2349310\n",
      "Epoch 149\tIter    6\tLoss:2129.6179199\tRec Loss:2122.3100586\tKL Div:7.3079481\n",
      "Epoch 149\tIter   12\tLoss:2146.0827637\tRec Loss:2138.8945312\tKL Div:7.1882229\n",
      "Epoch 149\tIter   18\tLoss:2107.4765625\tRec Loss:2100.2922363\tKL Div:7.1844482\n",
      "Epoch 150\tIter    6\tLoss:2088.0046387\tRec Loss:2080.8178711\tKL Div:7.1868582\n",
      "Epoch 150\tIter   12\tLoss:2167.4191895\tRec Loss:2160.0952148\tKL Div:7.3238535\n",
      "Epoch 150\tIter   18\tLoss:2122.3041992\tRec Loss:2114.9511719\tKL Div:7.3530970\n",
      "Epoch 151\tIter    6\tLoss:2101.3510742\tRec Loss:2094.2395020\tKL Div:7.1115398\n",
      "Epoch 151\tIter   12\tLoss:2152.6440430\tRec Loss:2145.2814941\tKL Div:7.3626652\n",
      "Epoch 151\tIter   18\tLoss:2130.4443359\tRec Loss:2123.1181641\tKL Div:7.3261566\n",
      "Epoch 152\tIter    6\tLoss:2157.1042480\tRec Loss:2149.8869629\tKL Div:7.2172661\n",
      "Epoch 152\tIter   12\tLoss:2155.3806152\tRec Loss:2148.1215820\tKL Div:7.2590618\n",
      "Epoch 152\tIter   18\tLoss:2131.1337891\tRec Loss:2123.8271484\tKL Div:7.3065600\n",
      "Epoch 153\tIter    6\tLoss:2169.3989258\tRec Loss:2162.0898438\tKL Div:7.3090892\n",
      "Epoch 153\tIter   12\tLoss:2185.2255859\tRec Loss:2177.8679199\tKL Div:7.3575492\n",
      "Epoch 153\tIter   18\tLoss:2143.1660156\tRec Loss:2135.8767090\tKL Div:7.2893524\n",
      "Epoch 154\tIter    6\tLoss:2125.5905762\tRec Loss:2118.2226562\tKL Div:7.3678942\n",
      "Epoch 154\tIter   12\tLoss:2171.8688965\tRec Loss:2164.6076660\tKL Div:7.2611232\n",
      "Epoch 154\tIter   18\tLoss:2165.5822754\tRec Loss:2158.1850586\tKL Div:7.3972631\n",
      "Epoch 155\tIter    6\tLoss:2145.1862793\tRec Loss:2137.6799316\tKL Div:7.5063281\n",
      "Epoch 155\tIter   12\tLoss:2114.3962402\tRec Loss:2107.1398926\tKL Div:7.2562284\n",
      "Epoch 155\tIter   18\tLoss:2149.4958496\tRec Loss:2141.9860840\tKL Div:7.5097961\n",
      "Epoch 156\tIter    6\tLoss:2145.2893066\tRec Loss:2137.7897949\tKL Div:7.4994650\n",
      "Epoch 156\tIter   12\tLoss:2132.7702637\tRec Loss:2125.4038086\tKL Div:7.3663635\n",
      "Epoch 156\tIter   18\tLoss:2141.4580078\tRec Loss:2134.0720215\tKL Div:7.3860207\n",
      "Epoch 157\tIter    6\tLoss:2115.9475098\tRec Loss:2108.6667480\tKL Div:7.2808247\n",
      "Epoch 157\tIter   12\tLoss:2137.8637695\tRec Loss:2130.4868164\tKL Div:7.3768797\n",
      "Epoch 157\tIter   18\tLoss:2115.8447266\tRec Loss:2108.4020996\tKL Div:7.4427176\n",
      "Epoch 158\tIter    6\tLoss:2173.7282715\tRec Loss:2166.2756348\tKL Div:7.4525504\n",
      "Epoch 158\tIter   12\tLoss:2128.5214844\tRec Loss:2121.0959473\tKL Div:7.4256086\n",
      "Epoch 158\tIter   18\tLoss:2201.1367188\tRec Loss:2193.7548828\tKL Div:7.3817725\n",
      "Epoch 159\tIter    6\tLoss:2139.7453613\tRec Loss:2132.2031250\tKL Div:7.5421200\n",
      "Epoch 159\tIter   12\tLoss:2129.0883789\tRec Loss:2121.8125000\tKL Div:7.2758884\n",
      "Epoch 159\tIter   18\tLoss:2177.8085938\tRec Loss:2170.3288574\tKL Div:7.4798517\n",
      "Epoch 160\tIter    6\tLoss:2103.1433105\tRec Loss:2095.8002930\tKL Div:7.3428984\n",
      "Epoch 160\tIter   12\tLoss:2160.4174805\tRec Loss:2152.8720703\tKL Div:7.5454335\n",
      "Epoch 160\tIter   18\tLoss:2108.1176758\tRec Loss:2100.7343750\tKL Div:7.3831968\n",
      "Epoch 160\tLoss:2139.7363847\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  573,   319,    95,   319,  1114,   319, 13238,   319,   319,  1761,\n",
      "          573,   317,  1654,   235,   751,  1295,   433,    99,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['melody', 'drum', 'bass', 'noise', 'build', 'post_rock', 'debut', 'melodic', 'opener', 'close', 'line', 'tone', 'instrumental', 'sonic', 'rhythm']\n",
      "['line', 'love', 'lyric', 'debut', 'big', 'turn', 'title', 'kind', 'group', 'close', 'leave', 'point', 'bit', 'hard', 'single']\n",
      "['live', 'version', 'set', 'disc', 'cover', 'record', 'include', 'studio', 'original', 'recording', 'early', 'group', 'reissue', 'label', 'compilation']\n",
      "['line', 'close', 'turn', 'title', 'kind', 'melody', 'bit', 'leave', 'point', 'group', 'start', 'debut', 'drum', 'year', 'listen']\n",
      "['house', 'beat', 'dance', 'mix', 'label', 'techno', 'bass', 'producer', 'synth', 'artist', 'electronic', 'dj', 'drum', 'sample', 'rhythm']\n",
      "['line', 'turn', 'love', 'kind', 'title', 'close', 'year', 'point', 'big', 'debut', 'world', 'leave', 'single', 'hard', 'early']\n",
      "['\\xa0', 'project', 'sense', 'year', 'early', 'point', 'title', 'synth', 'single', 'producer', 'world', 'past', 'close', 'debut', 'solo']\n",
      "['line', 'title', 'debut', 'melody', 'close', 'kind', 'turn', 'drum', 'bit', 'big', 'point', 'leave', 'hard', 'group', 'year']\n",
      "['line', 'love', 'turn', 'close', 'kind', 'year', 'world', 'title', 'big', 'early', 'point', 'leave', 'debut', 'hard', 'single']\n",
      "['band', 'love', 'guy', 'day', 'lyric', 'bad', 'pretty', 'indie', 'get', 'cover', 'write', 'man', 'fan', 'let', 'big']\n",
      "['melody', 'drum', 'bass', 'instrumental', 'group', 'bit', 'pretty', 'band', 'keyboard', 'beat', 'rhythm', 'interesting', 'tune', 'disc', 'noise']\n",
      "['life', 'love', 'sing', 'world', 'man', 'write', 'people', 'woman', 'word', 'death', 'story', 'live', 'young', 'day', 'line']\n",
      "['folk', 'sing', 'lyric', 'piano', 'love', 'arrangement', 'line', 'solo', 'string', 'country', 'melody', 'word', 'close', 'record', 'acoustic']\n",
      "['dance', 'people', 'disco', 'single', 'party', 'beat', 'bad', 'big', 'fun', 'guy', 'get', 'listen', 'man', 'kid', 'stuff']\n",
      "['piece', 'drone', 'electronic', 'noise', 'tone', 'world', 'piano', 'sense', 'ambient', 'instrument', 'composition', 'space', 'create', 'note', 'musician']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'production', 'verse', 'year', 'producer', 'feature', 'artist', 'flow', 'style', 'line', 'sample']\n",
      "['synth', 'ep', 'melody', 'electronic', 'beat', 'debut', 'production', 'close', 'duo', 'sense', 'kind', 'build', 'instrumental', 'line', 'texture']\n",
      "['lyric', 'chorus', 'hook', 'title', 'debut', 'line', 'band', 'indie_rock', 'melody', 'indie', 'love', 'big', 'kind', 'emo', 'turn']\n",
      "['punk', 'metal', 'riff', 'band', 'noise', 'hardcore', 'drum', 'drummer', 'heavy', 'guitarist', 'scream', 'black', 'bass', 'live', 'death']\n",
      "['love', 'girl', 'single', 'r&b', 'artist', 'singer', 'sing', 'lyric', 'hit', 'star', 'soul', 'big', 'debut', 'year', 'indie']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  573,   319,    95,   319,  1114,   319, 13238,   319,   319,  1761,\n",
      "          573,   317,  1654,   235,   751,  1295,   433,    99,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  573,   319,    95,   319,  1114,   319, 13238,   319,   319,  1761,\n",
      "          573,   317,  1654,   235,   751,  1295,   433,    99,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.44666666666666666\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "melody\n",
      "drum\n",
      "bass\n",
      "noise\n",
      "build\n",
      "post_rock\n",
      "debut\n",
      "melodic\n",
      "opener\n",
      "close\n",
      "line\n",
      "tone\n",
      "instrumental\n",
      "sonic\n",
      "rhythm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence c_uci 0.014154525966249467\n",
      "coherence c_npmi 0.009808207284352199\n",
      "coherence c_cv 0.3626111818737362\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  573,   319,    95,   319,  1114,   319, 13238,   319,   319,  1761,\n",
      "          573,   317,  1654,   235,   751,  1295,   433,    99,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 161\tIter    6\tLoss:2128.7636719\tRec Loss:2121.2895508\tKL Div:7.4741788\n",
      "Epoch 161\tIter   12\tLoss:2125.6682129\tRec Loss:2118.2082520\tKL Div:7.4599199\n",
      "Epoch 161\tIter   18\tLoss:2128.7243652\tRec Loss:2121.4265137\tKL Div:7.2977562\n",
      "Epoch 162\tIter    6\tLoss:2156.5605469\tRec Loss:2149.0395508\tKL Div:7.5209951\n",
      "Epoch 162\tIter   12\tLoss:2154.8886719\tRec Loss:2147.3144531\tKL Div:7.5742903\n",
      "Epoch 162\tIter   18\tLoss:2122.7565918\tRec Loss:2115.1977539\tKL Div:7.5589147\n",
      "Epoch 163\tIter    6\tLoss:2102.4829102\tRec Loss:2095.0815430\tKL Div:7.4013567\n",
      "Epoch 163\tIter   12\tLoss:2132.9033203\tRec Loss:2125.3076172\tKL Div:7.5958095\n",
      "Epoch 163\tIter   18\tLoss:2155.4287109\tRec Loss:2147.9829102\tKL Div:7.4458780\n",
      "Epoch 164\tIter    6\tLoss:2103.1640625\tRec Loss:2095.6110840\tKL Div:7.5529003\n",
      "Epoch 164\tIter   12\tLoss:2144.4980469\tRec Loss:2137.0136719\tKL Div:7.4844427\n",
      "Epoch 164\tIter   18\tLoss:2154.6560059\tRec Loss:2147.0805664\tKL Div:7.5754089\n",
      "Epoch 165\tIter    6\tLoss:2161.2194824\tRec Loss:2153.6037598\tKL Div:7.6156397\n",
      "Epoch 165\tIter   12\tLoss:2154.6772461\tRec Loss:2147.1010742\tKL Div:7.5761433\n",
      "Epoch 165\tIter   18\tLoss:2140.7407227\tRec Loss:2133.1459961\tKL Div:7.5947027\n",
      "Epoch 166\tIter    6\tLoss:2142.6462402\tRec Loss:2135.1501465\tKL Div:7.4960690\n",
      "Epoch 166\tIter   12\tLoss:2116.6716309\tRec Loss:2109.0727539\tKL Div:7.5988178\n",
      "Epoch 166\tIter   18\tLoss:2135.3659668\tRec Loss:2127.8100586\tKL Div:7.5560236\n",
      "Epoch 167\tIter    6\tLoss:2139.5712891\tRec Loss:2131.9335938\tKL Div:7.6376095\n",
      "Epoch 167\tIter   12\tLoss:2143.6921387\tRec Loss:2136.1950684\tKL Div:7.4970398\n",
      "Epoch 167\tIter   18\tLoss:2104.5771484\tRec Loss:2097.1174316\tKL Div:7.4596081\n",
      "Epoch 168\tIter    6\tLoss:2181.2817383\tRec Loss:2173.5937500\tKL Div:7.6879563\n",
      "Epoch 168\tIter   12\tLoss:2148.2536621\tRec Loss:2140.6196289\tKL Div:7.6340790\n",
      "Epoch 168\tIter   18\tLoss:2174.3415527\tRec Loss:2166.6313477\tKL Div:7.7101865\n",
      "Epoch 169\tIter    6\tLoss:2100.3815918\tRec Loss:2092.9892578\tKL Div:7.3922477\n",
      "Epoch 169\tIter   12\tLoss:2135.6748047\tRec Loss:2128.0273438\tKL Div:7.6475658\n",
      "Epoch 169\tIter   18\tLoss:2120.5913086\tRec Loss:2113.1506348\tKL Div:7.4407692\n",
      "Epoch 170\tIter    6\tLoss:2142.2846680\tRec Loss:2134.5708008\tKL Div:7.7139511\n",
      "Epoch 170\tIter   12\tLoss:2156.6455078\tRec Loss:2149.0078125\tKL Div:7.6377401\n",
      "Epoch 170\tIter   18\tLoss:2133.8315430\tRec Loss:2126.2299805\tKL Div:7.6016846\n",
      "Epoch 171\tIter    6\tLoss:2094.3203125\tRec Loss:2086.8081055\tKL Div:7.5120921\n",
      "Epoch 171\tIter   12\tLoss:2098.4904785\tRec Loss:2091.0197754\tKL Div:7.4707537\n",
      "Epoch 171\tIter   18\tLoss:2141.9460449\tRec Loss:2134.2995605\tKL Div:7.6464219\n",
      "Epoch 172\tIter    6\tLoss:2155.3730469\tRec Loss:2147.7065430\tKL Div:7.6664171\n",
      "Epoch 172\tIter   12\tLoss:2145.7700195\tRec Loss:2138.1909180\tKL Div:7.5791616\n",
      "Epoch 172\tIter   18\tLoss:2131.6291504\tRec Loss:2124.0932617\tKL Div:7.5358753\n",
      "Epoch 173\tIter    6\tLoss:2159.3020020\tRec Loss:2151.5729980\tKL Div:7.7290759\n",
      "Epoch 173\tIter   12\tLoss:2142.7094727\tRec Loss:2135.1308594\tKL Div:7.5786819\n",
      "Epoch 173\tIter   18\tLoss:2166.6054688\tRec Loss:2158.9567871\tKL Div:7.6487288\n",
      "Epoch 174\tIter    6\tLoss:2126.6506348\tRec Loss:2118.9794922\tKL Div:7.6712523\n",
      "Epoch 174\tIter   12\tLoss:2106.8295898\tRec Loss:2099.2480469\tKL Div:7.5814590\n",
      "Epoch 174\tIter   18\tLoss:2191.0615234\tRec Loss:2183.4645996\tKL Div:7.5968227\n",
      "Epoch 175\tIter    6\tLoss:2133.5734863\tRec Loss:2125.7978516\tKL Div:7.7756033\n",
      "Epoch 175\tIter   12\tLoss:2128.2009277\tRec Loss:2120.6831055\tKL Div:7.5178194\n",
      "Epoch 175\tIter   18\tLoss:2119.6452637\tRec Loss:2111.8735352\tKL Div:7.7717805\n",
      "Epoch 176\tIter    6\tLoss:2108.5524902\tRec Loss:2101.0305176\tKL Div:7.5218983\n",
      "Epoch 176\tIter   12\tLoss:2127.9421387\tRec Loss:2120.1738281\tKL Div:7.7682180\n",
      "Epoch 176\tIter   18\tLoss:2149.0339355\tRec Loss:2141.4128418\tKL Div:7.6210489\n",
      "Epoch 177\tIter    6\tLoss:2135.1420898\tRec Loss:2127.3959961\tKL Div:7.7460685\n",
      "Epoch 177\tIter   12\tLoss:2123.6970215\tRec Loss:2116.1481934\tKL Div:7.5488191\n",
      "Epoch 177\tIter   18\tLoss:2159.8405762\tRec Loss:2152.1513672\tKL Div:7.6892862\n",
      "Epoch 178\tIter    6\tLoss:2173.8923340\tRec Loss:2166.0307617\tKL Div:7.8615723\n",
      "Epoch 178\tIter   12\tLoss:2107.7890625\tRec Loss:2100.1887207\tKL Div:7.6003361\n",
      "Epoch 178\tIter   18\tLoss:2136.8593750\tRec Loss:2129.1284180\tKL Div:7.7308583\n",
      "Epoch 179\tIter    6\tLoss:2117.4768066\tRec Loss:2109.8393555\tKL Div:7.6374044\n",
      "Epoch 179\tIter   12\tLoss:2123.1262207\tRec Loss:2115.4455566\tKL Div:7.6805863\n",
      "Epoch 179\tIter   18\tLoss:2139.8110352\tRec Loss:2132.2182617\tKL Div:7.5926828\n",
      "Epoch 180\tIter    6\tLoss:2104.6325684\tRec Loss:2096.8762207\tKL Div:7.7563920\n",
      "Epoch 180\tIter   12\tLoss:2108.1057129\tRec Loss:2100.5705566\tKL Div:7.5352077\n",
      "Epoch 180\tIter   18\tLoss:2136.0485840\tRec Loss:2128.3364258\tKL Div:7.7121282\n",
      "Epoch 181\tIter    6\tLoss:2110.4370117\tRec Loss:2102.8198242\tKL Div:7.6172519\n",
      "Epoch 181\tIter   12\tLoss:2125.7084961\tRec Loss:2117.9868164\tKL Div:7.7216206\n",
      "Epoch 181\tIter   18\tLoss:2155.8059082\tRec Loss:2148.0083008\tKL Div:7.7975206\n",
      "Epoch 182\tIter    6\tLoss:2129.0859375\tRec Loss:2121.4594727\tKL Div:7.6265321\n",
      "Epoch 182\tIter   12\tLoss:2119.2575684\tRec Loss:2111.4880371\tKL Div:7.7695422\n",
      "Epoch 182\tIter   18\tLoss:2119.2966309\tRec Loss:2111.6552734\tKL Div:7.6413898\n",
      "Epoch 183\tIter    6\tLoss:2119.2565918\tRec Loss:2111.5742188\tKL Div:7.6824255\n",
      "Epoch 183\tIter   12\tLoss:2206.8750000\tRec Loss:2199.1386719\tKL Div:7.7362566\n",
      "Epoch 183\tIter   18\tLoss:2092.8132324\tRec Loss:2085.1062012\tKL Div:7.7071075\n",
      "Epoch 184\tIter    6\tLoss:2084.0449219\tRec Loss:2076.3525391\tKL Div:7.6924438\n",
      "Epoch 184\tIter   12\tLoss:2126.7211914\tRec Loss:2118.9907227\tKL Div:7.7305393\n",
      "Epoch 184\tIter   18\tLoss:2156.4426270\tRec Loss:2148.6828613\tKL Div:7.7597852\n",
      "Epoch 185\tIter    6\tLoss:2139.0739746\tRec Loss:2131.2436523\tKL Div:7.8304214\n",
      "Epoch 185\tIter   12\tLoss:2155.2873535\tRec Loss:2147.6567383\tKL Div:7.6306267\n",
      "Epoch 185\tIter   18\tLoss:2121.0209961\tRec Loss:2113.3515625\tKL Div:7.6695185\n",
      "Epoch 186\tIter    6\tLoss:2147.3630371\tRec Loss:2139.5234375\tKL Div:7.8396311\n",
      "Epoch 186\tIter   12\tLoss:2130.6674805\tRec Loss:2122.9255371\tKL Div:7.7418671\n",
      "Epoch 186\tIter   18\tLoss:2156.6496582\tRec Loss:2148.8344727\tKL Div:7.8151302\n",
      "Epoch 187\tIter    6\tLoss:2161.8801270\tRec Loss:2154.0183105\tKL Div:7.8618207\n",
      "Epoch 187\tIter   12\tLoss:2117.5961914\tRec Loss:2109.9990234\tKL Div:7.5972023\n",
      "Epoch 187\tIter   18\tLoss:2094.6552734\tRec Loss:2086.9499512\tKL Div:7.7052536\n",
      "Epoch 188\tIter    6\tLoss:2138.7768555\tRec Loss:2131.0664062\tKL Div:7.7103419\n",
      "Epoch 188\tIter   12\tLoss:2130.7094727\tRec Loss:2122.8498535\tKL Div:7.8596177\n",
      "Epoch 188\tIter   18\tLoss:2170.6789551\tRec Loss:2162.9401855\tKL Div:7.7388697\n",
      "Epoch 189\tIter    6\tLoss:2136.3498535\tRec Loss:2128.4733887\tKL Div:7.8764706\n",
      "Epoch 189\tIter   12\tLoss:2127.2976074\tRec Loss:2119.5598145\tKL Div:7.7377338\n",
      "Epoch 189\tIter   18\tLoss:2112.7106934\tRec Loss:2104.8994141\tKL Div:7.8112488\n",
      "Epoch 190\tIter    6\tLoss:2148.7019043\tRec Loss:2140.9257812\tKL Div:7.7760901\n",
      "Epoch 190\tIter   12\tLoss:2147.0422363\tRec Loss:2139.3769531\tKL Div:7.6653719\n",
      "Epoch 190\tIter   18\tLoss:2161.1276855\tRec Loss:2153.1528320\tKL Div:7.9748216\n",
      "Epoch 191\tIter    6\tLoss:2130.6003418\tRec Loss:2122.7609863\tKL Div:7.8393364\n",
      "Epoch 191\tIter   12\tLoss:2103.5236816\tRec Loss:2095.8881836\tKL Div:7.6354628\n",
      "Epoch 191\tIter   18\tLoss:2150.8532715\tRec Loss:2142.9162598\tKL Div:7.9369535\n",
      "Epoch 192\tIter    6\tLoss:2160.3408203\tRec Loss:2152.4326172\tKL Div:7.9082332\n",
      "Epoch 192\tIter   12\tLoss:2083.8732910\tRec Loss:2076.3159180\tKL Div:7.5573025\n",
      "Epoch 192\tIter   18\tLoss:2152.5468750\tRec Loss:2144.6079102\tKL Div:7.9390488\n",
      "Epoch 193\tIter    6\tLoss:2160.3698730\tRec Loss:2152.4362793\tKL Div:7.9335828\n",
      "Epoch 193\tIter   12\tLoss:2136.9443359\tRec Loss:2129.3342285\tKL Div:7.6101341\n",
      "Epoch 193\tIter   18\tLoss:2164.9926758\tRec Loss:2156.9904785\tKL Div:8.0021439\n",
      "Epoch 194\tIter    6\tLoss:2132.0676270\tRec Loss:2124.3356934\tKL Div:7.7319689\n",
      "Epoch 194\tIter   12\tLoss:2133.2687988\tRec Loss:2125.3269043\tKL Div:7.9418626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194\tIter   18\tLoss:2124.2858887\tRec Loss:2116.4628906\tKL Div:7.8229265\n",
      "Epoch 195\tIter    6\tLoss:2129.2084961\tRec Loss:2121.3464355\tKL Div:7.8619614\n",
      "Epoch 195\tIter   12\tLoss:2144.9162598\tRec Loss:2137.0585938\tKL Div:7.8576679\n",
      "Epoch 195\tIter   18\tLoss:2129.6584473\tRec Loss:2121.8701172\tKL Div:7.7882576\n",
      "Epoch 196\tIter    6\tLoss:2148.1201172\tRec Loss:2140.1018066\tKL Div:8.0182114\n",
      "Epoch 196\tIter   12\tLoss:2160.3110352\tRec Loss:2152.3041992\tKL Div:8.0067768\n",
      "Epoch 196\tIter   18\tLoss:2119.4582520\tRec Loss:2111.5830078\tKL Div:7.8751345\n",
      "Epoch 197\tIter    6\tLoss:2101.9814453\tRec Loss:2094.1796875\tKL Div:7.8017020\n",
      "Epoch 197\tIter   12\tLoss:2071.7485352\tRec Loss:2063.8344727\tKL Div:7.9141350\n",
      "Epoch 197\tIter   18\tLoss:2139.8037109\tRec Loss:2132.0300293\tKL Div:7.7736144\n",
      "Epoch 198\tIter    6\tLoss:2147.1328125\tRec Loss:2139.1892090\tKL Div:7.9435153\n",
      "Epoch 198\tIter   12\tLoss:2119.4721680\tRec Loss:2111.6958008\tKL Div:7.7763691\n",
      "Epoch 198\tIter   18\tLoss:2131.1096191\tRec Loss:2123.3383789\tKL Div:7.7713528\n",
      "Epoch 199\tIter    6\tLoss:2126.9304199\tRec Loss:2119.0300293\tKL Div:7.9004440\n",
      "Epoch 199\tIter   12\tLoss:2125.8967285\tRec Loss:2118.0544434\tKL Div:7.8423104\n",
      "Epoch 199\tIter   18\tLoss:2108.7858887\tRec Loss:2100.9274902\tKL Div:7.8584805\n",
      "Epoch 200\tIter    6\tLoss:2112.7653809\tRec Loss:2104.9433594\tKL Div:7.8220582\n",
      "Epoch 200\tIter   12\tLoss:2121.6633301\tRec Loss:2113.8505859\tKL Div:7.8126626\n",
      "Epoch 200\tIter   18\tLoss:2153.1445312\tRec Loss:2145.1376953\tKL Div:8.0069466\n",
      "Epoch 200\tLoss:2133.8638812\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  254,   573,    95,   319,   235,   319, 13238,   319,    97,    97,\n",
      "          573,   317,  1654,   235,   751,  1295,   433,    99,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drum', 'melody', 'noise', 'build', 'bass', 'drone', 'tone', 'beat', 'close', 'rhythm', 'heavy', 'slow', 'instrumental', 'line', 'open']\n",
      "['melody', 'indie', 'chorus', 'lyric', 'debut', 'pretty', 'hook', 'love', 'ep', 'group', 'harmony', 'bit', 'band', 'line', 'opener']\n",
      "['live', 'version', 'set', 'disc', 'include', 'record', 'cover', 'original', 'recording', 'studio', 'label', 'group', 'soul', 'early', 'compilation']\n",
      "['line', 'close', 'turn', 'title', 'kind', 'melody', 'leave', 'start', 'point', 'listen', 'world', 'bit', 'love', 'debut', 'drum']\n",
      "['dance', 'house', 'mix', 'beat', 'label', 'techno', 'bass', 'producer', 'synth', 'dj', 'artist', 'electronic', 'drum', 'rhythm', 'club']\n",
      "['line', 'love', 'turn', 'close', 'point', 'world', 'title', 'kind', 'year', 'leave', 'debut', 'start', 'big', 'hard', 'listen']\n",
      "['\\xa0', 'project', 'point', 'ep', 'synth', 'title', 'solo', 'early', 'lyric', 'close', 'sense', 'producer', 'year', 'place', 'world']\n",
      "['line', 'melody', 'debut', 'drum', 'close', 'turn', 'title', 'beat', 'kind', 'point', 'bit', 'big', 'start', 'leave', 'synth']\n",
      "['love', 'line', 'turn', 'close', 'world', 'year', 'kind', 'title', 'point', 'leave', 'early', 'debut', 'place', 'big', 'sense']\n",
      "['love', 'band', 'punk', 'day', 'bad', 'guy', 'cover', 'write', 'man', 'get', 'big', 'solo', 'fan', 'pretty', 'girl']\n",
      "['melody', 'drum', 'disc', 'instrumental', 'bass', 'listen', 'beat', 'bit', 'interesting', 'feature', 'group', 'noise', 'begin', 'pretty', 'tune']\n",
      "['life', 'love', 'world', 'sing', 'man', 'people', 'write', 'woman', 'word', 'story', 'death', 'live', 'black', 'line', 'lyric']\n",
      "['folk', 'sing', 'lyric', 'love', 'country', 'piano', 'arrangement', 'solo', 'line', 'string', 'melody', 'word', 'record', 'close', 'light']\n",
      "['dance', 'people', 'beat', 'bad', 'party', 'disco', 'guy', 'single', 'big', 'fun', 'get', 'girl', 'pretty', 'man', 'kid']\n",
      "['piece', 'electronic', 'drone', 'world', 'piano', 'tone', 'ambient', 'instrument', 'composition', 'sense', 'space', 'noise', 'composer', 'musician', 'create']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'production', 'mixtape', 'verse', 'year', 'producer', 'flow', 'style', 'feature', 'sample', 'line', 'artist']\n",
      "['synth', 'ep', 'beat', 'electronic', 'melody', 'debut', 'sense', 'close', 'production', 'sample', 'kind', 'build', 'instrumental', 'line', 'duo']\n",
      "['lyric', 'love', 'chorus', 'title', 'band', 'hook', 'indie_rock', 'debut', 'kind', 'line', 'punk', 'big', 'emo', 'melody', 'life']\n",
      "['punk', 'metal', 'riff', 'band', 'hardcore', 'noise', 'group', 'drummer', 'black_metal', 'death', 'scream', 'guitarist', 'black', 'live', 'early']\n",
      "['love', 'r&b', 'single', 'singer', 'artist', 'star', 'girl', 'hit', 'year', 'sing', 'soul', 'debut', 'big', 'producer', 'group']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  254,   573,    95,   319,   235,   319, 13238,   319,    97,    97,\n",
      "          573,   317,  1654,   235,   751,  1295,   433,    99,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  254,   573,    95,   319,   235,   319, 13238,   319,    97,    97,\n",
      "          573,   317,  1654,   235,   751,  1295,   433,    99,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.43666666666666665\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drum\n",
      "melody\n",
      "noise\n",
      "build\n",
      "bass\n",
      "drone\n",
      "tone\n",
      "beat\n",
      "close\n",
      "rhythm\n",
      "heavy\n",
      "slow\n",
      "instrumental\n",
      "line\n",
      "open\n",
      "coherence c_uci 0.019471564416080896\n",
      "coherence c_npmi 0.01001139696186048\n",
      "coherence c_cv 0.3645904741352133\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  254,   573,    95,   319,   235,   319, 13238,   319,    97,    97,\n",
      "          573,   317,  1654,   235,   751,  1295,   433,    99,   131,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 201\tIter    6\tLoss:2121.2062988\tRec Loss:2113.4360352\tKL Div:7.7702284\n",
      "Epoch 201\tIter   12\tLoss:2100.3896484\tRec Loss:2092.5991211\tKL Div:7.7904935\n",
      "Epoch 201\tIter   18\tLoss:2151.6108398\tRec Loss:2143.8305664\tKL Div:7.7801590\n",
      "Epoch 202\tIter    6\tLoss:2127.2915039\tRec Loss:2119.1416016\tKL Div:8.1498842\n",
      "Epoch 202\tIter   12\tLoss:2117.7038574\tRec Loss:2109.9826660\tKL Div:7.7211404\n",
      "Epoch 202\tIter   18\tLoss:2139.6135254\tRec Loss:2131.5869141\tKL Div:8.0266304\n",
      "Epoch 203\tIter    6\tLoss:2190.6445312\tRec Loss:2182.5507812\tKL Div:8.0938702\n",
      "Epoch 203\tIter   12\tLoss:2156.1032715\tRec Loss:2148.2246094\tKL Div:7.8787661\n",
      "Epoch 203\tIter   18\tLoss:2143.9829102\tRec Loss:2136.1447754\tKL Div:7.8380213\n",
      "Epoch 204\tIter    6\tLoss:2118.1782227\tRec Loss:2110.1528320\tKL Div:8.0253506\n",
      "Epoch 204\tIter   12\tLoss:2183.4975586\tRec Loss:2175.5551758\tKL Div:7.9422932\n",
      "Epoch 204\tIter   18\tLoss:2128.7460938\tRec Loss:2121.0280762\tKL Div:7.7179685\n",
      "Epoch 205\tIter    6\tLoss:2123.0971680\tRec Loss:2115.1735840\tKL Div:7.9235630\n",
      "Epoch 205\tIter   12\tLoss:2136.6674805\tRec Loss:2128.7692871\tKL Div:7.8981576\n",
      "Epoch 205\tIter   18\tLoss:2140.8557129\tRec Loss:2133.0576172\tKL Div:7.7980275\n",
      "Epoch 206\tIter    6\tLoss:2159.7336426\tRec Loss:2151.7290039\tKL Div:8.0046492\n",
      "Epoch 206\tIter   12\tLoss:2143.9577637\tRec Loss:2136.1562500\tKL Div:7.8015885\n",
      "Epoch 206\tIter   18\tLoss:2118.3195801\tRec Loss:2110.3957520\tKL Div:7.9237404\n",
      "Epoch 207\tIter    6\tLoss:2122.8369141\tRec Loss:2114.9567871\tKL Div:7.8800755\n",
      "Epoch 207\tIter   12\tLoss:2145.5009766\tRec Loss:2137.3666992\tKL Div:8.1341915\n",
      "Epoch 207\tIter   18\tLoss:2189.1884766\tRec Loss:2181.2910156\tKL Div:7.8973994\n",
      "Epoch 208\tIter    6\tLoss:2150.3149414\tRec Loss:2142.1923828\tKL Div:8.1224422\n",
      "Epoch 208\tIter   12\tLoss:2157.9318848\tRec Loss:2150.1738281\tKL Div:7.7579513\n",
      "Epoch 208\tIter   18\tLoss:2156.6931152\tRec Loss:2148.7294922\tKL Div:7.9636936\n",
      "Epoch 209\tIter    6\tLoss:2120.5417480\tRec Loss:2112.6772461\tKL Div:7.8644090\n",
      "Epoch 209\tIter   12\tLoss:2090.6218262\tRec Loss:2082.6118164\tKL Div:8.0101290\n",
      "Epoch 209\tIter   18\tLoss:2138.3701172\tRec Loss:2130.5410156\tKL Div:7.8290882\n",
      "Epoch 210\tIter    6\tLoss:2147.1569824\tRec Loss:2139.0527344\tKL Div:8.1043663\n",
      "Epoch 210\tIter   12\tLoss:2167.9599609\tRec Loss:2160.0187988\tKL Div:7.9411569\n",
      "Epoch 210\tIter   18\tLoss:2118.4294434\tRec Loss:2110.3120117\tKL Div:8.1173668\n",
      "Epoch 211\tIter    6\tLoss:2133.9604492\tRec Loss:2126.0625000\tKL Div:7.8978872\n",
      "Epoch 211\tIter   12\tLoss:2117.8818359\tRec Loss:2110.0800781\tKL Div:7.8018742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211\tIter   18\tLoss:2176.2253418\tRec Loss:2168.2624512\tKL Div:7.9627790\n",
      "Epoch 212\tIter    6\tLoss:2120.6948242\tRec Loss:2112.6567383\tKL Div:8.0381174\n",
      "Epoch 212\tIter   12\tLoss:2139.3547363\tRec Loss:2131.6064453\tKL Div:7.7482624\n",
      "Epoch 212\tIter   18\tLoss:2101.3669434\tRec Loss:2093.3002930\tKL Div:8.0667639\n",
      "Epoch 213\tIter    6\tLoss:2074.7827148\tRec Loss:2066.9304199\tKL Div:7.8522344\n",
      "Epoch 213\tIter   12\tLoss:2129.4018555\tRec Loss:2121.4355469\tKL Div:7.9662075\n",
      "Epoch 213\tIter   18\tLoss:2114.7331543\tRec Loss:2106.7812500\tKL Div:7.9519491\n",
      "Epoch 214\tIter    6\tLoss:2138.4899902\tRec Loss:2130.3208008\tKL Div:8.1691113\n",
      "Epoch 214\tIter   12\tLoss:2125.6020508\tRec Loss:2117.7250977\tKL Div:7.8768373\n",
      "Epoch 214\tIter   18\tLoss:2135.0827637\tRec Loss:2127.0957031\tKL Div:7.9871297\n",
      "Epoch 215\tIter    6\tLoss:2132.2382812\tRec Loss:2124.2102051\tKL Div:8.0280294\n",
      "Epoch 215\tIter   12\tLoss:2138.7624512\tRec Loss:2130.7592773\tKL Div:8.0031605\n",
      "Epoch 215\tIter   18\tLoss:2131.2817383\tRec Loss:2123.4096680\tKL Div:7.8719683\n",
      "Epoch 216\tIter    6\tLoss:2133.3090820\tRec Loss:2125.2084961\tKL Div:8.1004734\n",
      "Epoch 216\tIter   12\tLoss:2133.7175293\tRec Loss:2125.7583008\tKL Div:7.9592872\n",
      "Epoch 216\tIter   18\tLoss:2130.8085938\tRec Loss:2122.8139648\tKL Div:7.9947290\n",
      "Epoch 217\tIter    6\tLoss:2135.5266113\tRec Loss:2127.4938965\tKL Div:8.0327148\n",
      "Epoch 217\tIter   12\tLoss:2096.9624023\tRec Loss:2089.0700684\tKL Div:7.8922625\n",
      "Epoch 217\tIter   18\tLoss:2142.6762695\tRec Loss:2134.7797852\tKL Div:7.8964939\n",
      "Epoch 218\tIter    6\tLoss:2108.6269531\tRec Loss:2100.6489258\tKL Div:7.9781141\n",
      "Epoch 218\tIter   12\tLoss:2156.8569336\tRec Loss:2148.9550781\tKL Div:7.9017506\n",
      "Epoch 218\tIter   18\tLoss:2153.7834473\tRec Loss:2145.7414551\tKL Div:8.0419312\n",
      "Epoch 219\tIter    6\tLoss:2147.8742676\tRec Loss:2139.8330078\tKL Div:8.0411425\n",
      "Epoch 219\tIter   12\tLoss:2154.0046387\tRec Loss:2145.9106445\tKL Div:8.0940704\n",
      "Epoch 219\tIter   18\tLoss:2154.2717285\tRec Loss:2146.3708496\tKL Div:7.9009376\n",
      "Epoch 220\tIter    6\tLoss:2078.9804688\tRec Loss:2070.8930664\tKL Div:8.0874538\n",
      "Epoch 220\tIter   12\tLoss:2067.1154785\tRec Loss:2059.3046875\tKL Div:7.8106809\n",
      "Epoch 220\tIter   18\tLoss:2137.4282227\tRec Loss:2129.3608398\tKL Div:8.0673027\n",
      "Epoch 221\tIter    6\tLoss:2102.4387207\tRec Loss:2094.3833008\tKL Div:8.0555334\n",
      "Epoch 221\tIter   12\tLoss:2127.5148926\tRec Loss:2119.6052246\tKL Div:7.9096627\n",
      "Epoch 221\tIter   18\tLoss:2139.6779785\tRec Loss:2131.4782715\tKL Div:8.1997147\n",
      "Epoch 222\tIter    6\tLoss:2146.8112793\tRec Loss:2138.7717285\tKL Div:8.0394773\n",
      "Epoch 222\tIter   12\tLoss:2126.5698242\tRec Loss:2118.5527344\tKL Div:8.0171356\n",
      "Epoch 222\tIter   18\tLoss:2113.8212891\tRec Loss:2105.9897461\tKL Div:7.8315110\n",
      "Epoch 223\tIter    6\tLoss:2134.3645020\tRec Loss:2126.1892090\tKL Div:8.1752062\n",
      "Epoch 223\tIter   12\tLoss:2143.9287109\tRec Loss:2136.0400391\tKL Div:7.8886166\n",
      "Epoch 223\tIter   18\tLoss:2150.9934082\tRec Loss:2142.8168945\tKL Div:8.1765747\n",
      "Epoch 224\tIter    6\tLoss:2187.8830566\tRec Loss:2179.8417969\tKL Div:8.0411577\n",
      "Epoch 224\tIter   12\tLoss:2141.3613281\tRec Loss:2133.1201172\tKL Div:8.2412090\n",
      "Epoch 224\tIter   18\tLoss:2109.8188477\tRec Loss:2101.8088379\tKL Div:8.0100212\n",
      "Epoch 225\tIter    6\tLoss:2128.0571289\tRec Loss:2119.9467773\tKL Div:8.1103582\n",
      "Epoch 225\tIter   12\tLoss:2164.7028809\tRec Loss:2156.6857910\tKL Div:8.0171900\n",
      "Epoch 225\tIter   18\tLoss:2131.0158691\tRec Loss:2122.9357910\tKL Div:8.0800781\n",
      "Epoch 226\tIter    6\tLoss:2147.6884766\tRec Loss:2139.6132812\tKL Div:8.0751390\n",
      "Epoch 226\tIter   12\tLoss:2193.6845703\tRec Loss:2185.5720215\tKL Div:8.1124706\n",
      "Epoch 226\tIter   18\tLoss:2111.5566406\tRec Loss:2103.4389648\tKL Div:8.1176701\n",
      "Epoch 227\tIter    6\tLoss:2116.2722168\tRec Loss:2108.3347168\tKL Div:7.9373894\n",
      "Epoch 227\tIter   12\tLoss:2162.4843750\tRec Loss:2154.3554688\tKL Div:8.1290216\n",
      "Epoch 227\tIter   18\tLoss:2155.0771484\tRec Loss:2146.8791504\tKL Div:8.1978836\n",
      "Epoch 228\tIter    6\tLoss:2127.0898438\tRec Loss:2119.1313477\tKL Div:7.9586000\n",
      "Epoch 228\tIter   12\tLoss:2143.3964844\tRec Loss:2135.1989746\tKL Div:8.1975422\n",
      "Epoch 228\tIter   18\tLoss:2155.2185059\tRec Loss:2147.3012695\tKL Div:7.9173484\n",
      "Epoch 229\tIter    6\tLoss:2110.3850098\tRec Loss:2102.3066406\tKL Div:8.0784092\n",
      "Epoch 229\tIter   12\tLoss:2157.9155273\tRec Loss:2149.8461914\tKL Div:8.0693321\n",
      "Epoch 229\tIter   18\tLoss:2138.4992676\tRec Loss:2130.3986816\tKL Div:8.1005812\n",
      "Epoch 230\tIter    6\tLoss:2122.0424805\tRec Loss:2113.9829102\tKL Div:8.0596275\n",
      "Epoch 230\tIter   12\tLoss:2119.2722168\tRec Loss:2111.2656250\tKL Div:8.0064726\n",
      "Epoch 230\tIter   18\tLoss:2111.8625488\tRec Loss:2103.6774902\tKL Div:8.1849966\n",
      "Epoch 231\tIter    6\tLoss:2178.3469238\tRec Loss:2170.1816406\tKL Div:8.1653833\n",
      "Epoch 231\tIter   12\tLoss:2103.1711426\tRec Loss:2095.2792969\tKL Div:7.8918772\n",
      "Epoch 231\tIter   18\tLoss:2144.1770020\tRec Loss:2135.9111328\tKL Div:8.2658091\n",
      "Epoch 232\tIter    6\tLoss:2145.3359375\tRec Loss:2137.1875000\tKL Div:8.1484241\n",
      "Epoch 232\tIter   12\tLoss:2133.5561523\tRec Loss:2125.4165039\tKL Div:8.1395473\n",
      "Epoch 232\tIter   18\tLoss:2171.5573730\tRec Loss:2163.4785156\tKL Div:8.0788937\n",
      "Epoch 233\tIter    6\tLoss:2129.0898438\tRec Loss:2120.9213867\tKL Div:8.1683578\n",
      "Epoch 233\tIter   12\tLoss:2128.3813477\tRec Loss:2120.3349609\tKL Div:8.0463400\n",
      "Epoch 233\tIter   18\tLoss:2116.7629395\tRec Loss:2108.8039551\tKL Div:7.9590793\n",
      "Epoch 234\tIter    6\tLoss:2153.5117188\tRec Loss:2145.2709961\tKL Div:8.2407475\n",
      "Epoch 234\tIter   12\tLoss:2110.2004395\tRec Loss:2102.1469727\tKL Div:8.0535431\n",
      "Epoch 234\tIter   18\tLoss:2127.8178711\tRec Loss:2119.6857910\tKL Div:8.1320152\n",
      "Epoch 235\tIter    6\tLoss:2151.1821289\tRec Loss:2143.0229492\tKL Div:8.1592712\n",
      "Epoch 235\tIter   12\tLoss:2114.1743164\tRec Loss:2106.1589355\tKL Div:8.0154333\n",
      "Epoch 235\tIter   18\tLoss:2140.6801758\tRec Loss:2132.4697266\tKL Div:8.2103815\n",
      "Epoch 236\tIter    6\tLoss:2133.7944336\tRec Loss:2125.6794434\tKL Div:8.1150684\n",
      "Epoch 236\tIter   12\tLoss:2136.8752441\tRec Loss:2128.8286133\tKL Div:8.0466022\n",
      "Epoch 236\tIter   18\tLoss:2118.8696289\tRec Loss:2110.8154297\tKL Div:8.0541763\n",
      "Epoch 237\tIter    6\tLoss:2132.9465332\tRec Loss:2124.6818848\tKL Div:8.2646513\n",
      "Epoch 237\tIter   12\tLoss:2104.6992188\tRec Loss:2096.6665039\tKL Div:8.0326500\n",
      "Epoch 237\tIter   18\tLoss:2163.2595215\tRec Loss:2155.0527344\tKL Div:8.2066917\n",
      "Epoch 238\tIter    6\tLoss:2133.0141602\tRec Loss:2124.7985840\tKL Div:8.2156124\n",
      "Epoch 238\tIter   12\tLoss:2142.9099121\tRec Loss:2134.8300781\tKL Div:8.0798664\n",
      "Epoch 238\tIter   18\tLoss:2179.7243652\tRec Loss:2171.4709473\tKL Div:8.2534828\n",
      "Epoch 239\tIter    6\tLoss:2117.7075195\tRec Loss:2109.4238281\tKL Div:8.2836294\n",
      "Epoch 239\tIter   12\tLoss:2130.0378418\tRec Loss:2122.0427246\tKL Div:7.9950204\n",
      "Epoch 239\tIter   18\tLoss:2111.6923828\tRec Loss:2103.4096680\tKL Div:8.2827358\n",
      "Epoch 240\tIter    6\tLoss:2121.3701172\tRec Loss:2113.1948242\tKL Div:8.1752310\n",
      "Epoch 240\tIter   12\tLoss:2114.2915039\tRec Loss:2106.0996094\tKL Div:8.1920023\n",
      "Epoch 240\tIter   18\tLoss:2154.9860840\tRec Loss:2146.8554688\tKL Div:8.1306362\n",
      "Epoch 240\tLoss:2130.5631400\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  254,   573,   454,   319,   235,    97, 13238,  1761,  1307,    97,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drum', 'noise', 'drone', 'melody', 'build', 'bass', 'tone', 'close', 'slow', 'beat', 'open', 'heavy', 'space', 'sense', 'light']\n",
      "['melody', 'indie', 'lyric', 'love', 'chorus', 'harmony', 'debut', 'group', 'bit', 'pretty', 'piano', 'line', 'arrangement', 'write', 'hook']\n",
      "['version', 'live', 'set', 'disc', 'include', 'record', 'studio', 'cover', 'original', 'label', 'recording', 'group', 'soul', 'early', 'reissue']\n",
      "['line', 'turn', 'world', 'close', 'love', 'listen', 'leave', 'title', 'kind', 'lyric', 'word', 'start', 'place', 'point', 'let']\n",
      "['dance', 'house', 'beat', 'mix', 'label', 'techno', 'bass', 'producer', 'dj', 'synth', 'artist', 'club', 'drum', 'remix', 'electronic']\n",
      "['love', 'line', 'world', 'turn', 'close', 'title', 'kind', 'point', 'year', 'leave', 'debut', 'listen', 'hard', 'place', 'sense']\n",
      "['\\xa0', 'project', 'point', 'early', 'synth', 'solo', 'title', 'artist', 'ep', 'year', 'producer', 'sense', 'place', 'close', 'lyric']\n",
      "['band', 'drum', 'riff', 'bass', 'hook', 'chorus', 'debut', 'melody', 'line', 'beat', 'punk', 'ep', 'lyric', 'rhythm', 'pretty']\n",
      "['sing', 'love', 'lyric', 'line', 'life', 'word', 'world', 'feeling', 'emotional', 'dream', 'sense', 'close', 'place', 'write', 'debut']\n",
      "['love', 'punk', 'band', 'cover', 'live', 'solo', 'day', 'fan', 'man', 'bad', 'guy', 'big', 'single', 'get', 'year']\n",
      "['drum', 'melody', 'disc', 'instrumental', 'jazz', 'beat', 'listen', 'bass', 'bit', 'interesting', 'begin', 'feature', 'noise', 'people', 'piece']\n",
      "['life', 'love', 'world', 'man', 'people', 'write', 'sing', 'woman', 'story', 'word', 'death', 'black', 'live', 'tell', 'die']\n",
      "['folk', 'sing', 'country', 'lyric', 'love', 'solo', 'string', 'piano', 'blue', 'arrangement', 'line', 'acoustic', 'record', 'cover', 'word']\n",
      "['dance', 'people', 'bad', 'beat', 'party', 'man', 'guy', 'listen', 'get', 'girl', 'kid', 'fun', 'disco', 'love', 'single']\n",
      "['piece', 'electronic', 'piano', 'world', 'drone', 'ambient', 'composition', 'instrument', 'tone', 'composer', 'sense', 'musician', 'create', 'space', 'note']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'production', 'verse', 'year', 'producer', 'flow', 'style', 'feature', 'sample', 'line', 'artist']\n",
      "['synth', 'ep', 'beat', 'electronic', 'melody', 'debut', 'sense', 'production', 'kind', 'instrumental', 'sample', 'close', 'texture', 'project', 'duo']\n",
      "['lyric', 'love', 'title', 'kind', 'indie_rock', 'chorus', 'band', 'life', 'big', 'punk', 'debut', 'line', 'indie', 'people', 'hook']\n",
      "['metal', 'punk', 'riff', 'hardcore', 'band', 'black_metal', 'death', 'noise', 'group', 'drummer', 'early', 'live', 'guitarist', 'black', 'scream']\n",
      "['love', 'r&b', 'single', 'hit', 'singer', 'artist', 'star', 'year', 'girl', 'debut', 'soul', 'big', 'producer', 'dance', 'sing']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  254,   573,   454,   319,   235,    97, 13238,  1761,  1307,    97,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  254,   573,   454,   319,   235,    97, 13238,  1761,  1307,    97,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.4666666666666667\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drum\n",
      "noise\n",
      "drone\n",
      "melody\n",
      "build\n",
      "bass\n",
      "tone\n",
      "close\n",
      "slow\n",
      "beat\n",
      "open\n",
      "heavy\n",
      "sense\n",
      "space\n",
      "light\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence c_uci 0.04244011620489339\n",
      "coherence c_npmi 0.012994535830163259\n",
      "coherence c_cv 0.37721913049949013\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  254,   573,   454,   319,   235,    97, 13238,  1761,  1307,    97,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 241\tIter    6\tLoss:2122.7863770\tRec Loss:2114.5065918\tKL Div:8.2798128\n",
      "Epoch 241\tIter   12\tLoss:2151.7229004\tRec Loss:2143.6140137\tKL Div:8.1087837\n",
      "Epoch 241\tIter   18\tLoss:2135.4602051\tRec Loss:2127.4072266\tKL Div:8.0529060\n",
      "Epoch 242\tIter    6\tLoss:2087.1010742\tRec Loss:2078.8903809\tKL Div:8.2106342\n",
      "Epoch 242\tIter   12\tLoss:2114.3957520\tRec Loss:2106.1599121\tKL Div:8.2357779\n",
      "Epoch 242\tIter   18\tLoss:2094.0087891\tRec Loss:2086.1298828\tKL Div:7.8789454\n",
      "Epoch 243\tIter    6\tLoss:2183.2250977\tRec Loss:2174.8146973\tKL Div:8.4103374\n",
      "Epoch 243\tIter   12\tLoss:2118.6550293\tRec Loss:2110.6582031\tKL Div:7.9968643\n",
      "Epoch 243\tIter   18\tLoss:2098.4992676\tRec Loss:2090.3190918\tKL Div:8.1801577\n",
      "Epoch 244\tIter    6\tLoss:2124.5625000\tRec Loss:2116.3278809\tKL Div:8.2346973\n",
      "Epoch 244\tIter   12\tLoss:2167.7863770\tRec Loss:2159.5197754\tKL Div:8.2666006\n",
      "Epoch 244\tIter   18\tLoss:2137.9147949\tRec Loss:2129.7739258\tKL Div:8.1408806\n",
      "Epoch 245\tIter    6\tLoss:2145.6862793\tRec Loss:2137.4184570\tKL Div:8.2679348\n",
      "Epoch 245\tIter   12\tLoss:2102.8862305\tRec Loss:2094.7539062\tKL Div:8.1322136\n",
      "Epoch 245\tIter   18\tLoss:2119.0495605\tRec Loss:2110.9270020\tKL Div:8.1225863\n",
      "Epoch 246\tIter    6\tLoss:2131.8894043\tRec Loss:2123.5354004\tKL Div:8.3540583\n",
      "Epoch 246\tIter   12\tLoss:2137.2585449\tRec Loss:2129.2143555\tKL Div:8.0442429\n",
      "Epoch 246\tIter   18\tLoss:2118.3662109\tRec Loss:2110.1062012\tKL Div:8.2599583\n",
      "Epoch 247\tIter    6\tLoss:2135.5361328\tRec Loss:2127.3444824\tKL Div:8.1915722\n",
      "Epoch 247\tIter   12\tLoss:2129.2148438\tRec Loss:2121.0187988\tKL Div:8.1959400\n",
      "Epoch 247\tIter   18\tLoss:2154.2263184\tRec Loss:2146.0732422\tKL Div:8.1529989\n",
      "Epoch 248\tIter    6\tLoss:2146.8117676\tRec Loss:2138.5039062\tKL Div:8.3078232\n",
      "Epoch 248\tIter   12\tLoss:2123.3239746\tRec Loss:2115.2895508\tKL Div:8.0343666\n",
      "Epoch 248\tIter   18\tLoss:2153.5524902\tRec Loss:2145.0751953\tKL Div:8.4772263\n",
      "Epoch 249\tIter    6\tLoss:2122.9187012\tRec Loss:2114.8347168\tKL Div:8.0838947\n",
      "Epoch 249\tIter   12\tLoss:2106.2038574\tRec Loss:2097.9140625\tKL Div:8.2898026\n",
      "Epoch 249\tIter   18\tLoss:2166.9626465\tRec Loss:2158.6572266\tKL Div:8.3054562\n",
      "Epoch 250\tIter    6\tLoss:2151.8793945\tRec Loss:2143.6611328\tKL Div:8.2183046\n",
      "Epoch 250\tIter   12\tLoss:2073.3298340\tRec Loss:2065.1159668\tKL Div:8.2138691\n",
      "Epoch 250\tIter   18\tLoss:2130.1101074\tRec Loss:2121.8393555\tKL Div:8.2706671\n",
      "Epoch 251\tIter    6\tLoss:2164.5280762\tRec Loss:2156.4152832\tKL Div:8.1127129\n",
      "Epoch 251\tIter   12\tLoss:2112.0622559\tRec Loss:2103.8349609\tKL Div:8.2274017\n",
      "Epoch 251\tIter   18\tLoss:2110.7077637\tRec Loss:2102.4453125\tKL Div:8.2625198\n",
      "Epoch 252\tIter    6\tLoss:2116.2666016\tRec Loss:2108.0615234\tKL Div:8.2050343\n",
      "Epoch 252\tIter   12\tLoss:2137.9123535\tRec Loss:2129.6359863\tKL Div:8.2763348\n",
      "Epoch 252\tIter   18\tLoss:2127.9152832\tRec Loss:2119.7045898\tKL Div:8.2105770\n",
      "Epoch 253\tIter    6\tLoss:2131.6245117\tRec Loss:2123.4287109\tKL Div:8.1957054\n",
      "Epoch 253\tIter   12\tLoss:2102.6149902\tRec Loss:2094.3125000\tKL Div:8.3024979\n",
      "Epoch 253\tIter   18\tLoss:2115.7880859\tRec Loss:2107.7375488\tKL Div:8.0505085\n",
      "Epoch 254\tIter    6\tLoss:2119.6437988\tRec Loss:2111.3325195\tKL Div:8.3113451\n",
      "Epoch 254\tIter   12\tLoss:2146.1669922\tRec Loss:2137.8339844\tKL Div:8.3329182\n",
      "Epoch 254\tIter   18\tLoss:2155.1972656\tRec Loss:2147.0119629\tKL Div:8.1851940\n",
      "Epoch 255\tIter    6\tLoss:2127.3244629\tRec Loss:2119.0891113\tKL Div:8.2354698\n",
      "Epoch 255\tIter   12\tLoss:2129.6591797\tRec Loss:2121.3701172\tKL Div:8.2891331\n",
      "Epoch 255\tIter   18\tLoss:2178.4050293\tRec Loss:2170.1743164\tKL Div:8.2306442\n",
      "Epoch 256\tIter    6\tLoss:2082.9155273\tRec Loss:2074.7189941\tKL Div:8.1965971\n",
      "Epoch 256\tIter   12\tLoss:2131.2048340\tRec Loss:2122.9304199\tKL Div:8.2744770\n",
      "Epoch 256\tIter   18\tLoss:2098.9465332\tRec Loss:2090.7026367\tKL Div:8.2438145\n",
      "Epoch 257\tIter    6\tLoss:2179.7509766\tRec Loss:2171.1752930\tKL Div:8.5757847\n",
      "Epoch 257\tIter   12\tLoss:2093.8322754\tRec Loss:2085.7099609\tKL Div:8.1222830\n",
      "Epoch 257\tIter   18\tLoss:2097.5554199\tRec Loss:2089.1762695\tKL Div:8.3792400\n",
      "Epoch 258\tIter    6\tLoss:2149.7248535\tRec Loss:2141.3762207\tKL Div:8.3485718\n",
      "Epoch 258\tIter   12\tLoss:2162.8601074\tRec Loss:2154.6037598\tKL Div:8.2564278\n",
      "Epoch 258\tIter   18\tLoss:2124.3149414\tRec Loss:2116.0361328\tKL Div:8.2787495\n",
      "Epoch 259\tIter    6\tLoss:2147.2497559\tRec Loss:2138.7111816\tKL Div:8.5386200\n",
      "Epoch 259\tIter   12\tLoss:2136.9858398\tRec Loss:2128.6853027\tKL Div:8.3005838\n",
      "Epoch 259\tIter   18\tLoss:2111.0224609\tRec Loss:2102.7397461\tKL Div:8.2827759\n",
      "Epoch 260\tIter    6\tLoss:2108.4440918\tRec Loss:2100.1406250\tKL Div:8.3035812\n",
      "Epoch 260\tIter   12\tLoss:2123.4343262\tRec Loss:2115.1496582\tKL Div:8.2845993\n",
      "Epoch 260\tIter   18\tLoss:2104.4265137\tRec Loss:2096.0815430\tKL Div:8.3449011\n",
      "Epoch 261\tIter    6\tLoss:2127.0712891\tRec Loss:2118.7795410\tKL Div:8.2916946\n",
      "Epoch 261\tIter   12\tLoss:2151.2336426\tRec Loss:2143.0195312\tKL Div:8.2141113\n",
      "Epoch 261\tIter   18\tLoss:2138.9746094\tRec Loss:2130.5317383\tKL Div:8.4429913\n",
      "Epoch 262\tIter    6\tLoss:2162.2712402\tRec Loss:2153.7363281\tKL Div:8.5348606\n",
      "Epoch 262\tIter   12\tLoss:2141.8271484\tRec Loss:2133.6677246\tKL Div:8.1593933\n",
      "Epoch 262\tIter   18\tLoss:2114.0488281\tRec Loss:2105.6284180\tKL Div:8.4204512\n",
      "Epoch 263\tIter    6\tLoss:2163.8618164\tRec Loss:2155.4301758\tKL Div:8.4317093\n",
      "Epoch 263\tIter   12\tLoss:2116.8552246\tRec Loss:2108.4753418\tKL Div:8.3799191\n",
      "Epoch 263\tIter   18\tLoss:2134.6752930\tRec Loss:2126.2097168\tKL Div:8.4656019\n",
      "Epoch 264\tIter    6\tLoss:2099.7011719\tRec Loss:2091.5285645\tKL Div:8.1726961\n",
      "Epoch 264\tIter   12\tLoss:2157.6086426\tRec Loss:2149.1713867\tKL Div:8.4371862\n",
      "Epoch 264\tIter   18\tLoss:2137.5527344\tRec Loss:2129.3361816\tKL Div:8.2164621\n",
      "Epoch 265\tIter    6\tLoss:2137.5588379\tRec Loss:2128.9426270\tKL Div:8.6163244\n",
      "Epoch 265\tIter   12\tLoss:2118.5107422\tRec Loss:2110.3105469\tKL Div:8.2001514\n",
      "Epoch 265\tIter   18\tLoss:2126.6667480\tRec Loss:2118.1250000\tKL Div:8.5417671\n",
      "Epoch 266\tIter    6\tLoss:2115.2080078\tRec Loss:2106.7998047\tKL Div:8.4082766\n",
      "Epoch 266\tIter   12\tLoss:2126.7951660\tRec Loss:2118.4775391\tKL Div:8.3175602\n",
      "Epoch 266\tIter   18\tLoss:2128.8669434\tRec Loss:2120.4536133\tKL Div:8.4132910\n",
      "Epoch 267\tIter    6\tLoss:2103.6379395\tRec Loss:2095.2485352\tKL Div:8.3892899\n",
      "Epoch 267\tIter   12\tLoss:2138.7148438\tRec Loss:2130.2712402\tKL Div:8.4436607\n",
      "Epoch 267\tIter   18\tLoss:2102.9802246\tRec Loss:2094.6474609\tKL Div:8.3327732\n",
      "Epoch 268\tIter    6\tLoss:2140.0029297\tRec Loss:2131.4943848\tKL Div:8.5085640\n",
      "Epoch 268\tIter   12\tLoss:2125.2377930\tRec Loss:2116.9526367\tKL Div:8.2852058\n",
      "Epoch 268\tIter   18\tLoss:2140.5712891\tRec Loss:2132.1020508\tKL Div:8.4693565\n",
      "Epoch 269\tIter    6\tLoss:2102.6452637\tRec Loss:2094.1098633\tKL Div:8.5354643\n",
      "Epoch 269\tIter   12\tLoss:2135.8879395\tRec Loss:2127.5673828\tKL Div:8.3204536\n",
      "Epoch 269\tIter   18\tLoss:2089.5537109\tRec Loss:2081.0898438\tKL Div:8.4639740\n",
      "Epoch 270\tIter    6\tLoss:2185.6235352\tRec Loss:2177.2358398\tKL Div:8.3877058\n",
      "Epoch 270\tIter   12\tLoss:2113.0520020\tRec Loss:2104.6528320\tKL Div:8.3991346\n",
      "Epoch 270\tIter   18\tLoss:2134.5766602\tRec Loss:2126.1237793\tKL Div:8.4529352\n",
      "Epoch 271\tIter    6\tLoss:2131.8527832\tRec Loss:2123.5007324\tKL Div:8.3521023\n",
      "Epoch 271\tIter   12\tLoss:2121.7846680\tRec Loss:2113.4824219\tKL Div:8.3021679\n",
      "Epoch 271\tIter   18\tLoss:2136.3178711\tRec Loss:2127.8413086\tKL Div:8.4764767\n",
      "Epoch 272\tIter    6\tLoss:2111.7204590\tRec Loss:2103.1862793\tKL Div:8.5341797\n",
      "Epoch 272\tIter   12\tLoss:2104.1564941\tRec Loss:2095.7705078\tKL Div:8.3858948\n",
      "Epoch 272\tIter   18\tLoss:2165.1840820\tRec Loss:2156.6591797\tKL Div:8.5247993\n",
      "Epoch 273\tIter    6\tLoss:2115.9506836\tRec Loss:2107.5144043\tKL Div:8.4363308\n",
      "Epoch 273\tIter   12\tLoss:2150.4038086\tRec Loss:2141.8503418\tKL Div:8.5534058\n",
      "Epoch 273\tIter   18\tLoss:2082.9050293\tRec Loss:2074.4995117\tKL Div:8.4055948\n",
      "Epoch 274\tIter    6\tLoss:2128.9111328\tRec Loss:2120.4487305\tKL Div:8.4624691\n",
      "Epoch 274\tIter   12\tLoss:2115.7775879\tRec Loss:2107.2836914\tKL Div:8.4939156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274\tIter   18\tLoss:2132.3552246\tRec Loss:2123.9541016\tKL Div:8.4011984\n",
      "Epoch 275\tIter    6\tLoss:2163.8642578\tRec Loss:2155.2937012\tKL Div:8.5705404\n",
      "Epoch 275\tIter   12\tLoss:2139.1665039\tRec Loss:2130.8027344\tKL Div:8.3637381\n",
      "Epoch 275\tIter   18\tLoss:2092.0336914\tRec Loss:2083.6022949\tKL Div:8.4314384\n",
      "Epoch 276\tIter    6\tLoss:2171.5466309\tRec Loss:2163.0590820\tKL Div:8.4875088\n",
      "Epoch 276\tIter   12\tLoss:2133.5029297\tRec Loss:2124.8901367\tKL Div:8.6129074\n",
      "Epoch 276\tIter   18\tLoss:2147.0410156\tRec Loss:2138.7275391\tKL Div:8.3133621\n",
      "Epoch 277\tIter    6\tLoss:2120.5515137\tRec Loss:2111.8869629\tKL Div:8.6646414\n",
      "Epoch 277\tIter   12\tLoss:2106.9912109\tRec Loss:2098.5522461\tKL Div:8.4390793\n",
      "Epoch 277\tIter   18\tLoss:2133.3508301\tRec Loss:2124.8208008\tKL Div:8.5300827\n",
      "Epoch 278\tIter    6\tLoss:2127.4787598\tRec Loss:2118.9370117\tKL Div:8.5418377\n",
      "Epoch 278\tIter   12\tLoss:2210.9179688\tRec Loss:2202.3627930\tKL Div:8.5551939\n",
      "Epoch 278\tIter   18\tLoss:2162.4301758\tRec Loss:2154.0141602\tKL Div:8.4159794\n",
      "Epoch 279\tIter    6\tLoss:2132.8237305\tRec Loss:2124.2453613\tKL Div:8.5783539\n",
      "Epoch 279\tIter   12\tLoss:2120.7365723\tRec Loss:2112.2978516\tKL Div:8.4387293\n",
      "Epoch 279\tIter   18\tLoss:2105.4438477\tRec Loss:2097.0012207\tKL Div:8.4426126\n",
      "Epoch 280\tIter    6\tLoss:2135.3828125\tRec Loss:2126.9072266\tKL Div:8.4756718\n",
      "Epoch 280\tIter   12\tLoss:2118.7966309\tRec Loss:2110.3254395\tKL Div:8.4711332\n",
      "Epoch 280\tIter   18\tLoss:2174.0617676\tRec Loss:2165.6264648\tKL Div:8.4354134\n",
      "Epoch 280\tLoss:2131.7365126\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,   916,   454,    99,  1114,    97, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['noise', 'drone', 'drum', 'tone', 'build', 'bass', 'close', 'melody', 'slow', 'light', 'sense', 'space', 'piece', 'beat', 'open']\n",
      "['indie', 'love', 'melody', 'lyric', 'harmony', 'debut', 'group', 'chorus', 'arrangement', 'piano', 'bit', 'write', 'pretty', 'line', 'day']\n",
      "['version', 'live', 'set', 'disc', 'include', 'record', 'cover', 'recording', 'original', 'studio', 'label', 'group', 'compilation', 'soul', 'early']\n",
      "['lyric', 'man', 'line', 'listen', 'kid', 'day', 'love', 'bad', 'get', 'guy', 'word', 'people', 'let', 'wait', 'start']\n",
      "['house', 'dance', 'mix', 'beat', 'label', 'techno', 'bass', 'producer', 'dj', 'synth', 'artist', 'club', 'drum', 'electronic', 'sample']\n",
      "['love', 'line', 'turn', 'close', 'world', 'title', 'kind', 'year', 'point', 'leave', 'hard', 'start', 'sense', 'listen', 'big']\n",
      "['\\xa0', 'project', 'point', 'solo', 'title', 'early', 'artist', 'year', 'ep', 'synth', 'close', 'producer', 'place', 'idea', 'lyric']\n",
      "['melody', 'riff', 'band', 'drum', 'debut', 'chorus', 'hook', 'bass', 'punk', 'line', 'ep', 'group', 'rhythm', 'post_punk', 'opener']\n",
      "['love', 'sing', 'lyric', 'life', 'world', 'word', 'line', 'feeling', 'write', 'close', 'dream', 'light', 'sense', 'leave', 'place']\n",
      "['punk', 'love', 'live', 'cover', 'band', 'solo', 'single', 'big', 'fan', 'day', 'classic', 'early', 'year', 'hit', 'bad']\n",
      "['drum', 'melody', 'disc', 'jazz', 'electronic', 'beat', 'bass', 'noise', 'instrumental', 'listen', 'piece', 'begin', 'interesting', 'bit', 'feature']\n",
      "['life', 'man', 'world', 'people', 'love', 'write', 'woman', 'black', 'sing', 'story', 'word', 'death', 'call', 'live', 'tell']\n",
      "['folk', 'country', 'solo', 'blue', 'sing', 'cover', 'lyric', 'acoustic', 'record', 'string', 'arrangement', 'piano', 'line', 'love', 'home']\n",
      "['dance', 'people', 'beat', 'party', 'bad', 'guy', 'fun', 'single', 'love', 'big', 'disco', 'get', 'girl', 'pretty', 'listen']\n",
      "['piece', 'electronic', 'piano', 'world', 'composition', 'composer', 'instrument', 'drone', 'ambient', 'film', 'sense', 'tone', 'musician', 'jazz', 'create']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'production', 'verse', 'year', 'flow', 'producer', 'style', 'feature', 'artist', 'line', 'sample']\n",
      "['synth', 'ep', 'beat', 'electronic', 'melody', 'debut', 'sense', 'kind', 'production', 'sample', 'close', 'instrumental', 'project', 'style', 'texture']\n",
      "['lyric', 'title', 'love', 'kind', 'band', 'indie_rock', 'chorus', 'punk', 'indie', 'life', 'people', 'point', 'hook', 'big', 'emo']\n",
      "['metal', 'punk', 'riff', 'hardcore', 'band', 'noise', 'black_metal', 'death', 'group', 'drummer', 'early', 'guitarist', 'black', 'scream', 'live']\n",
      "['love', 'r&b', 'single', 'hit', 'singer', 'artist', 'star', 'girl', 'year', 'producer', 'debut', 'soul', 'big', 'prince', 'dance']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,   916,   454,    99,  1114,    97, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,   916,   454,    99,  1114,    97, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.48\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "noise\n",
      "drone\n",
      "drum\n",
      "tone\n",
      "build\n",
      "bass\n",
      "close\n",
      "melody\n",
      "slow\n",
      "light\n",
      "sense\n",
      "space\n",
      "piece\n",
      "beat\n",
      "open\n",
      "coherence c_uci 0.023153001324700252\n",
      "coherence c_npmi 0.01358148912541261\n",
      "coherence c_cv 0.3813119208686954\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,   916,   454,    99,  1114,    97, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 281\tIter    6\tLoss:2125.3215332\tRec Loss:2116.6406250\tKL Div:8.6808720\n",
      "Epoch 281\tIter   12\tLoss:2185.1811523\tRec Loss:2176.6386719\tKL Div:8.5425615\n",
      "Epoch 281\tIter   18\tLoss:2160.4870605\tRec Loss:2151.8442383\tKL Div:8.6427212\n",
      "Epoch 282\tIter    6\tLoss:2091.9843750\tRec Loss:2083.4233398\tKL Div:8.5610771\n",
      "Epoch 282\tIter   12\tLoss:2178.2878418\tRec Loss:2169.7709961\tKL Div:8.5169392\n",
      "Epoch 282\tIter   18\tLoss:2067.9272461\tRec Loss:2059.5366211\tKL Div:8.3906088\n",
      "Epoch 283\tIter    6\tLoss:2162.2463379\tRec Loss:2153.6308594\tKL Div:8.6155634\n",
      "Epoch 283\tIter   12\tLoss:2114.8940430\tRec Loss:2106.4455566\tKL Div:8.4484091\n",
      "Epoch 283\tIter   18\tLoss:2145.4526367\tRec Loss:2136.8183594\tKL Div:8.6342163\n",
      "Epoch 284\tIter    6\tLoss:2122.6523438\tRec Loss:2113.9294434\tKL Div:8.7228661\n",
      "Epoch 284\tIter   12\tLoss:2154.0129395\tRec Loss:2145.6323242\tKL Div:8.3805580\n",
      "Epoch 284\tIter   18\tLoss:2148.9436035\tRec Loss:2140.2712402\tKL Div:8.6723070\n",
      "Epoch 285\tIter    6\tLoss:2151.8911133\tRec Loss:2143.4689941\tKL Div:8.4221029\n",
      "Epoch 285\tIter   12\tLoss:2095.3889160\tRec Loss:2086.8437500\tKL Div:8.5450583\n",
      "Epoch 285\tIter   18\tLoss:2114.4028320\tRec Loss:2105.9008789\tKL Div:8.5020237\n",
      "Epoch 286\tIter    6\tLoss:2150.8630371\tRec Loss:2142.1445312\tKL Div:8.7184181\n",
      "Epoch 286\tIter   12\tLoss:2164.8220215\tRec Loss:2156.3715820\tKL Div:8.4504604\n",
      "Epoch 286\tIter   18\tLoss:2093.1638184\tRec Loss:2084.7077637\tKL Div:8.4560738\n",
      "Epoch 287\tIter    6\tLoss:2108.0952148\tRec Loss:2099.4973145\tKL Div:8.5978985\n",
      "Epoch 287\tIter   12\tLoss:2119.1096191\tRec Loss:2110.6787109\tKL Div:8.4309883\n",
      "Epoch 287\tIter   18\tLoss:2141.2961426\tRec Loss:2132.7150879\tKL Div:8.5810585\n",
      "Epoch 288\tIter    6\tLoss:2126.9936523\tRec Loss:2118.4218750\tKL Div:8.5717335\n",
      "Epoch 288\tIter   12\tLoss:2117.5356445\tRec Loss:2109.0139160\tKL Div:8.5217266\n",
      "Epoch 288\tIter   18\tLoss:2189.3825684\tRec Loss:2180.8244629\tKL Div:8.5580826\n",
      "Epoch 289\tIter    6\tLoss:2150.6044922\tRec Loss:2142.0126953\tKL Div:8.5917549\n",
      "Epoch 289\tIter   12\tLoss:2097.7595215\tRec Loss:2089.1782227\tKL Div:8.5813007\n",
      "Epoch 289\tIter   18\tLoss:2119.8176270\tRec Loss:2111.3544922\tKL Div:8.4631977\n",
      "Epoch 290\tIter    6\tLoss:2143.7443848\tRec Loss:2135.0415039\tKL Div:8.7027769\n",
      "Epoch 290\tIter   12\tLoss:2165.1850586\tRec Loss:2156.4960938\tKL Div:8.6890373\n",
      "Epoch 290\tIter   18\tLoss:2149.1525879\tRec Loss:2140.6569824\tKL Div:8.4955482\n",
      "Epoch 291\tIter    6\tLoss:2129.3698730\tRec Loss:2120.7592773\tKL Div:8.6106815\n",
      "Epoch 291\tIter   12\tLoss:2142.3215332\tRec Loss:2133.7014160\tKL Div:8.6200390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291\tIter   18\tLoss:2110.2353516\tRec Loss:2101.6831055\tKL Div:8.5521317\n",
      "Epoch 292\tIter    6\tLoss:2128.5273438\tRec Loss:2119.8464355\tKL Div:8.6810074\n",
      "Epoch 292\tIter   12\tLoss:2136.9763184\tRec Loss:2128.5224609\tKL Div:8.4537926\n",
      "Epoch 292\tIter   18\tLoss:2074.5852051\tRec Loss:2066.1162109\tKL Div:8.4690800\n",
      "Epoch 293\tIter    6\tLoss:2139.2155762\tRec Loss:2130.6989746\tKL Div:8.5165844\n",
      "Epoch 293\tIter   12\tLoss:2112.7236328\tRec Loss:2104.1555176\tKL Div:8.5681534\n",
      "Epoch 293\tIter   18\tLoss:2119.8996582\tRec Loss:2111.3720703\tKL Div:8.5275640\n",
      "Epoch 294\tIter    6\tLoss:2151.1433105\tRec Loss:2142.4968262\tKL Div:8.6464005\n",
      "Epoch 294\tIter   12\tLoss:2095.0395508\tRec Loss:2086.4912109\tKL Div:8.5483589\n",
      "Epoch 294\tIter   18\tLoss:2144.8959961\tRec Loss:2136.2563477\tKL Div:8.6396446\n",
      "Epoch 295\tIter    6\tLoss:2171.0305176\tRec Loss:2162.4606934\tKL Div:8.5698318\n",
      "Epoch 295\tIter   12\tLoss:2084.5153809\tRec Loss:2075.9318848\tKL Div:8.5834885\n",
      "Epoch 295\tIter   18\tLoss:2160.2988281\tRec Loss:2151.6740723\tKL Div:8.6247463\n",
      "Epoch 296\tIter    6\tLoss:2159.4963379\tRec Loss:2150.8930664\tKL Div:8.6032333\n",
      "Epoch 296\tIter   12\tLoss:2095.1496582\tRec Loss:2086.6147461\tKL Div:8.5348549\n",
      "Epoch 296\tIter   18\tLoss:2175.7880859\tRec Loss:2167.1787109\tKL Div:8.6094532\n",
      "Epoch 297\tIter    6\tLoss:2131.3742676\tRec Loss:2122.6206055\tKL Div:8.7535524\n",
      "Epoch 297\tIter   12\tLoss:2120.6616211\tRec Loss:2112.2099609\tKL Div:8.4516277\n",
      "Epoch 297\tIter   18\tLoss:2124.3942871\tRec Loss:2115.7260742\tKL Div:8.6683102\n",
      "Epoch 298\tIter    6\tLoss:2139.2697754\tRec Loss:2130.7385254\tKL Div:8.5312767\n",
      "Epoch 298\tIter   12\tLoss:2120.3520508\tRec Loss:2111.7084961\tKL Div:8.6436272\n",
      "Epoch 298\tIter   18\tLoss:2149.7541504\tRec Loss:2141.1435547\tKL Div:8.6106987\n",
      "Epoch 299\tIter    6\tLoss:2104.7456055\tRec Loss:2096.0500488\tKL Div:8.6956291\n",
      "Epoch 299\tIter   12\tLoss:2120.4096680\tRec Loss:2111.8537598\tKL Div:8.5558739\n",
      "Epoch 299\tIter   18\tLoss:2129.5400391\tRec Loss:2120.9765625\tKL Div:8.5634289\n",
      "Epoch 300\tIter    6\tLoss:2129.5021973\tRec Loss:2120.8173828\tKL Div:8.6847219\n",
      "Epoch 300\tIter   12\tLoss:2153.6564941\tRec Loss:2145.0585938\tKL Div:8.5979252\n",
      "Epoch 300\tIter   18\tLoss:2152.8918457\tRec Loss:2144.2768555\tKL Div:8.6150646\n",
      "Epoch 301\tIter    6\tLoss:2137.6184082\tRec Loss:2128.8759766\tKL Div:8.7424850\n",
      "Epoch 301\tIter   12\tLoss:2116.9511719\tRec Loss:2108.4504395\tKL Div:8.5007839\n",
      "Epoch 301\tIter   18\tLoss:2130.9396973\tRec Loss:2122.4355469\tKL Div:8.5042114\n",
      "Epoch 302\tIter    6\tLoss:2102.4113770\tRec Loss:2093.7099609\tKL Div:8.7015209\n",
      "Epoch 302\tIter   12\tLoss:2095.8366699\tRec Loss:2087.3237305\tKL Div:8.5128813\n",
      "Epoch 302\tIter   18\tLoss:2108.8286133\tRec Loss:2100.2968750\tKL Div:8.5318432\n",
      "Epoch 303\tIter    6\tLoss:2092.1508789\tRec Loss:2083.4580078\tKL Div:8.6928110\n",
      "Epoch 303\tIter   12\tLoss:2136.2468262\tRec Loss:2127.7172852\tKL Div:8.5294781\n",
      "Epoch 303\tIter   18\tLoss:2133.0205078\tRec Loss:2124.4494629\tKL Div:8.5709591\n",
      "Epoch 304\tIter    6\tLoss:2130.2358398\tRec Loss:2121.6088867\tKL Div:8.6268911\n",
      "Epoch 304\tIter   12\tLoss:2128.4667969\tRec Loss:2119.8759766\tKL Div:8.5909042\n",
      "Epoch 304\tIter   18\tLoss:2111.7302246\tRec Loss:2103.0458984\tKL Div:8.6843243\n",
      "Epoch 305\tIter    6\tLoss:2153.2390137\tRec Loss:2144.6533203\tKL Div:8.5856304\n",
      "Epoch 305\tIter   12\tLoss:2083.5849609\tRec Loss:2074.9699707\tKL Div:8.6150131\n",
      "Epoch 305\tIter   18\tLoss:2118.4035645\tRec Loss:2109.8403320\tKL Div:8.5633135\n",
      "Epoch 306\tIter    6\tLoss:2140.6157227\tRec Loss:2131.9389648\tKL Div:8.6767693\n",
      "Epoch 306\tIter   12\tLoss:2135.4904785\tRec Loss:2127.0600586\tKL Div:8.4302988\n",
      "Epoch 306\tIter   18\tLoss:2137.1818848\tRec Loss:2128.4492188\tKL Div:8.7327480\n",
      "Epoch 307\tIter    6\tLoss:2098.6892090\tRec Loss:2090.0810547\tKL Div:8.6082096\n",
      "Epoch 307\tIter   12\tLoss:2129.3344727\tRec Loss:2120.7365723\tKL Div:8.5979557\n",
      "Epoch 307\tIter   18\tLoss:2141.8654785\tRec Loss:2133.1865234\tKL Div:8.6789770\n",
      "Epoch 308\tIter    6\tLoss:2110.6943359\tRec Loss:2102.1235352\tKL Div:8.5708485\n",
      "Epoch 308\tIter   12\tLoss:2148.2990723\tRec Loss:2139.5107422\tKL Div:8.7882786\n",
      "Epoch 308\tIter   18\tLoss:2092.3933105\tRec Loss:2083.8593750\tKL Div:8.5339575\n",
      "Epoch 309\tIter    6\tLoss:2147.0920410\tRec Loss:2138.2993164\tKL Div:8.7926054\n",
      "Epoch 309\tIter   12\tLoss:2141.9257812\tRec Loss:2133.4729004\tKL Div:8.4528294\n",
      "Epoch 309\tIter   18\tLoss:2081.5063477\tRec Loss:2072.8903809\tKL Div:8.6159744\n",
      "Epoch 310\tIter    6\tLoss:2110.5312500\tRec Loss:2102.0134277\tKL Div:8.5177794\n",
      "Epoch 310\tIter   12\tLoss:2145.8459473\tRec Loss:2137.1508789\tKL Div:8.6950245\n",
      "Epoch 310\tIter   18\tLoss:2164.0917969\tRec Loss:2155.4970703\tKL Div:8.5948277\n",
      "Epoch 311\tIter    6\tLoss:2121.9821777\tRec Loss:2113.3359375\tKL Div:8.6462955\n",
      "Epoch 311\tIter   12\tLoss:2120.7089844\tRec Loss:2112.0546875\tKL Div:8.6541948\n",
      "Epoch 311\tIter   18\tLoss:2114.9689941\tRec Loss:2106.2563477\tKL Div:8.7126350\n",
      "Epoch 312\tIter    6\tLoss:2124.3598633\tRec Loss:2115.6865234\tKL Div:8.6733246\n",
      "Epoch 312\tIter   12\tLoss:2131.9880371\tRec Loss:2123.2480469\tKL Div:8.7400064\n",
      "Epoch 312\tIter   18\tLoss:2151.7888184\tRec Loss:2143.1894531\tKL Div:8.5992985\n",
      "Epoch 313\tIter    6\tLoss:2117.1342773\tRec Loss:2108.5539551\tKL Div:8.5803127\n",
      "Epoch 313\tIter   12\tLoss:2100.4995117\tRec Loss:2091.9475098\tKL Div:8.5519543\n",
      "Epoch 313\tIter   18\tLoss:2177.5253906\tRec Loss:2168.9162598\tKL Div:8.6092310\n",
      "Epoch 314\tIter    6\tLoss:2106.1625977\tRec Loss:2097.6303711\tKL Div:8.5321636\n",
      "Epoch 314\tIter   12\tLoss:2158.3857422\tRec Loss:2149.6445312\tKL Div:8.7412758\n",
      "Epoch 314\tIter   18\tLoss:2130.6137695\tRec Loss:2122.0524902\tKL Div:8.5612946\n",
      "Epoch 315\tIter    6\tLoss:2131.6049805\tRec Loss:2122.9941406\tKL Div:8.6108055\n",
      "Epoch 315\tIter   12\tLoss:2112.6037598\tRec Loss:2103.8461914\tKL Div:8.7576008\n",
      "Epoch 315\tIter   18\tLoss:2139.5603027\tRec Loss:2130.9692383\tKL Div:8.5911531\n",
      "Epoch 316\tIter    6\tLoss:2143.0437012\tRec Loss:2134.3457031\tKL Div:8.6978788\n",
      "Epoch 316\tIter   12\tLoss:2092.4243164\tRec Loss:2083.8261719\tKL Div:8.5982399\n",
      "Epoch 316\tIter   18\tLoss:2166.9353027\tRec Loss:2158.3090820\tKL Div:8.6262693\n",
      "Epoch 317\tIter    6\tLoss:2135.9772949\tRec Loss:2127.2861328\tKL Div:8.6910810\n",
      "Epoch 317\tIter   12\tLoss:2147.9887695\tRec Loss:2139.2631836\tKL Div:8.7255211\n",
      "Epoch 317\tIter   18\tLoss:2153.0847168\tRec Loss:2144.2949219\tKL Div:8.7897339\n",
      "Epoch 318\tIter    6\tLoss:2126.8652344\tRec Loss:2118.0947266\tKL Div:8.7703953\n",
      "Epoch 318\tIter   12\tLoss:2089.4025879\tRec Loss:2080.7824707\tKL Div:8.6200600\n",
      "Epoch 318\tIter   18\tLoss:2167.4899902\tRec Loss:2158.9062500\tKL Div:8.5838013\n",
      "Epoch 319\tIter    6\tLoss:2106.0981445\tRec Loss:2097.3862305\tKL Div:8.7119713\n",
      "Epoch 319\tIter   12\tLoss:2120.8525391\tRec Loss:2112.1948242\tKL Div:8.6577244\n",
      "Epoch 319\tIter   18\tLoss:2150.1752930\tRec Loss:2141.4323730\tKL Div:8.7428389\n",
      "Epoch 320\tIter    6\tLoss:2127.3127441\tRec Loss:2118.6005859\tKL Div:8.7121096\n",
      "Epoch 320\tIter   12\tLoss:2170.4833984\tRec Loss:2161.7687988\tKL Div:8.7145042\n",
      "Epoch 320\tIter   18\tLoss:2125.9104004\tRec Loss:2117.3762207\tKL Div:8.5342636\n",
      "Epoch 320\tLoss:2128.0053529\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,   916,    95,    99,  1114,    97, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['noise', 'drone', 'drum', 'tone', 'build', 'bass', 'close', 'melody', 'sense', 'piece', 'light', 'space', 'slow', 'open', 'beat']\n",
      "['indie', 'love', 'melody', 'lyric', 'debut', 'group', 'harmony', 'chorus', 'arrangement', 'bit', 'piano', 'write', 'pretty', 'line', 'string']\n",
      "['live', 'version', 'disc', 'set', 'include', 'record', 'recording', 'studio', 'label', 'original', 'soul', 'cover', 'compilation', 'early', 'group']\n",
      "['lyric', 'man', 'listen', 'bad', 'day', 'guy', 'kid', 'get', 'love', 'line', '\\n', 'let', 'people', 'world', 'write']\n",
      "['house', 'dance', 'mix', 'label', 'beat', 'techno', 'producer', 'bass', 'dj', 'synth', 'artist', 'club', 'rhythm', 'drum', 'set']\n",
      "['love', 'line', 'turn', 'title', 'close', 'year', 'world', 'debut', 'point', 'kind', 'sense', 'leave', 'hard', 'big', 'early']\n",
      "['\\xa0', 'project', 'point', 'solo', 'title', 'artist', 'ep', 'early', 'year', 'synth', 'place', 'producer', 'close', 'sense', 'lyric']\n",
      "['melody', 'debut', 'riff', 'chorus', 'drum', 'band', 'hook', 'line', 'bass', 'punk', 'group', 'rhythm', 'ep', 'hard', 'opener']\n",
      "['sing', 'love', 'lyric', 'life', 'world', 'line', 'word', 'feeling', 'write', 'close', 'leave', 'light', 'sense', 'relationship', 'dream']\n",
      "['punk', 'live', 'love', 'band', 'cover', 'single', 'solo', 'big', 'early', 'fan', 'classic', 'day', 'year', 'pollard', 'group']\n",
      "['drum', 'jazz', 'melody', 'electronic', 'beat', 'disc', 'bass', 'noise', 'interesting', 'piece', 'instrumental', 'bit', 'listen', 'begin', 'feature']\n",
      "['life', 'man', 'world', 'people', 'write', 'love', 'woman', 'black', 'sing', 'story', 'word', 'call', 'year', 'live', 'young']\n",
      "['folk', 'country', 'blue', 'solo', 'cover', 'sing', 'lyric', 'acoustic', 'record', 'line', 'arrangement', 'love', 'man', 'artist', 'oldham']\n",
      "['dance', 'beat', 'disco', 'people', 'party', 'single', 'bad', 'big', 'fun', 'synth', 'love', 'pretty', 'guy', 'funk', 'get']\n",
      "['piece', 'piano', 'electronic', 'world', 'composer', 'composition', 'musician', 'instrument', 'film', 'ambient', 'create', 'sense', 'drone', 'tone', 'jazz']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'production', 'verse', 'year', 'producer', 'flow', 'style', 'line', 'sample', 'feature', 'artist']\n",
      "['synth', 'ep', 'beat', 'electronic', 'melody', 'debut', 'sample', 'sense', 'production', 'kind', 'instrumental', 'close', 'project', 'style', 'build']\n",
      "['lyric', 'title', 'kind', 'love', 'band', 'indie_rock', 'chorus', 'life', 'emo', 'people', 'punk', 'indie', 'point', 'hook', 'sort']\n",
      "['metal', 'punk', 'riff', 'hardcore', 'band', 'death', 'black_metal', 'noise', 'group', 'scream', 'early', 'drummer', 'drum', 'black', 'guitarist']\n",
      "['love', 'r&b', 'single', 'hit', 'singer', 'artist', 'star', 'year', 'girl', 'debut', 'producer', 'soul', 'big', 'prince', 'dance']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,   916,    95,    99,  1114,    97, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  346,   916,    95,    99,  1114,    97, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.48\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "noise\n",
      "drone\n",
      "drum\n",
      "tone\n",
      "build\n",
      "bass\n",
      "close\n",
      "melody\n",
      "sense\n",
      "piece\n",
      "light\n",
      "space\n",
      "slow\n",
      "open\n",
      "beat\n",
      "coherence c_uci -0.040109058683468916\n",
      "coherence c_npmi 0.01087445167678725\n",
      "coherence c_cv 0.3793673780392439\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,   916,    95,    99,  1114,    97, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 321\tIter    6\tLoss:2118.7543945\tRec Loss:2109.9931641\tKL Div:8.7611313\n",
      "Epoch 321\tIter   12\tLoss:2120.4516602\tRec Loss:2111.7543945\tKL Div:8.6972618\n",
      "Epoch 321\tIter   18\tLoss:2155.4462891\tRec Loss:2146.6882324\tKL Div:8.7580757\n",
      "Epoch 322\tIter    6\tLoss:2125.8918457\tRec Loss:2117.2460938\tKL Div:8.6456881\n",
      "Epoch 322\tIter   12\tLoss:2109.5295410\tRec Loss:2100.7526855\tKL Div:8.7769232\n",
      "Epoch 322\tIter   18\tLoss:2131.5148926\tRec Loss:2122.9907227\tKL Div:8.5242119\n",
      "Epoch 323\tIter    6\tLoss:2099.6984863\tRec Loss:2090.9880371\tKL Div:8.7105045\n",
      "Epoch 323\tIter   12\tLoss:2150.3718262\tRec Loss:2141.6489258\tKL Div:8.7229280\n",
      "Epoch 323\tIter   18\tLoss:2124.3811035\tRec Loss:2115.7009277\tKL Div:8.6801071\n",
      "Epoch 324\tIter    6\tLoss:2090.7778320\tRec Loss:2082.1499023\tKL Div:8.6279392\n",
      "Epoch 324\tIter   12\tLoss:2106.2460938\tRec Loss:2097.6469727\tKL Div:8.5990849\n",
      "Epoch 324\tIter   18\tLoss:2147.6569824\tRec Loss:2138.9860840\tKL Div:8.6708508\n",
      "Epoch 325\tIter    6\tLoss:2107.2470703\tRec Loss:2098.7124023\tKL Div:8.5345573\n",
      "Epoch 325\tIter   12\tLoss:2142.1828613\tRec Loss:2133.5437012\tKL Div:8.6391745\n",
      "Epoch 325\tIter   18\tLoss:2163.3347168\tRec Loss:2154.5075684\tKL Div:8.8270969\n",
      "Epoch 326\tIter    6\tLoss:2129.5654297\tRec Loss:2120.9033203\tKL Div:8.6621656\n",
      "Epoch 326\tIter   12\tLoss:2134.1286621\tRec Loss:2125.2617188\tKL Div:8.8668652\n",
      "Epoch 326\tIter   18\tLoss:2127.3195801\tRec Loss:2118.7341309\tKL Div:8.5854244\n",
      "Epoch 327\tIter    6\tLoss:2147.1572266\tRec Loss:2138.4052734\tKL Div:8.7519131\n",
      "Epoch 327\tIter   12\tLoss:2121.1755371\tRec Loss:2112.4536133\tKL Div:8.7220325\n",
      "Epoch 327\tIter   18\tLoss:2127.4155273\tRec Loss:2118.8393555\tKL Div:8.5760689\n",
      "Epoch 328\tIter    6\tLoss:2135.4953613\tRec Loss:2126.8056641\tKL Div:8.6897945\n",
      "Epoch 328\tIter   12\tLoss:2134.4699707\tRec Loss:2125.5605469\tKL Div:8.9094582\n",
      "Epoch 328\tIter   18\tLoss:2169.0881348\tRec Loss:2160.3569336\tKL Div:8.7312279\n",
      "Epoch 329\tIter    6\tLoss:2141.2319336\tRec Loss:2132.3803711\tKL Div:8.8516750\n",
      "Epoch 329\tIter   12\tLoss:2132.0205078\tRec Loss:2123.2827148\tKL Div:8.7377424\n",
      "Epoch 329\tIter   18\tLoss:2154.0754395\tRec Loss:2145.4431152\tKL Div:8.6322556\n",
      "Epoch 330\tIter    6\tLoss:2150.8911133\tRec Loss:2142.1215820\tKL Div:8.7695637\n",
      "Epoch 330\tIter   12\tLoss:2172.1721191\tRec Loss:2163.4960938\tKL Div:8.6759129\n",
      "Epoch 330\tIter   18\tLoss:2156.5236816\tRec Loss:2147.6616211\tKL Div:8.8621721\n",
      "Epoch 331\tIter    6\tLoss:2111.8981934\tRec Loss:2103.1020508\tKL Div:8.7961788\n",
      "Epoch 331\tIter   12\tLoss:2110.4641113\tRec Loss:2101.9411621\tKL Div:8.5229235\n",
      "Epoch 331\tIter   18\tLoss:2132.7058105\tRec Loss:2123.9558105\tKL Div:8.7498865\n",
      "Epoch 332\tIter    6\tLoss:2139.7172852\tRec Loss:2130.7521973\tKL Div:8.9650450\n",
      "Epoch 332\tIter   12\tLoss:2155.6796875\tRec Loss:2147.0678711\tKL Div:8.6118069\n",
      "Epoch 332\tIter   18\tLoss:2140.9133301\tRec Loss:2132.0397949\tKL Div:8.8735151\n",
      "Epoch 333\tIter    6\tLoss:2110.4841309\tRec Loss:2101.8935547\tKL Div:8.5905352\n",
      "Epoch 333\tIter   12\tLoss:2122.0034180\tRec Loss:2113.0876465\tKL Div:8.9158039\n",
      "Epoch 333\tIter   18\tLoss:2081.1091309\tRec Loss:2072.5285645\tKL Div:8.5804558\n",
      "Epoch 334\tIter    6\tLoss:2123.2465820\tRec Loss:2114.4521484\tKL Div:8.7944546\n",
      "Epoch 334\tIter   12\tLoss:2135.3210449\tRec Loss:2126.5212402\tKL Div:8.7998104\n",
      "Epoch 334\tIter   18\tLoss:2083.5690918\tRec Loss:2075.0551758\tKL Div:8.5139427\n",
      "Epoch 335\tIter    6\tLoss:2161.8510742\tRec Loss:2153.0505371\tKL Div:8.8005333\n",
      "Epoch 335\tIter   12\tLoss:2136.5751953\tRec Loss:2127.8623047\tKL Div:8.7129002\n",
      "Epoch 335\tIter   18\tLoss:2121.9262695\tRec Loss:2113.3056641\tKL Div:8.6206379\n",
      "Epoch 336\tIter    6\tLoss:2160.6042480\tRec Loss:2151.6328125\tKL Div:8.9713726\n",
      "Epoch 336\tIter   12\tLoss:2154.6779785\tRec Loss:2145.9965820\tKL Div:8.6813049\n",
      "Epoch 336\tIter   18\tLoss:2175.2968750\tRec Loss:2166.3635254\tKL Div:8.9332275\n",
      "Epoch 337\tIter    6\tLoss:2122.9548340\tRec Loss:2114.4848633\tKL Div:8.4699402\n",
      "Epoch 337\tIter   12\tLoss:2121.2902832\tRec Loss:2112.5319824\tKL Div:8.7584038\n",
      "Epoch 337\tIter   18\tLoss:2150.0310059\tRec Loss:2141.6247559\tKL Div:8.4062538\n",
      "Epoch 338\tIter    6\tLoss:2106.4311523\tRec Loss:2097.6801758\tKL Div:8.7510958\n",
      "Epoch 338\tIter   12\tLoss:2117.1381836\tRec Loss:2108.6955566\tKL Div:8.4425812\n",
      "Epoch 338\tIter   18\tLoss:2157.6892090\tRec Loss:2148.7473145\tKL Div:8.9417915\n",
      "Epoch 339\tIter    6\tLoss:2172.6816406\tRec Loss:2163.7905273\tKL Div:8.8910666\n",
      "Epoch 339\tIter   12\tLoss:2139.9252930\tRec Loss:2131.2631836\tKL Div:8.6622000\n",
      "Epoch 339\tIter   18\tLoss:2116.7727051\tRec Loss:2108.1308594\tKL Div:8.6418552\n",
      "Epoch 340\tIter    6\tLoss:2129.3107910\tRec Loss:2120.5336914\tKL Div:8.7770357\n",
      "Epoch 340\tIter   12\tLoss:2126.0202637\tRec Loss:2117.4213867\tKL Div:8.5989189\n",
      "Epoch 340\tIter   18\tLoss:2110.5537109\tRec Loss:2101.8293457\tKL Div:8.7242565\n",
      "Epoch 341\tIter    6\tLoss:2132.4504395\tRec Loss:2123.6057129\tKL Div:8.8446245\n",
      "Epoch 341\tIter   12\tLoss:2095.8425293\tRec Loss:2087.2448730\tKL Div:8.5975628\n",
      "Epoch 341\tIter   18\tLoss:2105.0546875\tRec Loss:2096.1342773\tKL Div:8.9203701\n",
      "Epoch 342\tIter    6\tLoss:2108.2392578\tRec Loss:2099.6252441\tKL Div:8.6140480\n",
      "Epoch 342\tIter   12\tLoss:2158.0595703\tRec Loss:2149.4436035\tKL Div:8.6159363\n",
      "Epoch 342\tIter   18\tLoss:2121.4721680\tRec Loss:2112.6350098\tKL Div:8.8370571\n",
      "Epoch 343\tIter    6\tLoss:2148.7060547\tRec Loss:2140.0195312\tKL Div:8.6864986\n",
      "Epoch 343\tIter   12\tLoss:2111.9433594\tRec Loss:2103.1611328\tKL Div:8.7822618\n",
      "Epoch 343\tIter   18\tLoss:2166.2373047\tRec Loss:2157.4848633\tKL Div:8.7523289\n",
      "Epoch 344\tIter    6\tLoss:2116.7248535\tRec Loss:2108.0166016\tKL Div:8.7083607\n",
      "Epoch 344\tIter   12\tLoss:2143.1162109\tRec Loss:2134.3862305\tKL Div:8.7298927\n",
      "Epoch 344\tIter   18\tLoss:2115.4179688\tRec Loss:2106.5739746\tKL Div:8.8439236\n",
      "Epoch 345\tIter    6\tLoss:2103.1486816\tRec Loss:2094.5144043\tKL Div:8.6343985\n",
      "Epoch 345\tIter   12\tLoss:2137.6191406\tRec Loss:2128.8876953\tKL Div:8.7315655\n",
      "Epoch 345\tIter   18\tLoss:2134.7795410\tRec Loss:2126.1301270\tKL Div:8.6493034\n",
      "Epoch 346\tIter    6\tLoss:2113.8530273\tRec Loss:2104.9663086\tKL Div:8.8867645\n",
      "Epoch 346\tIter   12\tLoss:2094.2868652\tRec Loss:2085.8137207\tKL Div:8.4730730\n",
      "Epoch 346\tIter   18\tLoss:2122.3964844\tRec Loss:2113.5541992\tKL Div:8.8423977\n",
      "Epoch 347\tIter    6\tLoss:2101.6242676\tRec Loss:2092.9916992\tKL Div:8.6325998\n",
      "Epoch 347\tIter   12\tLoss:2148.6142578\tRec Loss:2140.0502930\tKL Div:8.5640259\n",
      "Epoch 347\tIter   18\tLoss:2090.6953125\tRec Loss:2082.0488281\tKL Div:8.6464968\n",
      "Epoch 348\tIter    6\tLoss:2139.8142090\tRec Loss:2131.0527344\tKL Div:8.7614613\n",
      "Epoch 348\tIter   12\tLoss:2138.4140625\tRec Loss:2129.7495117\tKL Div:8.6645670\n",
      "Epoch 348\tIter   18\tLoss:2141.9655762\tRec Loss:2133.0827637\tKL Div:8.8827229\n",
      "Epoch 349\tIter    6\tLoss:2157.9882812\tRec Loss:2149.2407227\tKL Div:8.7476749\n",
      "Epoch 349\tIter   12\tLoss:2101.2963867\tRec Loss:2092.6372070\tKL Div:8.6590614\n",
      "Epoch 349\tIter   18\tLoss:2127.7226562\tRec Loss:2118.9887695\tKL Div:8.7339230\n",
      "Epoch 350\tIter    6\tLoss:2118.9216309\tRec Loss:2110.0898438\tKL Div:8.8317699\n",
      "Epoch 350\tIter   12\tLoss:2126.6982422\tRec Loss:2118.0234375\tKL Div:8.6748838\n",
      "Epoch 350\tIter   18\tLoss:2143.4169922\tRec Loss:2134.6589355\tKL Div:8.7581425\n",
      "Epoch 351\tIter    6\tLoss:2096.8688965\tRec Loss:2088.0949707\tKL Div:8.7738085\n",
      "Epoch 351\tIter   12\tLoss:2116.5502930\tRec Loss:2107.8559570\tKL Div:8.6944199\n",
      "Epoch 351\tIter   18\tLoss:2102.4877930\tRec Loss:2093.9133301\tKL Div:8.5743999\n",
      "Epoch 352\tIter    6\tLoss:2110.8645020\tRec Loss:2102.1010742\tKL Div:8.7634373\n",
      "Epoch 352\tIter   12\tLoss:2129.2487793\tRec Loss:2120.6579590\tKL Div:8.5907173\n",
      "Epoch 352\tIter   18\tLoss:2130.8122559\tRec Loss:2122.0239258\tKL Div:8.7884302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353\tIter    6\tLoss:2119.3234863\tRec Loss:2110.6579590\tKL Div:8.6656322\n",
      "Epoch 353\tIter   12\tLoss:2133.5581055\tRec Loss:2124.9187012\tKL Div:8.6394625\n",
      "Epoch 353\tIter   18\tLoss:2153.2490234\tRec Loss:2144.3837891\tKL Div:8.8651886\n",
      "Epoch 354\tIter    6\tLoss:2122.0976562\tRec Loss:2113.2954102\tKL Div:8.8023586\n",
      "Epoch 354\tIter   12\tLoss:2102.5751953\tRec Loss:2093.9548340\tKL Div:8.6203861\n",
      "Epoch 354\tIter   18\tLoss:2121.2460938\tRec Loss:2112.4636230\tKL Div:8.7825613\n",
      "Epoch 355\tIter    6\tLoss:2142.3242188\tRec Loss:2133.4838867\tKL Div:8.8403358\n",
      "Epoch 355\tIter   12\tLoss:2139.9233398\tRec Loss:2131.1032715\tKL Div:8.8201847\n",
      "Epoch 355\tIter   18\tLoss:2113.2580566\tRec Loss:2104.4943848\tKL Div:8.7637482\n",
      "Epoch 356\tIter    6\tLoss:2158.6928711\tRec Loss:2149.7758789\tKL Div:8.9169922\n",
      "Epoch 356\tIter   12\tLoss:2139.7641602\tRec Loss:2131.2126465\tKL Div:8.5514469\n",
      "Epoch 356\tIter   18\tLoss:2163.0336914\tRec Loss:2154.2153320\tKL Div:8.8183937\n",
      "Epoch 357\tIter    6\tLoss:2169.6525879\tRec Loss:2160.7028809\tKL Div:8.9497023\n",
      "Epoch 357\tIter   12\tLoss:2164.7062988\tRec Loss:2155.9096680\tKL Div:8.7966862\n",
      "Epoch 357\tIter   18\tLoss:2110.3532715\tRec Loss:2101.5585938\tKL Div:8.7946033\n",
      "Epoch 358\tIter    6\tLoss:2111.6577148\tRec Loss:2102.8417969\tKL Div:8.8158493\n",
      "Epoch 358\tIter   12\tLoss:2124.9863281\tRec Loss:2116.3129883\tKL Div:8.6733990\n",
      "Epoch 358\tIter   18\tLoss:2117.6342773\tRec Loss:2108.8093262\tKL Div:8.8249702\n",
      "Epoch 359\tIter    6\tLoss:2093.5664062\tRec Loss:2084.7988281\tKL Div:8.7675524\n",
      "Epoch 359\tIter   12\tLoss:2135.0612793\tRec Loss:2126.5161133\tKL Div:8.5450573\n",
      "Epoch 359\tIter   18\tLoss:2137.5346680\tRec Loss:2128.6508789\tKL Div:8.8837585\n",
      "Epoch 360\tIter    6\tLoss:2094.7409668\tRec Loss:2086.1850586\tKL Div:8.5558281\n",
      "Epoch 360\tIter   12\tLoss:2126.5422363\tRec Loss:2117.6108398\tKL Div:8.9315157\n",
      "Epoch 360\tIter   18\tLoss:2142.1882324\tRec Loss:2133.4348145\tKL Div:8.7534113\n",
      "Epoch 360\tLoss:2126.9034321\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,    97,   454,    99,  1114,   433, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['noise', 'drone', 'drum', 'tone', 'melody', 'build', 'close', 'piece', 'bass', 'sense', 'space', 'light', 'slow', 'open', 'dark']\n",
      "['love', 'indie', 'melody', 'lyric', 'arrangement', 'group', 'debut', 'harmony', 'chorus', 'piano', 'bit', 'pretty', 'line', 'write', 'string']\n",
      "['version', 'live', 'set', 'disc', 'include', 'record', 'studio', 'cover', 'original', 'label', 'recording', 'soul', 'group', 'early', 'compilation']\n",
      "['lyric', 'man', 'listen', 'bad', 'guy', 'get', 'day', 'kid', '\\n', 'line', 'people', 'let', 'love', 'shit', 'pretty']\n",
      "['house', 'dance', 'mix', 'techno', 'label', 'beat', 'producer', 'bass', 'dj', 'artist', 'synth', 'drum', 'club', 'rhythm', 'set']\n",
      "['synth', 'debut', 'love', 'ep', 'dance', 'single', 'title', 'production', 'big', 'hook', 'close', 'turn', 'line', 'point', 'lyric']\n",
      "['\\xa0', 'project', 'point', 'solo', 'artist', 'title', 'year', 'early', 'place', 'producer', 'ep', 'close', 'feature', 'lyric', 'idea']\n",
      "['melody', 'debut', 'riff', 'drum', 'band', 'chorus', 'hook', 'line', 'bass', 'group', 'rhythm', 'punk', 'ep', 'opener', 'hard']\n",
      "['love', 'sing', 'life', 'lyric', 'world', 'word', 'line', 'feeling', 'write', 'close', 'leave', 'sense', 'light', 'place', 'death']\n",
      "['punk', 'live', 'love', 'single', 'band', 'solo', 'cover', 'big', 'early', 'fan', 'day', 'classic', 'year', 'group', 'hit']\n",
      "['drum', 'melody', 'jazz', 'electronic', 'noise', 'beat', 'bass', 'disc', 'piece', 'instrumental', 'interesting', 'bit', 'live', 'listen', 'feature']\n",
      "['life', 'man', 'world', 'people', 'write', 'love', 'woman', 'black', 'sing', 'story', 'word', 'live', 'call', 'year', 'white']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'lyric', 'acoustic', 'record', 'love', 'line', 'man', 'style', 'artist', 'oldham']\n",
      "['dance', 'beat', 'disco', 'people', 'single', 'party', 'big', 'fun', 'love', 'bad', 'synth', 'funk', 'mix', 'pretty', 'stuff']\n",
      "['piece', 'piano', 'electronic', 'world', 'film', 'composition', 'composer', 'instrument', 'musician', 'create', 'recording', 'jazz', 'ambient', 'sense', 'note']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'verse', 'production', 'year', 'producer', 'flow', 'style', 'feature', 'line', 'sample', 'big']\n",
      "['synth', 'beat', 'electronic', 'ep', 'melody', 'sample', 'debut', 'sense', 'instrumental', 'kind', 'production', 'artist', 'close', 'style', 'line']\n",
      "['lyric', 'title', 'kind', 'love', 'band', 'life', 'indie_rock', 'people', 'emo', 'sort', 'punk', 'chorus', 'point', 'indie', 'line']\n",
      "['metal', 'punk', 'riff', 'hardcore', 'band', 'noise', 'black_metal', 'death', 'group', 'drummer', 'drum', 'heavy', 'early', 'black', 'scream']\n",
      "['love', 'r&b', 'hit', 'artist', 'singer', 'single', 'star', 'year', 'girl', 'producer', 'soul', 'debut', 'big', 'prince', 'woman']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,    97,   454,    99,  1114,   433, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,    97,   454,    99,  1114,   433, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.48333333333333334\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "noise\n",
      "drone\n",
      "drum\n",
      "tone\n",
      "melody\n",
      "build\n",
      "close\n",
      "piece\n",
      "bass\n",
      "sense\n",
      "space\n",
      "light\n",
      "slow\n",
      "open\n",
      "dark\n",
      "coherence c_uci 0.019211101811173425\n",
      "coherence c_npmi 0.013038367770886775\n",
      "coherence c_cv 0.3776652047786377\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,    97,   454,    99,  1114,   433, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 361\tIter    6\tLoss:2121.1408691\tRec Loss:2112.4458008\tKL Div:8.6950665\n",
      "Epoch 361\tIter   12\tLoss:2118.8879395\tRec Loss:2110.1357422\tKL Div:8.7523174\n",
      "Epoch 361\tIter   18\tLoss:2122.6701660\tRec Loss:2113.8637695\tKL Div:8.8063354\n",
      "Epoch 362\tIter    6\tLoss:2106.3583984\tRec Loss:2097.6032715\tKL Div:8.7550449\n",
      "Epoch 362\tIter   12\tLoss:2156.2241211\tRec Loss:2147.5175781\tKL Div:8.7064714\n",
      "Epoch 362\tIter   18\tLoss:2107.3989258\tRec Loss:2098.7539062\tKL Div:8.6451311\n",
      "Epoch 363\tIter    6\tLoss:2112.5495605\tRec Loss:2103.7353516\tKL Div:8.8141842\n",
      "Epoch 363\tIter   12\tLoss:2115.0324707\tRec Loss:2106.3676758\tKL Div:8.6648998\n",
      "Epoch 363\tIter   18\tLoss:2113.4792480\tRec Loss:2104.7885742\tKL Div:8.6906662\n",
      "Epoch 364\tIter    6\tLoss:2151.0170898\tRec Loss:2142.1494141\tKL Div:8.8676844\n",
      "Epoch 364\tIter   12\tLoss:2115.6672363\tRec Loss:2107.0449219\tKL Div:8.6223755\n",
      "Epoch 364\tIter   18\tLoss:2150.1611328\tRec Loss:2141.2514648\tKL Div:8.9097414\n",
      "Epoch 365\tIter    6\tLoss:2136.9184570\tRec Loss:2128.1298828\tKL Div:8.7884665\n",
      "Epoch 365\tIter   12\tLoss:2146.0231934\tRec Loss:2137.2673340\tKL Div:8.7558079\n",
      "Epoch 365\tIter   18\tLoss:2104.0620117\tRec Loss:2095.4111328\tKL Div:8.6508961\n",
      "Epoch 366\tIter    6\tLoss:2106.3195801\tRec Loss:2097.5332031\tKL Div:8.7864771\n",
      "Epoch 366\tIter   12\tLoss:2161.0329590\tRec Loss:2152.1401367\tKL Div:8.8928061\n",
      "Epoch 366\tIter   18\tLoss:2147.6145020\tRec Loss:2138.9909668\tKL Div:8.6234264\n",
      "Epoch 367\tIter    6\tLoss:2160.9348145\tRec Loss:2152.0136719\tKL Div:8.9211273\n",
      "Epoch 367\tIter   12\tLoss:2144.8996582\tRec Loss:2136.1638184\tKL Div:8.7357769\n",
      "Epoch 367\tIter   18\tLoss:2106.3837891\tRec Loss:2097.6337891\tKL Div:8.7499523\n",
      "Epoch 368\tIter    6\tLoss:2142.6806641\tRec Loss:2134.0200195\tKL Div:8.6605997\n",
      "Epoch 368\tIter   12\tLoss:2099.9106445\tRec Loss:2091.1245117\tKL Div:8.7860718\n",
      "Epoch 368\tIter   18\tLoss:2116.6425781\tRec Loss:2108.0073242\tKL Div:8.6352539\n",
      "Epoch 369\tIter    6\tLoss:2108.3854980\tRec Loss:2099.6042480\tKL Div:8.7812557\n",
      "Epoch 369\tIter   12\tLoss:2122.4062500\tRec Loss:2113.7255859\tKL Div:8.6806355\n",
      "Epoch 369\tIter   18\tLoss:2091.5964355\tRec Loss:2082.8476562\tKL Div:8.7487717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370\tIter    6\tLoss:2163.1975098\tRec Loss:2154.4304199\tKL Div:8.7671051\n",
      "Epoch 370\tIter   12\tLoss:2124.9633789\tRec Loss:2116.3481445\tKL Div:8.6153526\n",
      "Epoch 370\tIter   18\tLoss:2110.4790039\tRec Loss:2101.6918945\tKL Div:8.7872047\n",
      "Epoch 371\tIter    6\tLoss:2139.2050781\tRec Loss:2130.3496094\tKL Div:8.8554268\n",
      "Epoch 371\tIter   12\tLoss:2161.5297852\tRec Loss:2152.7612305\tKL Div:8.7685623\n",
      "Epoch 371\tIter   18\tLoss:2130.3557129\tRec Loss:2121.6345215\tKL Div:8.7212381\n",
      "Epoch 372\tIter    6\tLoss:2137.0378418\tRec Loss:2128.1113281\tKL Div:8.9265366\n",
      "Epoch 372\tIter   12\tLoss:2099.2431641\tRec Loss:2090.5478516\tKL Div:8.6953526\n",
      "Epoch 372\tIter   18\tLoss:2149.6179199\tRec Loss:2140.8249512\tKL Div:8.7929010\n",
      "Epoch 373\tIter    6\tLoss:2089.0153809\tRec Loss:2080.2802734\tKL Div:8.7351847\n",
      "Epoch 373\tIter   12\tLoss:2152.6562500\tRec Loss:2143.8251953\tKL Div:8.8311415\n",
      "Epoch 373\tIter   18\tLoss:2134.3676758\tRec Loss:2125.3447266\tKL Div:9.0228777\n",
      "Epoch 374\tIter    6\tLoss:2145.2392578\tRec Loss:2136.4262695\tKL Div:8.8130150\n",
      "Epoch 374\tIter   12\tLoss:2142.6499023\tRec Loss:2133.7624512\tKL Div:8.8875647\n",
      "Epoch 374\tIter   18\tLoss:2138.6987305\tRec Loss:2130.1179199\tKL Div:8.5808344\n",
      "Epoch 375\tIter    6\tLoss:2137.7509766\tRec Loss:2128.8002930\tKL Div:8.9507027\n",
      "Epoch 375\tIter   12\tLoss:2164.3312988\tRec Loss:2155.5773926\tKL Div:8.7539253\n",
      "Epoch 375\tIter   18\tLoss:2098.4663086\tRec Loss:2089.6870117\tKL Div:8.7793198\n",
      "Epoch 376\tIter    6\tLoss:2121.5065918\tRec Loss:2112.8120117\tKL Div:8.6946869\n",
      "Epoch 376\tIter   12\tLoss:2133.6330566\tRec Loss:2124.7644043\tKL Div:8.8687630\n",
      "Epoch 376\tIter   18\tLoss:2124.1921387\tRec Loss:2115.3242188\tKL Div:8.8679161\n",
      "Epoch 377\tIter    6\tLoss:2135.1860352\tRec Loss:2126.4243164\tKL Div:8.7617207\n",
      "Epoch 377\tIter   12\tLoss:2140.1130371\tRec Loss:2131.3579102\tKL Div:8.7552109\n",
      "Epoch 377\tIter   18\tLoss:2115.6252441\tRec Loss:2106.7973633\tKL Div:8.8278198\n",
      "Epoch 378\tIter    6\tLoss:2071.3601074\tRec Loss:2062.5849609\tKL Div:8.7750826\n",
      "Epoch 378\tIter   12\tLoss:2167.0119629\tRec Loss:2158.3237305\tKL Div:8.6882610\n",
      "Epoch 378\tIter   18\tLoss:2174.4216309\tRec Loss:2165.4204102\tKL Div:9.0011063\n",
      "Epoch 379\tIter    6\tLoss:2163.0656738\tRec Loss:2154.2451172\tKL Div:8.8206577\n",
      "Epoch 379\tIter   12\tLoss:2147.2973633\tRec Loss:2138.4670410\tKL Div:8.8302355\n",
      "Epoch 379\tIter   18\tLoss:2152.7229004\tRec Loss:2143.9355469\tKL Div:8.7873878\n",
      "Epoch 380\tIter    6\tLoss:2120.5791016\tRec Loss:2111.6350098\tKL Div:8.9440193\n",
      "Epoch 380\tIter   12\tLoss:2126.7448730\tRec Loss:2118.0310059\tKL Div:8.7139454\n",
      "Epoch 380\tIter   18\tLoss:2126.1757812\tRec Loss:2117.3342285\tKL Div:8.8414593\n",
      "Epoch 381\tIter    6\tLoss:2103.6916504\tRec Loss:2094.9331055\tKL Div:8.7584639\n",
      "Epoch 381\tIter   12\tLoss:2163.1818848\tRec Loss:2154.2519531\tKL Div:8.9300194\n",
      "Epoch 381\tIter   18\tLoss:2118.8850098\tRec Loss:2110.1301270\tKL Div:8.7549133\n",
      "Epoch 382\tIter    6\tLoss:2122.9694824\tRec Loss:2114.2353516\tKL Div:8.7341690\n",
      "Epoch 382\tIter   12\tLoss:2117.4895020\tRec Loss:2108.6359863\tKL Div:8.8534842\n",
      "Epoch 382\tIter   18\tLoss:2140.1647949\tRec Loss:2131.4038086\tKL Div:8.7610493\n",
      "Epoch 383\tIter    6\tLoss:2126.3369141\tRec Loss:2117.4616699\tKL Div:8.8751411\n",
      "Epoch 383\tIter   12\tLoss:2126.1557617\tRec Loss:2117.3735352\tKL Div:8.7821827\n",
      "Epoch 383\tIter   18\tLoss:2127.3522949\tRec Loss:2118.5717773\tKL Div:8.7805653\n",
      "Epoch 384\tIter    6\tLoss:2143.2265625\tRec Loss:2134.4729004\tKL Div:8.7537289\n",
      "Epoch 384\tIter   12\tLoss:2153.4492188\tRec Loss:2144.5651855\tKL Div:8.8840580\n",
      "Epoch 384\tIter   18\tLoss:2122.9023438\tRec Loss:2114.2490234\tKL Div:8.6533175\n",
      "Epoch 385\tIter    6\tLoss:2119.3608398\tRec Loss:2110.6535645\tKL Div:8.7073441\n",
      "Epoch 385\tIter   12\tLoss:2188.2656250\tRec Loss:2179.3193359\tKL Div:8.9462872\n",
      "Epoch 385\tIter   18\tLoss:2116.2500000\tRec Loss:2107.4511719\tKL Div:8.7988539\n",
      "Epoch 386\tIter    6\tLoss:2176.6308594\tRec Loss:2167.6542969\tKL Div:8.9765921\n",
      "Epoch 386\tIter   12\tLoss:2152.6179199\tRec Loss:2143.8857422\tKL Div:8.7322006\n",
      "Epoch 386\tIter   18\tLoss:2084.4555664\tRec Loss:2075.7351074\tKL Div:8.7205782\n",
      "Epoch 387\tIter    6\tLoss:2115.8928223\tRec Loss:2107.0241699\tKL Div:8.8686581\n",
      "Epoch 387\tIter   12\tLoss:2128.0500488\tRec Loss:2119.2673340\tKL Div:8.7828321\n",
      "Epoch 387\tIter   18\tLoss:2150.6484375\tRec Loss:2141.7145996\tKL Div:8.9337339\n",
      "Epoch 388\tIter    6\tLoss:2131.4877930\tRec Loss:2122.7421875\tKL Div:8.7455292\n",
      "Epoch 388\tIter   12\tLoss:2152.1169434\tRec Loss:2143.3249512\tKL Div:8.7918758\n",
      "Epoch 388\tIter   18\tLoss:2113.8430176\tRec Loss:2105.0227051\tKL Div:8.8204079\n",
      "Epoch 389\tIter    6\tLoss:2139.0327148\tRec Loss:2130.0944824\tKL Div:8.9382935\n",
      "Epoch 389\tIter   12\tLoss:2096.0366211\tRec Loss:2087.3171387\tKL Div:8.7194300\n",
      "Epoch 389\tIter   18\tLoss:2160.7121582\tRec Loss:2151.8601074\tKL Div:8.8519878\n",
      "Epoch 390\tIter    6\tLoss:2098.7561035\tRec Loss:2090.0058594\tKL Div:8.7503328\n",
      "Epoch 390\tIter   12\tLoss:2124.2126465\tRec Loss:2115.2866211\tKL Div:8.9260750\n",
      "Epoch 390\tIter   18\tLoss:2114.6882324\tRec Loss:2106.0917969\tKL Div:8.5965033\n",
      "Epoch 391\tIter    6\tLoss:2118.7438965\tRec Loss:2109.8798828\tKL Div:8.8639879\n",
      "Epoch 391\tIter   12\tLoss:2119.3095703\tRec Loss:2110.6540527\tKL Div:8.6556263\n",
      "Epoch 391\tIter   18\tLoss:2140.1367188\tRec Loss:2131.2944336\tKL Div:8.8422194\n",
      "Epoch 392\tIter    6\tLoss:2142.9553223\tRec Loss:2133.9331055\tKL Div:9.0221376\n",
      "Epoch 392\tIter   12\tLoss:2147.2583008\tRec Loss:2138.5786133\tKL Div:8.6796741\n",
      "Epoch 392\tIter   18\tLoss:2182.4140625\tRec Loss:2173.4165039\tKL Div:8.9976692\n",
      "Epoch 393\tIter    6\tLoss:2103.5275879\tRec Loss:2094.7680664\tKL Div:8.7595911\n",
      "Epoch 393\tIter   12\tLoss:2109.3034668\tRec Loss:2100.5239258\tKL Div:8.7795095\n",
      "Epoch 393\tIter   18\tLoss:2116.0336914\tRec Loss:2107.1997070\tKL Div:8.8340893\n",
      "Epoch 394\tIter    6\tLoss:2103.3164062\tRec Loss:2094.6328125\tKL Div:8.6835270\n",
      "Epoch 394\tIter   12\tLoss:2136.8632812\tRec Loss:2127.9157715\tKL Div:8.9475670\n",
      "Epoch 394\tIter   18\tLoss:2131.1130371\tRec Loss:2122.4960938\tKL Div:8.6169682\n",
      "Epoch 395\tIter    6\tLoss:2096.8291016\tRec Loss:2087.9375000\tKL Div:8.8916626\n",
      "Epoch 395\tIter   12\tLoss:2157.5541992\tRec Loss:2148.8203125\tKL Div:8.7338228\n",
      "Epoch 395\tIter   18\tLoss:2123.0629883\tRec Loss:2114.2060547\tKL Div:8.8569307\n",
      "Epoch 396\tIter    6\tLoss:2083.6230469\tRec Loss:2074.8093262\tKL Div:8.8136463\n",
      "Epoch 396\tIter   12\tLoss:2168.3876953\tRec Loss:2159.5900879\tKL Div:8.7976446\n",
      "Epoch 396\tIter   18\tLoss:2155.0073242\tRec Loss:2146.1904297\tKL Div:8.8169155\n",
      "Epoch 397\tIter    6\tLoss:2123.1621094\tRec Loss:2114.2392578\tKL Div:8.9228754\n",
      "Epoch 397\tIter   12\tLoss:2156.1667480\tRec Loss:2147.3371582\tKL Div:8.8296661\n",
      "Epoch 397\tIter   18\tLoss:2145.3994141\tRec Loss:2136.5585938\tKL Div:8.8407993\n",
      "Epoch 398\tIter    6\tLoss:2143.3203125\tRec Loss:2134.5576172\tKL Div:8.7626238\n",
      "Epoch 398\tIter   12\tLoss:2156.2941895\tRec Loss:2147.4184570\tKL Div:8.8756475\n",
      "Epoch 398\tIter   18\tLoss:2164.8454590\tRec Loss:2156.0449219\tKL Div:8.8005610\n",
      "Epoch 399\tIter    6\tLoss:2166.9362793\tRec Loss:2157.8979492\tKL Div:9.0384274\n",
      "Epoch 399\tIter   12\tLoss:2132.5827637\tRec Loss:2123.9028320\tKL Div:8.6799736\n",
      "Epoch 399\tIter   18\tLoss:2143.6308594\tRec Loss:2134.7624512\tKL Div:8.8683434\n",
      "Epoch 400\tIter    6\tLoss:2165.2644043\tRec Loss:2156.4208984\tKL Div:8.8434305\n",
      "Epoch 400\tIter   12\tLoss:2129.4707031\tRec Loss:2120.4853516\tKL Div:8.9854727\n",
      "Epoch 400\tIter   18\tLoss:2165.0859375\tRec Loss:2156.4729004\tKL Div:8.6130867\n",
      "Epoch 400\tLoss:2125.5245747\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,    97,    95,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['noise', 'drone', 'tone', 'drum', 'build', 'piece', 'close', 'melody', 'space', 'bass', 'sense', 'light', 'open', 'slow', 'dark']\n",
      "['love', 'melody', 'indie', 'lyric', 'debut', 'harmony', 'arrangement', 'piano', 'chorus', 'group', 'bit', 'write', 'line', 'pretty', 'string']\n",
      "['live', 'version', 'disc', 'set', 'include', 'record', 'cover', 'soul', 'recording', 'studio', 'label', 'original', 'group', 'compilation', 'early']\n",
      "['man', 'lyric', 'listen', 'bad', 'guy', '\\n', 'get', 'day', 'kid', 'love', 'let', 'people', 'shit', 'line', 'pretty']\n",
      "['house', 'techno', 'mix', 'dance', 'label', 'beat', 'producer', 'bass', 'dj', 'artist', 'synth', 'drum', 'set', 'rhythm', 'club']\n",
      "['dance', 'synth', 'disco', 'love', 'single', 'duo', 'indie', 'big', 'debut', 'group', 'house', 'act', 'hook', 'turn', 'electro']\n",
      "['\\xa0', 'project', 'point', 'artist', 'year', 'solo', 'title', 'early', 'ep', 'place', 'close', 'producer', 'feature', 'sense', 'start']\n",
      "['melody', 'riff', 'debut', 'chorus', 'drum', 'band', 'hook', 'line', 'bass', 'punk', 'group', 'rhythm', 'ep', 'opener', 'hard']\n",
      "['love', 'sing', 'life', 'lyric', 'world', 'word', 'line', 'write', 'leave', 'feeling', 'close', 'light', 'death', 'sense', 'kind']\n",
      "['punk', 'live', 'love', 'single', 'band', 'cover', 'solo', 'early', 'big', 'fan', 'day', 'classic', 'group', 'smith', 'year']\n",
      "['drum', 'melody', 'jazz', 'beat', 'bass', 'electronic', 'noise', 'piece', 'interesting', 'disc', 'begin', 'feature', 'instrumental', 'group', 'bit']\n",
      "['life', 'man', 'world', 'people', 'write', 'black', 'love', 'woman', 'story', 'sing', 'word', 'political', 'live', 'call', 'young']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'acoustic', 'lyric', 'record', 'artist', 'oldham', 'man', 'love', 'style', 'arrangement']\n",
      "['dance', 'beat', 'people', 'party', 'single', 'big', 'fun', 'mix', 'disco', 'bad', 'hip_hop', 'pretty', 'stuff', 'funk', 'punk']\n",
      "['piece', 'piano', 'electronic', 'world', 'composer', 'composition', 'film', 'musician', 'instrument', 'jazz', 'recording', 'create', 'note', 'tone', 'sense']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'production', 'mixtape', 'verse', 'year', 'producer', 'flow', 'style', 'line', 'feature', 'sample', 'big']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'sense', 'instrumental', 'debut', 'artist', 'production', 'kind', 'drum', 'build', 'project']\n",
      "['lyric', 'kind', 'title', 'love', 'life', 'band', 'people', 'punk', 'indie_rock', 'point', 'emo', 'chorus', 'sort', 'indie', 'big']\n",
      "['metal', 'riff', 'punk', 'hardcore', 'band', 'black_metal', 'noise', 'death', 'drum', 'heavy', 'group', 'doom', 'drummer', 'black', 'scream']\n",
      "['love', 'r&b', 'artist', 'hit', 'singer', 'star', 'single', 'year', 'producer', 'girl', 'soul', 'prince', 'debut', 'big', 'woman']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,    97,    95,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,    97,    95,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5033333333333333\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "noise\n",
      "drone\n",
      "tone\n",
      "drum\n",
      "build\n",
      "piece\n",
      "close\n",
      "melody\n",
      "space\n",
      "bass\n",
      "sense\n",
      "light\n",
      "open\n",
      "slow\n",
      "dark\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence c_uci -0.020017901615820656\n",
      "coherence c_npmi 0.013261862525466833\n",
      "coherence c_cv 0.38666069035823714\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  346,    97,    95,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   235,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 401\tIter    6\tLoss:2083.9653320\tRec Loss:2074.9631348\tKL Div:9.0022068\n",
      "Epoch 401\tIter   12\tLoss:2082.5781250\tRec Loss:2073.8862305\tKL Div:8.6919842\n",
      "Epoch 401\tIter   18\tLoss:2115.6054688\tRec Loss:2106.7226562\tKL Div:8.8828449\n",
      "Epoch 402\tIter    6\tLoss:2129.7980957\tRec Loss:2121.0405273\tKL Div:8.7576389\n",
      "Epoch 402\tIter   12\tLoss:2103.3405762\tRec Loss:2094.5002441\tKL Div:8.8402958\n",
      "Epoch 402\tIter   18\tLoss:2148.1638184\tRec Loss:2139.2680664\tKL Div:8.8956966\n",
      "Epoch 403\tIter    6\tLoss:2145.2822266\tRec Loss:2136.4765625\tKL Div:8.8055763\n",
      "Epoch 403\tIter   12\tLoss:2155.4916992\tRec Loss:2146.6459961\tKL Div:8.8456221\n",
      "Epoch 403\tIter   18\tLoss:2128.8847656\tRec Loss:2120.0988770\tKL Div:8.7859926\n",
      "Epoch 404\tIter    6\tLoss:2120.9309082\tRec Loss:2111.9975586\tKL Div:8.9334583\n",
      "Epoch 404\tIter   12\tLoss:2126.7241211\tRec Loss:2117.9829102\tKL Div:8.7410889\n",
      "Epoch 404\tIter   18\tLoss:2115.8447266\tRec Loss:2106.8828125\tKL Div:8.9619217\n",
      "Epoch 405\tIter    6\tLoss:2136.8833008\tRec Loss:2128.0441895\tKL Div:8.8390312\n",
      "Epoch 405\tIter   12\tLoss:2102.2631836\tRec Loss:2093.4033203\tKL Div:8.8598137\n",
      "Epoch 405\tIter   18\tLoss:2147.4323730\tRec Loss:2138.4448242\tKL Div:8.9876556\n",
      "Epoch 406\tIter    6\tLoss:2136.4423828\tRec Loss:2127.5490723\tKL Div:8.8932590\n",
      "Epoch 406\tIter   12\tLoss:2144.5988770\tRec Loss:2135.7565918\tKL Div:8.8422050\n",
      "Epoch 406\tIter   18\tLoss:2145.0649414\tRec Loss:2136.2392578\tKL Div:8.8256321\n",
      "Epoch 407\tIter    6\tLoss:2119.5903320\tRec Loss:2110.5339355\tKL Div:9.0565157\n",
      "Epoch 407\tIter   12\tLoss:2123.1164551\tRec Loss:2114.3432617\tKL Div:8.7732964\n",
      "Epoch 407\tIter   18\tLoss:2114.2697754\tRec Loss:2105.5488281\tKL Div:8.7209949\n",
      "Epoch 408\tIter    6\tLoss:2082.1770020\tRec Loss:2073.2431641\tKL Div:8.9338665\n",
      "Epoch 408\tIter   12\tLoss:2148.1171875\tRec Loss:2139.3298340\tKL Div:8.7872314\n",
      "Epoch 408\tIter   18\tLoss:2129.5839844\tRec Loss:2120.7490234\tKL Div:8.8348618\n",
      "Epoch 409\tIter    6\tLoss:2142.5581055\tRec Loss:2133.6879883\tKL Div:8.8702087\n",
      "Epoch 409\tIter   12\tLoss:2157.7768555\tRec Loss:2148.8369141\tKL Div:8.9400091\n",
      "Epoch 409\tIter   18\tLoss:2166.8376465\tRec Loss:2157.9057617\tKL Div:8.9318447\n",
      "Epoch 410\tIter    6\tLoss:2103.2746582\tRec Loss:2094.4135742\tKL Div:8.8610153\n",
      "Epoch 410\tIter   12\tLoss:2129.2663574\tRec Loss:2120.3349609\tKL Div:8.9313345\n",
      "Epoch 410\tIter   18\tLoss:2135.3063965\tRec Loss:2126.5434570\tKL Div:8.7630157\n",
      "Epoch 411\tIter    6\tLoss:2124.1662598\tRec Loss:2115.2102051\tKL Div:8.9559441\n",
      "Epoch 411\tIter   12\tLoss:2131.7558594\tRec Loss:2122.8747559\tKL Div:8.8810568\n",
      "Epoch 411\tIter   18\tLoss:2109.8090820\tRec Loss:2101.0759277\tKL Div:8.7331944\n",
      "Epoch 412\tIter    6\tLoss:2141.7148438\tRec Loss:2132.7460938\tKL Div:8.9688377\n",
      "Epoch 412\tIter   12\tLoss:2144.0961914\tRec Loss:2135.1948242\tKL Div:8.9012690\n",
      "Epoch 412\tIter   18\tLoss:2137.9946289\tRec Loss:2129.0266113\tKL Div:8.9681015\n",
      "Epoch 413\tIter    6\tLoss:2148.5222168\tRec Loss:2139.5883789\tKL Div:8.9337969\n",
      "Epoch 413\tIter   12\tLoss:2146.9172363\tRec Loss:2137.9667969\tKL Div:8.9503689\n",
      "Epoch 413\tIter   18\tLoss:2159.6091309\tRec Loss:2150.7348633\tKL Div:8.8742409\n",
      "Epoch 414\tIter    6\tLoss:2138.0356445\tRec Loss:2129.0090332\tKL Div:9.0264978\n",
      "Epoch 414\tIter   12\tLoss:2116.9929199\tRec Loss:2108.4338379\tKL Div:8.5591736\n",
      "Epoch 414\tIter   18\tLoss:2156.6452637\tRec Loss:2147.5832520\tKL Div:9.0621080\n",
      "Epoch 415\tIter    6\tLoss:2143.4462891\tRec Loss:2134.6015625\tKL Div:8.8447113\n",
      "Epoch 415\tIter   12\tLoss:2135.8215332\tRec Loss:2127.0595703\tKL Div:8.7619286\n",
      "Epoch 415\tIter   18\tLoss:2142.9416504\tRec Loss:2134.0996094\tKL Div:8.8421593\n",
      "Epoch 416\tIter    6\tLoss:2108.8627930\tRec Loss:2099.8317871\tKL Div:9.0309410\n",
      "Epoch 416\tIter   12\tLoss:2099.9335938\tRec Loss:2091.2900391\tKL Div:8.6436424\n",
      "Epoch 416\tIter   18\tLoss:2167.0341797\tRec Loss:2157.9956055\tKL Div:9.0384979\n",
      "Epoch 417\tIter    6\tLoss:2148.5830078\tRec Loss:2139.4716797\tKL Div:9.1112862\n",
      "Epoch 417\tIter   12\tLoss:2127.8173828\tRec Loss:2119.0732422\tKL Div:8.7441158\n",
      "Epoch 417\tIter   18\tLoss:2132.0053711\tRec Loss:2123.2143555\tKL Div:8.7910318\n",
      "Epoch 418\tIter    6\tLoss:2110.2634277\tRec Loss:2101.3649902\tKL Div:8.8985167\n",
      "Epoch 418\tIter   12\tLoss:2158.1403809\tRec Loss:2149.2800293\tKL Div:8.8602552\n",
      "Epoch 418\tIter   18\tLoss:2081.5954590\tRec Loss:2072.8959961\tKL Div:8.6994839\n",
      "Epoch 419\tIter    6\tLoss:2200.7390137\tRec Loss:2191.8129883\tKL Div:8.9259434\n",
      "Epoch 419\tIter   12\tLoss:2123.2595215\tRec Loss:2114.4838867\tKL Div:8.7755833\n",
      "Epoch 419\tIter   18\tLoss:2089.7741699\tRec Loss:2080.8691406\tKL Div:8.9050560\n",
      "Epoch 420\tIter    6\tLoss:2111.3491211\tRec Loss:2102.5385742\tKL Div:8.8105078\n",
      "Epoch 420\tIter   12\tLoss:2127.5939941\tRec Loss:2118.6684570\tKL Div:8.9256506\n",
      "Epoch 420\tIter   18\tLoss:2158.2316895\tRec Loss:2149.4135742\tKL Div:8.8180027\n",
      "Epoch 421\tIter    6\tLoss:2161.1650391\tRec Loss:2152.0380859\tKL Div:9.1270037\n",
      "Epoch 421\tIter   12\tLoss:2148.0073242\tRec Loss:2139.2685547\tKL Div:8.7386932\n",
      "Epoch 421\tIter   18\tLoss:2104.3591309\tRec Loss:2095.5007324\tKL Div:8.8584118\n",
      "Epoch 422\tIter    6\tLoss:2100.4543457\tRec Loss:2091.5559082\tKL Div:8.8984241\n",
      "Epoch 422\tIter   12\tLoss:2094.9643555\tRec Loss:2086.2924805\tKL Div:8.6719112\n",
      "Epoch 422\tIter   18\tLoss:2131.8942871\tRec Loss:2122.8593750\tKL Div:9.0350008\n",
      "Epoch 423\tIter    6\tLoss:2112.5061035\tRec Loss:2103.7561035\tKL Div:8.7500610\n",
      "Epoch 423\tIter   12\tLoss:2136.2285156\tRec Loss:2127.1699219\tKL Div:9.0586901\n",
      "Epoch 423\tIter   18\tLoss:2122.4653320\tRec Loss:2113.6904297\tKL Div:8.7749119\n",
      "Epoch 424\tIter    6\tLoss:2114.9682617\tRec Loss:2105.9616699\tKL Div:9.0066891\n",
      "Epoch 424\tIter   12\tLoss:2098.2329102\tRec Loss:2089.5273438\tKL Div:8.7055626\n",
      "Epoch 424\tIter   18\tLoss:2152.4243164\tRec Loss:2143.3559570\tKL Div:9.0682373\n",
      "Epoch 425\tIter    6\tLoss:2141.6574707\tRec Loss:2132.6833496\tKL Div:8.9741430\n",
      "Epoch 425\tIter   12\tLoss:2087.3398438\tRec Loss:2078.7641602\tKL Div:8.5757151\n",
      "Epoch 425\tIter   18\tLoss:2135.0488281\tRec Loss:2126.1303711\tKL Div:8.9184875\n",
      "Epoch 426\tIter    6\tLoss:2116.5986328\tRec Loss:2107.9067383\tKL Div:8.6919994\n",
      "Epoch 426\tIter   12\tLoss:2116.6845703\tRec Loss:2107.7558594\tKL Div:8.9287472\n",
      "Epoch 426\tIter   18\tLoss:2117.9902344\tRec Loss:2109.2062988\tKL Div:8.7838602\n",
      "Epoch 427\tIter    6\tLoss:2145.9750977\tRec Loss:2136.9370117\tKL Div:9.0380917\n",
      "Epoch 427\tIter   12\tLoss:2081.4455566\tRec Loss:2072.7104492\tKL Div:8.7351036\n",
      "Epoch 427\tIter   18\tLoss:2128.0053711\tRec Loss:2119.2446289\tKL Div:8.7606621\n",
      "Epoch 428\tIter    6\tLoss:2139.7421875\tRec Loss:2130.6362305\tKL Div:9.1058884\n",
      "Epoch 428\tIter   12\tLoss:2113.3515625\tRec Loss:2104.7548828\tKL Div:8.5966358\n",
      "Epoch 428\tIter   18\tLoss:2146.8093262\tRec Loss:2137.8598633\tKL Div:8.9493446\n",
      "Epoch 429\tIter    6\tLoss:2175.5095215\tRec Loss:2166.6015625\tKL Div:8.9078941\n",
      "Epoch 429\tIter   12\tLoss:2115.5744629\tRec Loss:2106.7812500\tKL Div:8.7931957\n",
      "Epoch 429\tIter   18\tLoss:2141.5209961\tRec Loss:2132.5776367\tKL Div:8.9433117\n",
      "Epoch 430\tIter    6\tLoss:2090.9318848\tRec Loss:2082.0131836\tKL Div:8.9186020\n",
      "Epoch 430\tIter   12\tLoss:2090.1635742\tRec Loss:2081.3784180\tKL Div:8.7851391\n",
      "Epoch 430\tIter   18\tLoss:2172.6691895\tRec Loss:2163.7883301\tKL Div:8.8807983\n",
      "Epoch 431\tIter    6\tLoss:2139.2795410\tRec Loss:2130.2802734\tKL Div:8.9993362\n",
      "Epoch 431\tIter   12\tLoss:2149.1130371\tRec Loss:2140.3173828\tKL Div:8.7956591\n",
      "Epoch 431\tIter   18\tLoss:2130.8295898\tRec Loss:2121.9995117\tKL Div:8.8299618\n",
      "Epoch 432\tIter    6\tLoss:2133.6757812\tRec Loss:2124.8352051\tKL Div:8.8405704\n",
      "Epoch 432\tIter   12\tLoss:2123.8596191\tRec Loss:2115.0468750\tKL Div:8.8126478\n",
      "Epoch 432\tIter   18\tLoss:2144.3288574\tRec Loss:2135.3959961\tKL Div:8.9329357\n",
      "Epoch 433\tIter    6\tLoss:2138.7106934\tRec Loss:2129.9667969\tKL Div:8.7439451\n",
      "Epoch 433\tIter   12\tLoss:2147.7270508\tRec Loss:2138.6728516\tKL Div:9.0542011\n",
      "Epoch 433\tIter   18\tLoss:2126.1745605\tRec Loss:2117.3916016\tKL Div:8.7830601\n",
      "Epoch 434\tIter    6\tLoss:2110.6464844\tRec Loss:2101.7727051\tKL Div:8.8737602\n",
      "Epoch 434\tIter   12\tLoss:2164.9934082\tRec Loss:2156.1125488\tKL Div:8.8809376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 434\tIter   18\tLoss:2139.2595215\tRec Loss:2130.4482422\tKL Div:8.8112316\n",
      "Epoch 435\tIter    6\tLoss:2166.9077148\tRec Loss:2157.8950195\tKL Div:9.0126476\n",
      "Epoch 435\tIter   12\tLoss:2113.2075195\tRec Loss:2104.6525879\tKL Div:8.5550060\n",
      "Epoch 435\tIter   18\tLoss:2100.9123535\tRec Loss:2091.8864746\tKL Div:9.0259609\n",
      "Epoch 436\tIter    6\tLoss:2096.2451172\tRec Loss:2087.5051270\tKL Div:8.7398739\n",
      "Epoch 436\tIter   12\tLoss:2120.0729980\tRec Loss:2111.1604004\tKL Div:8.9127083\n",
      "Epoch 436\tIter   18\tLoss:2103.7880859\tRec Loss:2094.9350586\tKL Div:8.8530893\n",
      "Epoch 437\tIter    6\tLoss:2129.5422363\tRec Loss:2120.6816406\tKL Div:8.8606968\n",
      "Epoch 437\tIter   12\tLoss:2127.4931641\tRec Loss:2118.6284180\tKL Div:8.8647966\n",
      "Epoch 437\tIter   18\tLoss:2129.8754883\tRec Loss:2121.1154785\tKL Div:8.7600756\n",
      "Epoch 438\tIter    6\tLoss:2093.4890137\tRec Loss:2084.6535645\tKL Div:8.8355608\n",
      "Epoch 438\tIter   12\tLoss:2142.3444824\tRec Loss:2133.4062500\tKL Div:8.9382277\n",
      "Epoch 438\tIter   18\tLoss:2101.5664062\tRec Loss:2092.7814941\tKL Div:8.7847939\n",
      "Epoch 439\tIter    6\tLoss:2135.3276367\tRec Loss:2126.3867188\tKL Div:8.9409447\n",
      "Epoch 439\tIter   12\tLoss:2112.0415039\tRec Loss:2103.1333008\tKL Div:8.9082174\n",
      "Epoch 439\tIter   18\tLoss:2094.2998047\tRec Loss:2085.6196289\tKL Div:8.6801176\n",
      "Epoch 440\tIter    6\tLoss:2080.5085449\tRec Loss:2071.6301270\tKL Div:8.8784008\n",
      "Epoch 440\tIter   12\tLoss:2107.6027832\tRec Loss:2098.8061523\tKL Div:8.7965479\n",
      "Epoch 440\tIter   18\tLoss:2155.1242676\tRec Loss:2146.0458984\tKL Div:9.0783672\n",
      "Epoch 440\tLoss:2128.3340749\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,    99,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'tone', 'drum', 'build', 'piece', 'close', 'space', 'sense', 'melody', 'bass', 'light', 'slow', 'open', 'dark']\n",
      "['love', 'melody', 'indie', 'lyric', 'debut', 'arrangement', 'harmony', 'piano', 'group', 'chorus', 'write', 'line', 'string', 'bit', 'pretty']\n",
      "['live', 'version', 'disc', 'set', 'include', 'record', 'cover', 'soul', 'label', 'studio', 'recording', 'group', 'original', 'compilation', 'early']\n",
      "['lyric', 'man', 'bad', 'listen', 'guy', 'get', 'day', 'kid', '\\n', 'love', 'people', 'let', 'line', 'shit', 'girl']\n",
      "['house', 'techno', 'mix', 'dance', 'label', 'beat', 'producer', 'bass', 'dj', 'artist', 'synth', 'drum', 'rhythm', 'club', 'set']\n",
      "['dance', 'synth', 'disco', 'single', 'love', 'house', 'group', 'duo', 'big', 'indie', 'act', 'turn', 'debut', 'remix', 'electro']\n",
      "['\\xa0', 'project', 'point', 'artist', 'solo', 'title', 'year', 'ep', 'early', 'place', 'producer', 'close', 'feature', 'lyric', 'sense']\n",
      "['melody', 'riff', 'debut', 'chorus', 'drum', 'band', 'hook', 'line', 'group', 'bass', 'punk', 'rhythm', 'opener', 'ep', 'hard']\n",
      "['love', 'sing', 'life', 'lyric', 'world', 'line', 'word', 'write', 'feeling', 'close', 'leave', 'death', 'light', 'sense', 'kind']\n",
      "['punk', 'live', 'love', 'single', 'band', 'solo', 'cover', 'early', 'fan', 'big', 'classic', 'day', 'group', 'year', 'set']\n",
      "['drum', 'electronic', 'melody', 'jazz', 'noise', 'beat', 'bass', 'disc', 'piece', 'instrumental', 'begin', 'feature', 'interesting', 'bit', 'live']\n",
      "['life', 'man', 'people', 'world', 'love', 'write', 'black', 'woman', 'sing', 'word', 'story', 'call', 'live', 'young', 'year']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'lyric', 'acoustic', 'record', 'artist', 'love', 'man', 'oldham', 'style', 'american']\n",
      "['beat', 'dance', 'party', 'people', 'hip_hop', 'funk', 'mix', 'single', 'big', 'punk', 'fun', 'bad', 'sample', 'pretty', 'get']\n",
      "['piece', 'composer', 'piano', 'electronic', 'composition', 'world', 'film', 'musician', 'instrument', 'jazz', 'recording', 'create', 'note', 'string', 'score']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'verse', 'production', 'year', 'producer', 'flow', 'style', 'line', 'feature', 'sample', 'big']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'sense', 'debut', 'instrumental', 'artist', 'project', 'kind', 'production', 'drum', 'close']\n",
      "['lyric', 'title', 'kind', 'band', 'love', 'life', 'people', 'punk', 'indie_rock', 'emo', 'point', 'chorus', 'sort', 'big', 'indie']\n",
      "['metal', 'riff', 'punk', 'hardcore', 'band', 'black_metal', 'noise', 'death', 'drum', 'drummer', 'group', 'heavy', 'doom', 'early', 'scream']\n",
      "['love', 'r&b', 'artist', 'hit', 'singer', 'single', 'star', 'year', 'producer', 'girl', 'debut', 'soul', 'prince', 'big', 'woman']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,    99,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,    99,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "tone\n",
      "drum\n",
      "build\n",
      "piece\n",
      "close\n",
      "space\n",
      "sense\n",
      "melody\n",
      "bass\n",
      "light\n",
      "slow\n",
      "open\n",
      "dark\n",
      "coherence c_uci 0.014022597158987021\n",
      "coherence c_npmi 0.014648238732215754\n",
      "coherence c_cv 0.38577982627081214\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,    99,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 441\tIter    6\tLoss:2089.8310547\tRec Loss:2080.9780273\tKL Div:8.8531036\n",
      "Epoch 441\tIter   12\tLoss:2119.8671875\tRec Loss:2111.0563965\tKL Div:8.8108063\n",
      "Epoch 441\tIter   18\tLoss:2161.6428223\tRec Loss:2152.5629883\tKL Div:9.0799026\n",
      "Epoch 442\tIter    6\tLoss:2168.9318848\tRec Loss:2160.1293945\tKL Div:8.8023891\n",
      "Epoch 442\tIter   12\tLoss:2094.9116211\tRec Loss:2086.0170898\tKL Div:8.8945398\n",
      "Epoch 442\tIter   18\tLoss:2136.2199707\tRec Loss:2127.3110352\tKL Div:8.9088955\n",
      "Epoch 443\tIter    6\tLoss:2142.7290039\tRec Loss:2133.8007812\tKL Div:8.9282780\n",
      "Epoch 443\tIter   12\tLoss:2150.6821289\tRec Loss:2141.7871094\tKL Div:8.8949556\n",
      "Epoch 443\tIter   18\tLoss:2106.8586426\tRec Loss:2098.0322266\tKL Div:8.8264465\n",
      "Epoch 444\tIter    6\tLoss:2128.2514648\tRec Loss:2119.3544922\tKL Div:8.8968534\n",
      "Epoch 444\tIter   12\tLoss:2128.6398926\tRec Loss:2119.7597656\tKL Div:8.8802471\n",
      "Epoch 444\tIter   18\tLoss:2121.0122070\tRec Loss:2112.1594238\tKL Div:8.8528624\n",
      "Epoch 445\tIter    6\tLoss:2155.3332520\tRec Loss:2146.3647461\tKL Div:8.9684124\n",
      "Epoch 445\tIter   12\tLoss:2128.5439453\tRec Loss:2119.7290039\tKL Div:8.8149261\n",
      "Epoch 445\tIter   18\tLoss:2124.5336914\tRec Loss:2115.7956543\tKL Div:8.7380953\n",
      "Epoch 446\tIter    6\tLoss:2118.8408203\tRec Loss:2109.8115234\tKL Div:9.0294151\n",
      "Epoch 446\tIter   12\tLoss:2135.9995117\tRec Loss:2127.3857422\tKL Div:8.6136971\n",
      "Epoch 446\tIter   18\tLoss:2128.3852539\tRec Loss:2119.4409180\tKL Div:8.9442158\n",
      "Epoch 447\tIter    6\tLoss:2144.5263672\tRec Loss:2135.5446777\tKL Div:8.9818096\n",
      "Epoch 447\tIter   12\tLoss:2151.2822266\tRec Loss:2142.4091797\tKL Div:8.8731060\n",
      "Epoch 447\tIter   18\tLoss:2171.2368164\tRec Loss:2162.1723633\tKL Div:9.0644016\n",
      "Epoch 448\tIter    6\tLoss:2153.4436035\tRec Loss:2144.6054688\tKL Div:8.8380852\n",
      "Epoch 448\tIter   12\tLoss:2138.2421875\tRec Loss:2129.5153809\tKL Div:8.7268658\n",
      "Epoch 448\tIter   18\tLoss:2126.5886230\tRec Loss:2117.6972656\tKL Div:8.8914576\n",
      "Epoch 449\tIter    6\tLoss:2111.8178711\tRec Loss:2103.0026855\tKL Div:8.8150902\n",
      "Epoch 449\tIter   12\tLoss:2106.3198242\tRec Loss:2097.4091797\tKL Div:8.9105644\n",
      "Epoch 449\tIter   18\tLoss:2179.7077637\tRec Loss:2170.7431641\tKL Div:8.9645557\n",
      "Epoch 450\tIter    6\tLoss:2188.2531738\tRec Loss:2179.2700195\tKL Div:8.9832726\n",
      "Epoch 450\tIter   12\tLoss:2156.4362793\tRec Loss:2147.6586914\tKL Div:8.7776146\n",
      "Epoch 450\tIter   18\tLoss:2138.5549316\tRec Loss:2129.5168457\tKL Div:9.0379782\n",
      "Epoch 451\tIter    6\tLoss:2100.9687500\tRec Loss:2092.2416992\tKL Div:8.7270784\n",
      "Epoch 451\tIter   12\tLoss:2166.0346680\tRec Loss:2156.9707031\tKL Div:9.0639076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451\tIter   18\tLoss:2164.0195312\tRec Loss:2155.2592773\tKL Div:8.7602606\n",
      "Epoch 452\tIter    6\tLoss:2148.2436523\tRec Loss:2139.1752930\tKL Div:9.0684195\n",
      "Epoch 452\tIter   12\tLoss:2068.4748535\tRec Loss:2059.8271484\tKL Div:8.6477041\n",
      "Epoch 452\tIter   18\tLoss:2140.2031250\tRec Loss:2131.2705078\tKL Div:8.9327068\n",
      "Epoch 453\tIter    6\tLoss:2128.4877930\tRec Loss:2119.5009766\tKL Div:8.9867630\n",
      "Epoch 453\tIter   12\tLoss:2124.4885254\tRec Loss:2115.7067871\tKL Div:8.7817135\n",
      "Epoch 453\tIter   18\tLoss:2140.6096191\tRec Loss:2131.6923828\tKL Div:8.9172392\n",
      "Epoch 454\tIter    6\tLoss:2083.3122559\tRec Loss:2074.4230957\tKL Div:8.8890724\n",
      "Epoch 454\tIter   12\tLoss:2105.5673828\tRec Loss:2096.6494141\tKL Div:8.9178553\n",
      "Epoch 454\tIter   18\tLoss:2147.7468262\tRec Loss:2138.7856445\tKL Div:8.9611340\n",
      "Epoch 455\tIter    6\tLoss:2143.3374023\tRec Loss:2134.2055664\tKL Div:9.1319551\n",
      "Epoch 455\tIter   12\tLoss:2137.1501465\tRec Loss:2128.4147949\tKL Div:8.7352591\n",
      "Epoch 455\tIter   18\tLoss:2162.7058105\tRec Loss:2153.6206055\tKL Div:9.0851765\n",
      "Epoch 456\tIter    6\tLoss:2087.2770996\tRec Loss:2078.3330078\tKL Div:8.9441166\n",
      "Epoch 456\tIter   12\tLoss:2156.8054199\tRec Loss:2148.0830078\tKL Div:8.7225037\n",
      "Epoch 456\tIter   18\tLoss:2102.6584473\tRec Loss:2093.7197266\tKL Div:8.9386206\n",
      "Epoch 457\tIter    6\tLoss:2159.4843750\tRec Loss:2150.5122070\tKL Div:8.9721680\n",
      "Epoch 457\tIter   12\tLoss:2169.6088867\tRec Loss:2160.6379395\tKL Div:8.9708986\n",
      "Epoch 457\tIter   18\tLoss:2153.8640137\tRec Loss:2145.0322266\tKL Div:8.8318224\n",
      "Epoch 458\tIter    6\tLoss:2142.7832031\tRec Loss:2133.8000488\tKL Div:8.9832640\n",
      "Epoch 458\tIter   12\tLoss:2132.0424805\tRec Loss:2123.1254883\tKL Div:8.9169273\n",
      "Epoch 458\tIter   18\tLoss:2099.8806152\tRec Loss:2091.0192871\tKL Div:8.8613806\n",
      "Epoch 459\tIter    6\tLoss:2157.6677246\tRec Loss:2148.7280273\tKL Div:8.9396162\n",
      "Epoch 459\tIter   12\tLoss:2087.2385254\tRec Loss:2078.4409180\tKL Div:8.7977123\n",
      "Epoch 459\tIter   18\tLoss:2114.9216309\tRec Loss:2105.9704590\tKL Div:8.9511967\n",
      "Epoch 460\tIter    6\tLoss:2134.3066406\tRec Loss:2125.3857422\tKL Div:8.9208603\n",
      "Epoch 460\tIter   12\tLoss:2171.0742188\tRec Loss:2162.0166016\tKL Div:9.0575523\n",
      "Epoch 460\tIter   18\tLoss:2111.0373535\tRec Loss:2102.0776367\tKL Div:8.9596462\n",
      "Epoch 461\tIter    6\tLoss:2187.4111328\tRec Loss:2178.3225098\tKL Div:9.0886965\n",
      "Epoch 461\tIter   12\tLoss:2085.5690918\tRec Loss:2076.9011230\tKL Div:8.6679220\n",
      "Epoch 461\tIter   18\tLoss:2076.4704590\tRec Loss:2067.5642090\tKL Div:8.9062033\n",
      "Epoch 462\tIter    6\tLoss:2080.3950195\tRec Loss:2071.6208496\tKL Div:8.7741356\n",
      "Epoch 462\tIter   12\tLoss:2180.7724609\tRec Loss:2171.7624512\tKL Div:9.0099077\n",
      "Epoch 462\tIter   18\tLoss:2134.0751953\tRec Loss:2125.2924805\tKL Div:8.7827501\n",
      "Epoch 463\tIter    6\tLoss:2103.5256348\tRec Loss:2094.5212402\tKL Div:9.0044136\n",
      "Epoch 463\tIter   12\tLoss:2147.8872070\tRec Loss:2138.8237305\tKL Div:9.0635281\n",
      "Epoch 463\tIter   18\tLoss:2113.3041992\tRec Loss:2104.4914551\tKL Div:8.8126440\n",
      "Epoch 464\tIter    6\tLoss:2113.2077637\tRec Loss:2104.2036133\tKL Div:9.0042067\n",
      "Epoch 464\tIter   12\tLoss:2132.6791992\tRec Loss:2123.8166504\tKL Div:8.8624773\n",
      "Epoch 464\tIter   18\tLoss:2156.9150391\tRec Loss:2148.0039062\tKL Div:8.9111500\n",
      "Epoch 465\tIter    6\tLoss:2088.6645508\tRec Loss:2079.7465820\tKL Div:8.9178648\n",
      "Epoch 465\tIter   12\tLoss:2152.8356934\tRec Loss:2144.0141602\tKL Div:8.8214169\n",
      "Epoch 465\tIter   18\tLoss:2162.5957031\tRec Loss:2153.5942383\tKL Div:9.0013866\n",
      "Epoch 466\tIter    6\tLoss:2095.4689941\tRec Loss:2086.5917969\tKL Div:8.8771782\n",
      "Epoch 466\tIter   12\tLoss:2134.0004883\tRec Loss:2125.2937012\tKL Div:8.7066956\n",
      "Epoch 466\tIter   18\tLoss:2070.4755859\tRec Loss:2061.5810547\tKL Div:8.8945389\n",
      "Epoch 467\tIter    6\tLoss:2112.3264160\tRec Loss:2103.3886719\tKL Div:8.9376564\n",
      "Epoch 467\tIter   12\tLoss:2151.8159180\tRec Loss:2142.8032227\tKL Div:9.0127010\n",
      "Epoch 467\tIter   18\tLoss:2146.6342773\tRec Loss:2137.8005371\tKL Div:8.8336487\n",
      "Epoch 468\tIter    6\tLoss:2091.9663086\tRec Loss:2083.1157227\tKL Div:8.8505583\n",
      "Epoch 468\tIter   12\tLoss:2143.1555176\tRec Loss:2134.2475586\tKL Div:8.9080334\n",
      "Epoch 468\tIter   18\tLoss:2108.1718750\tRec Loss:2099.3417969\tKL Div:8.8300076\n",
      "Epoch 469\tIter    6\tLoss:2111.3559570\tRec Loss:2102.3425293\tKL Div:9.0134439\n",
      "Epoch 469\tIter   12\tLoss:2151.7919922\tRec Loss:2142.8999023\tKL Div:8.8920422\n",
      "Epoch 469\tIter   18\tLoss:2115.0761719\tRec Loss:2106.2241211\tKL Div:8.8520813\n",
      "Epoch 470\tIter    6\tLoss:2092.2343750\tRec Loss:2083.4995117\tKL Div:8.7347994\n",
      "Epoch 470\tIter   12\tLoss:2135.9282227\tRec Loss:2127.0439453\tKL Div:8.8843937\n",
      "Epoch 470\tIter   18\tLoss:2074.9812012\tRec Loss:2066.0871582\tKL Div:8.8941193\n",
      "Epoch 471\tIter    6\tLoss:2136.2941895\tRec Loss:2127.4228516\tKL Div:8.8713741\n",
      "Epoch 471\tIter   12\tLoss:2155.4372559\tRec Loss:2146.4394531\tKL Div:8.9978952\n",
      "Epoch 471\tIter   18\tLoss:2088.4189453\tRec Loss:2079.6494141\tKL Div:8.7695761\n",
      "Epoch 472\tIter    6\tLoss:2112.2355957\tRec Loss:2103.1894531\tKL Div:9.0460377\n",
      "Epoch 472\tIter   12\tLoss:2128.8430176\tRec Loss:2119.9584961\tKL Div:8.8844070\n",
      "Epoch 472\tIter   18\tLoss:2149.1035156\tRec Loss:2140.2827148\tKL Div:8.8208323\n",
      "Epoch 473\tIter    6\tLoss:2131.4943848\tRec Loss:2122.3461914\tKL Div:9.1482887\n",
      "Epoch 473\tIter   12\tLoss:2141.7309570\tRec Loss:2132.8881836\tKL Div:8.8426933\n",
      "Epoch 473\tIter   18\tLoss:2139.6035156\tRec Loss:2130.6665039\tKL Div:8.9371204\n",
      "Epoch 474\tIter    6\tLoss:2125.1530762\tRec Loss:2116.3156738\tKL Div:8.8374777\n",
      "Epoch 474\tIter   12\tLoss:2084.3273926\tRec Loss:2075.4882812\tKL Div:8.8391056\n",
      "Epoch 474\tIter   18\tLoss:2123.3955078\tRec Loss:2114.4897461\tKL Div:8.9057255\n",
      "Epoch 475\tIter    6\tLoss:2151.7729492\tRec Loss:2142.7812500\tKL Div:8.9916430\n",
      "Epoch 475\tIter   12\tLoss:2087.6594238\tRec Loss:2078.8159180\tKL Div:8.8434753\n",
      "Epoch 475\tIter   18\tLoss:2125.5336914\tRec Loss:2116.6040039\tKL Div:8.9296494\n",
      "Epoch 476\tIter    6\tLoss:2111.7265625\tRec Loss:2102.7429199\tKL Div:8.9837637\n",
      "Epoch 476\tIter   12\tLoss:2095.5270996\tRec Loss:2086.6298828\tKL Div:8.8973274\n",
      "Epoch 476\tIter   18\tLoss:2123.6301270\tRec Loss:2114.6860352\tKL Div:8.9439926\n",
      "Epoch 477\tIter    6\tLoss:2141.5280762\tRec Loss:2132.5688477\tKL Div:8.9592800\n",
      "Epoch 477\tIter   12\tLoss:2119.6149902\tRec Loss:2110.7128906\tKL Div:8.9021187\n",
      "Epoch 477\tIter   18\tLoss:2108.4270020\tRec Loss:2099.5644531\tKL Div:8.8626175\n",
      "Epoch 478\tIter    6\tLoss:2108.4555664\tRec Loss:2099.3686523\tKL Div:9.0868874\n",
      "Epoch 478\tIter   12\tLoss:2138.0595703\tRec Loss:2129.2631836\tKL Div:8.7964993\n",
      "Epoch 478\tIter   18\tLoss:2165.4089355\tRec Loss:2156.4899902\tKL Div:8.9189148\n",
      "Epoch 479\tIter    6\tLoss:2102.9306641\tRec Loss:2093.9082031\tKL Div:9.0223846\n",
      "Epoch 479\tIter   12\tLoss:2144.3708496\tRec Loss:2135.5061035\tKL Div:8.8647442\n",
      "Epoch 479\tIter   18\tLoss:2176.0891113\tRec Loss:2167.1704102\tKL Div:8.9186134\n",
      "Epoch 480\tIter    6\tLoss:2141.9997559\tRec Loss:2133.0473633\tKL Div:8.9525089\n",
      "Epoch 480\tIter   12\tLoss:2120.1445312\tRec Loss:2111.3330078\tKL Div:8.8114719\n",
      "Epoch 480\tIter   18\tLoss:2140.6735840\tRec Loss:2131.7121582\tKL Div:8.9615097\n",
      "Epoch 480\tLoss:2128.8320725\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'tone', 'piece', 'drum', 'build', 'sense', 'melody', 'close', 'space', 'bass', 'note', 'open', 'light', 'slow']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'debut', 'piano', 'group', 'harmony', 'chorus', 'string', 'write', 'line', 'bit', 'pretty']\n",
      "['live', 'version', 'disc', 'set', 'include', 'record', 'cover', 'soul', 'recording', 'label', 'studio', 'original', 'compilation', 'group', 'early']\n",
      "['man', 'lyric', 'bad', 'listen', 'guy', 'get', 'kid', 'day', '\\n', 'people', 'love', 'let', 'line', 'shit', 'pretty']\n",
      "['house', 'label', 'mix', 'techno', 'dance', 'beat', 'producer', 'bass', 'dj', 'artist', 'drum', 'synth', 'set', 'club', 'rhythm']\n",
      "['dance', 'synth', 'disco', 'single', 'love', 'house', 'group', 'duo', 'big', 'indie', 'remix', 'electro', 'act', 'club', 'turn']\n",
      "['\\xa0', 'project', 'point', 'artist', 'year', 'solo', 'title', 'early', 'ep', 'producer', 'synth', 'place', 'close', 'feature', 'sense']\n",
      "['melody', 'riff', 'debut', 'band', 'chorus', 'drum', 'line', 'hook', 'group', 'bass', 'punk', 'rhythm', 'ep', 'opener', 'hard']\n",
      "['love', 'sing', 'life', 'lyric', 'world', 'line', 'word', 'write', 'feeling', 'close', 'leave', 'light', 'death', 'sense', 'place']\n",
      "['punk', 'live', 'single', 'love', 'band', 'solo', 'cover', 'early', 'big', 'fan', 'day', 'group', 'classic', 'set', 'year']\n",
      "['drum', 'electronic', 'jazz', 'melody', 'noise', 'bass', 'beat', 'piece', 'interesting', 'disc', 'begin', 'instrumental', 'feature', 'group', 'bit']\n",
      "['life', 'man', 'world', 'people', 'black', 'woman', 'love', 'write', 'story', 'sing', 'word', 'live', 'political', 'call', 'year']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'lyric', 'acoustic', 'record', 'dylan', 'artist', 'love', 'american', 'man', 'oldham']\n",
      "['beat', 'dance', 'hip_hop', 'party', 'people', 'funk', 'sample', 'mix', 'bad', 'big', 'fun', 'single', 'punk', 'pretty', 'get']\n",
      "['piece', 'piano', 'composer', 'electronic', 'world', 'composition', 'jazz', 'film', 'musician', 'instrument', 'recording', 'create', 'note', 'string', 'tone']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'verse', 'production', 'year', 'producer', 'flow', 'style', 'line', 'feature', 'big', 'sample']\n",
      "['synth', 'beat', 'electronic', 'ep', 'melody', 'sample', 'sense', 'debut', 'artist', 'instrumental', 'project', 'drum', 'production', 'kind', 'close']\n",
      "['lyric', 'title', 'kind', 'band', 'love', 'life', 'people', 'punk', 'indie_rock', 'point', 'chorus', 'sort', 'emo', 'indie', 'big']\n",
      "['metal', 'riff', 'punk', 'hardcore', 'band', 'black_metal', 'noise', 'death', 'drum', 'drummer', 'group', 'doom', 'heavy', 'scream', 'early']\n",
      "['love', 'r&b', 'artist', 'singer', 'hit', 'star', 'single', 'year', 'producer', 'girl', 'debut', 'prince', 'soul', 'woman', 'drake']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "tone\n",
      "piece\n",
      "drum\n",
      "build\n",
      "sense\n",
      "melody\n",
      "close\n",
      "space\n",
      "bass\n",
      "note\n",
      "open\n",
      "light\n",
      "slow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence c_uci -0.009218938397881675\n",
      "coherence c_npmi 0.013590331586437712\n",
      "coherence c_cv 0.3895014920163413\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 481\tIter    6\tLoss:2146.6928711\tRec Loss:2137.5952148\tKL Div:9.0975351\n",
      "Epoch 481\tIter   12\tLoss:2152.9377441\tRec Loss:2144.0683594\tKL Div:8.8694544\n",
      "Epoch 481\tIter   18\tLoss:2126.0798340\tRec Loss:2117.2216797\tKL Div:8.8581953\n",
      "Epoch 482\tIter    6\tLoss:2143.3530273\tRec Loss:2134.2978516\tKL Div:9.0550947\n",
      "Epoch 482\tIter   12\tLoss:2120.9909668\tRec Loss:2112.4067383\tKL Div:8.5842619\n",
      "Epoch 482\tIter   18\tLoss:2162.3430176\tRec Loss:2153.3642578\tKL Div:8.9788303\n",
      "Epoch 483\tIter    6\tLoss:2101.5361328\tRec Loss:2092.6984863\tKL Div:8.8376923\n",
      "Epoch 483\tIter   12\tLoss:2119.9104004\tRec Loss:2111.0429688\tKL Div:8.8674107\n",
      "Epoch 483\tIter   18\tLoss:2132.9985352\tRec Loss:2124.0976562\tKL Div:8.9008846\n",
      "Epoch 484\tIter    6\tLoss:2088.9411621\tRec Loss:2080.0510254\tKL Div:8.8900967\n",
      "Epoch 484\tIter   12\tLoss:2148.0004883\tRec Loss:2139.0319824\tKL Div:8.9685411\n",
      "Epoch 484\tIter   18\tLoss:2137.2861328\tRec Loss:2128.4865723\tKL Div:8.7996330\n",
      "Epoch 485\tIter    6\tLoss:2129.5026855\tRec Loss:2120.5456543\tKL Div:8.9569931\n",
      "Epoch 485\tIter   12\tLoss:2139.7280273\tRec Loss:2130.8156738\tKL Div:8.9123516\n",
      "Epoch 485\tIter   18\tLoss:2087.0478516\tRec Loss:2078.2932129\tKL Div:8.7547541\n",
      "Epoch 486\tIter    6\tLoss:2161.5729980\tRec Loss:2152.4863281\tKL Div:9.0867329\n",
      "Epoch 486\tIter   12\tLoss:2137.2268066\tRec Loss:2128.5234375\tKL Div:8.7033367\n",
      "Epoch 486\tIter   18\tLoss:2148.0549316\tRec Loss:2139.0019531\tKL Div:9.0530014\n",
      "Epoch 487\tIter    6\tLoss:2145.1914062\tRec Loss:2136.2436523\tKL Div:8.9477634\n",
      "Epoch 487\tIter   12\tLoss:2155.3002930\tRec Loss:2146.3918457\tKL Div:8.9083529\n",
      "Epoch 487\tIter   18\tLoss:2170.7880859\tRec Loss:2161.7773438\tKL Div:9.0108414\n",
      "Epoch 488\tIter    6\tLoss:2136.6279297\tRec Loss:2127.6689453\tKL Div:8.9590340\n",
      "Epoch 488\tIter   12\tLoss:2126.8159180\tRec Loss:2117.7241211\tKL Div:9.0917301\n",
      "Epoch 488\tIter   18\tLoss:2134.1650391\tRec Loss:2125.3088379\tKL Div:8.8560829\n",
      "Epoch 489\tIter    6\tLoss:2111.8945312\tRec Loss:2102.9938965\tKL Div:8.9005318\n",
      "Epoch 489\tIter   12\tLoss:2138.5278320\tRec Loss:2129.6025391\tKL Div:8.9253349\n",
      "Epoch 489\tIter   18\tLoss:2105.5930176\tRec Loss:2096.7165527\tKL Div:8.8764439\n",
      "Epoch 490\tIter    6\tLoss:2120.0285645\tRec Loss:2111.0058594\tKL Div:9.0226479\n",
      "Epoch 490\tIter   12\tLoss:2114.4636230\tRec Loss:2105.6728516\tKL Div:8.7908382\n",
      "Epoch 490\tIter   18\tLoss:2120.4443359\tRec Loss:2111.4665527\tKL Div:8.9776812\n",
      "Epoch 491\tIter    6\tLoss:2073.5498047\tRec Loss:2064.7988281\tKL Div:8.7508583\n",
      "Epoch 491\tIter   12\tLoss:2142.0415039\tRec Loss:2133.0964355\tKL Div:8.9449596\n",
      "Epoch 491\tIter   18\tLoss:2145.6865234\tRec Loss:2136.7060547\tKL Div:8.9804363\n",
      "Epoch 492\tIter    6\tLoss:2120.5156250\tRec Loss:2111.5820312\tKL Div:8.9335270\n",
      "Epoch 492\tIter   12\tLoss:2128.2028809\tRec Loss:2119.3120117\tKL Div:8.8908939\n",
      "Epoch 492\tIter   18\tLoss:2195.3232422\tRec Loss:2186.3271484\tKL Div:8.9961109\n",
      "Epoch 493\tIter    6\tLoss:2152.5292969\tRec Loss:2143.3410645\tKL Div:9.1881657\n",
      "Epoch 493\tIter   12\tLoss:2149.7656250\tRec Loss:2140.8671875\tKL Div:8.8985548\n",
      "Epoch 493\tIter   18\tLoss:2141.7910156\tRec Loss:2132.9641113\tKL Div:8.8270245\n",
      "Epoch 494\tIter    6\tLoss:2173.5317383\tRec Loss:2164.5102539\tKL Div:9.0215282\n",
      "Epoch 494\tIter   12\tLoss:2126.8007812\tRec Loss:2117.9677734\tKL Div:8.8329439\n",
      "Epoch 494\tIter   18\tLoss:2147.3742676\tRec Loss:2138.1645508\tKL Div:9.2098064\n",
      "Epoch 495\tIter    6\tLoss:2146.8684082\tRec Loss:2138.1093750\tKL Div:8.7589140\n",
      "Epoch 495\tIter   12\tLoss:2094.8129883\tRec Loss:2085.6516113\tKL Div:9.1614609\n",
      "Epoch 495\tIter   18\tLoss:2116.5317383\tRec Loss:2107.7446289\tKL Div:8.7870293\n",
      "Epoch 496\tIter    6\tLoss:2161.0979004\tRec Loss:2152.1049805\tKL Div:8.9929714\n",
      "Epoch 496\tIter   12\tLoss:2135.6755371\tRec Loss:2126.6982422\tKL Div:8.9773169\n",
      "Epoch 496\tIter   18\tLoss:2105.5129395\tRec Loss:2096.5878906\tKL Div:8.9251442\n",
      "Epoch 497\tIter    6\tLoss:2133.1347656\tRec Loss:2124.1538086\tKL Div:8.9808674\n",
      "Epoch 497\tIter   12\tLoss:2117.9880371\tRec Loss:2109.1711426\tKL Div:8.8169203\n",
      "Epoch 497\tIter   18\tLoss:2095.1040039\tRec Loss:2086.3227539\tKL Div:8.7812634\n",
      "Epoch 498\tIter    6\tLoss:2129.5378418\tRec Loss:2120.5239258\tKL Div:9.0139627\n",
      "Epoch 498\tIter   12\tLoss:2076.9470215\tRec Loss:2068.1718750\tKL Div:8.7752638\n",
      "Epoch 498\tIter   18\tLoss:2144.5288086\tRec Loss:2135.5507812\tKL Div:8.9779339\n",
      "Epoch 499\tIter    6\tLoss:2109.1311035\tRec Loss:2100.3559570\tKL Div:8.7751865\n",
      "Epoch 499\tIter   12\tLoss:2136.0563965\tRec Loss:2127.0756836\tKL Div:8.9808178\n",
      "Epoch 499\tIter   18\tLoss:2159.1750488\tRec Loss:2150.3471680\tKL Div:8.8277855\n",
      "Epoch 500\tIter    6\tLoss:2129.5593262\tRec Loss:2120.5434570\tKL Div:9.0159187\n",
      "Epoch 500\tIter   12\tLoss:2121.7001953\tRec Loss:2112.7988281\tKL Div:8.9012699\n",
      "Epoch 500\tIter   18\tLoss:2151.4248047\tRec Loss:2142.4079590\tKL Div:9.0169525\n",
      "Epoch 501\tIter    6\tLoss:2109.6394043\tRec Loss:2100.7958984\tKL Div:8.8434973\n",
      "Epoch 501\tIter   12\tLoss:2073.3461914\tRec Loss:2064.5043945\tKL Div:8.8418322\n",
      "Epoch 501\tIter   18\tLoss:2152.0964355\tRec Loss:2143.1684570\tKL Div:8.9279690\n",
      "Epoch 502\tIter    6\tLoss:2138.3918457\tRec Loss:2129.4692383\tKL Div:8.9225731\n",
      "Epoch 502\tIter   12\tLoss:2091.5351562\tRec Loss:2082.4987793\tKL Div:9.0363178\n",
      "Epoch 502\tIter   18\tLoss:2117.8156738\tRec Loss:2108.9814453\tKL Div:8.8341236\n",
      "Epoch 503\tIter    6\tLoss:2141.1635742\tRec Loss:2132.2233887\tKL Div:8.9401617\n",
      "Epoch 503\tIter   12\tLoss:2124.8518066\tRec Loss:2116.0151367\tKL Div:8.8365850\n",
      "Epoch 503\tIter   18\tLoss:2164.7844238\tRec Loss:2155.7912598\tKL Div:8.9932137\n",
      "Epoch 504\tIter    6\tLoss:2119.2778320\tRec Loss:2110.4086914\tKL Div:8.8690500\n",
      "Epoch 504\tIter   12\tLoss:2152.7978516\tRec Loss:2144.0053711\tKL Div:8.7925034\n",
      "Epoch 504\tIter   18\tLoss:2080.2749023\tRec Loss:2071.3125000\tKL Div:8.9624729\n",
      "Epoch 505\tIter    6\tLoss:2086.7902832\tRec Loss:2077.9541016\tKL Div:8.8360910\n",
      "Epoch 505\tIter   12\tLoss:2141.8278809\tRec Loss:2132.9802246\tKL Div:8.8477211\n",
      "Epoch 505\tIter   18\tLoss:2146.6486816\tRec Loss:2137.6579590\tKL Div:8.9906950\n",
      "Epoch 506\tIter    6\tLoss:2109.8044434\tRec Loss:2101.0380859\tKL Div:8.7664661\n",
      "Epoch 506\tIter   12\tLoss:2095.9335938\tRec Loss:2086.9514160\tKL Div:8.9821148\n",
      "Epoch 506\tIter   18\tLoss:2116.3518066\tRec Loss:2107.3442383\tKL Div:9.0075893\n",
      "Epoch 507\tIter    6\tLoss:2130.0881348\tRec Loss:2121.1879883\tKL Div:8.9002075\n",
      "Epoch 507\tIter   12\tLoss:2096.2365723\tRec Loss:2087.4089355\tKL Div:8.8275280\n",
      "Epoch 507\tIter   18\tLoss:2085.7963867\tRec Loss:2076.9331055\tKL Div:8.8632755\n",
      "Epoch 508\tIter    6\tLoss:2100.5908203\tRec Loss:2091.7836914\tKL Div:8.8071308\n",
      "Epoch 508\tIter   12\tLoss:2186.9633789\tRec Loss:2177.8745117\tKL Div:9.0889587\n",
      "Epoch 508\tIter   18\tLoss:2078.9729004\tRec Loss:2070.0473633\tKL Div:8.9255466\n",
      "Epoch 509\tIter    6\tLoss:2127.5886230\tRec Loss:2118.7143555\tKL Div:8.8743830\n",
      "Epoch 509\tIter   12\tLoss:2085.8630371\tRec Loss:2076.9526367\tKL Div:8.9105206\n",
      "Epoch 509\tIter   18\tLoss:2123.9340820\tRec Loss:2115.0732422\tKL Div:8.8607445\n",
      "Epoch 510\tIter    6\tLoss:2101.0397949\tRec Loss:2092.0634766\tKL Div:8.9762802\n",
      "Epoch 510\tIter   12\tLoss:2100.8632812\tRec Loss:2092.0751953\tKL Div:8.7880478\n",
      "Epoch 510\tIter   18\tLoss:2167.1462402\tRec Loss:2158.0876465\tKL Div:9.0586147\n",
      "Epoch 511\tIter    6\tLoss:2149.1118164\tRec Loss:2139.8820801\tKL Div:9.2296638\n",
      "Epoch 511\tIter   12\tLoss:2119.2619629\tRec Loss:2110.5048828\tKL Div:8.7570438\n",
      "Epoch 511\tIter   18\tLoss:2103.4152832\tRec Loss:2094.4882812\tKL Div:8.9270897\n",
      "Epoch 512\tIter    6\tLoss:2122.3464355\tRec Loss:2113.3586426\tKL Div:8.9877377\n",
      "Epoch 512\tIter   12\tLoss:2074.6103516\tRec Loss:2065.8164062\tKL Div:8.7940254\n",
      "Epoch 512\tIter   18\tLoss:2143.9111328\tRec Loss:2135.0400391\tKL Div:8.8711891\n",
      "Epoch 513\tIter    6\tLoss:2109.7902832\tRec Loss:2100.8906250\tKL Div:8.8997726\n",
      "Epoch 513\tIter   12\tLoss:2153.7685547\tRec Loss:2144.7690430\tKL Div:8.9994011\n",
      "Epoch 513\tIter   18\tLoss:2085.2097168\tRec Loss:2076.5478516\tKL Div:8.6617794\n",
      "Epoch 514\tIter    6\tLoss:2135.0791016\tRec Loss:2125.8786621\tKL Div:9.2005329\n",
      "Epoch 514\tIter   12\tLoss:2118.1616211\tRec Loss:2109.2822266\tKL Div:8.8794250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 514\tIter   18\tLoss:2114.5451660\tRec Loss:2105.4770508\tKL Div:9.0681887\n",
      "Epoch 515\tIter    6\tLoss:2138.9316406\tRec Loss:2129.9228516\tKL Div:9.0088692\n",
      "Epoch 515\tIter   12\tLoss:2112.7248535\tRec Loss:2103.9013672\tKL Div:8.8234348\n",
      "Epoch 515\tIter   18\tLoss:2126.5878906\tRec Loss:2117.5976562\tKL Div:8.9902506\n",
      "Epoch 516\tIter    6\tLoss:2123.9150391\tRec Loss:2115.0605469\tKL Div:8.8544083\n",
      "Epoch 516\tIter   12\tLoss:2148.7275391\tRec Loss:2139.5878906\tKL Div:9.1396294\n",
      "Epoch 516\tIter   18\tLoss:2132.1484375\tRec Loss:2123.1616211\tKL Div:8.9867268\n",
      "Epoch 517\tIter    6\tLoss:2106.2026367\tRec Loss:2097.2758789\tKL Div:8.9266644\n",
      "Epoch 517\tIter   12\tLoss:2105.6276855\tRec Loss:2096.6997070\tKL Div:8.9280710\n",
      "Epoch 517\tIter   18\tLoss:2163.5031738\tRec Loss:2154.5720215\tKL Div:8.9311867\n",
      "Epoch 518\tIter    6\tLoss:2136.3820801\tRec Loss:2127.5439453\tKL Div:8.8380775\n",
      "Epoch 518\tIter   12\tLoss:2169.5954590\tRec Loss:2160.5581055\tKL Div:9.0373659\n",
      "Epoch 518\tIter   18\tLoss:2079.6250000\tRec Loss:2070.8408203\tKL Div:8.7840996\n",
      "Epoch 519\tIter    6\tLoss:2156.2119141\tRec Loss:2147.1064453\tKL Div:9.1054134\n",
      "Epoch 519\tIter   12\tLoss:2144.0788574\tRec Loss:2135.1997070\tKL Div:8.8792591\n",
      "Epoch 519\tIter   18\tLoss:2086.2937012\tRec Loss:2077.4824219\tKL Div:8.8112106\n",
      "Epoch 520\tIter    6\tLoss:2151.4340820\tRec Loss:2142.4140625\tKL Div:9.0199089\n",
      "Epoch 520\tIter   12\tLoss:2075.2402344\tRec Loss:2066.4558105\tKL Div:8.7845173\n",
      "Epoch 520\tIter   18\tLoss:2105.9609375\tRec Loss:2097.0681152\tKL Div:8.8927155\n",
      "Epoch 520\tLoss:2127.4810493\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'tone', 'piece', 'build', 'drum', 'sense', 'space', 'close', 'melody', 'bass', 'open', 'light', 'slow', 'dark']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'harmony', 'debut', 'group', 'piano', 'chorus', 'string', 'write', 'bit', 'line', 'pretty']\n",
      "['version', 'live', 'disc', 'set', 'include', 'record', 'soul', 'cover', 'label', 'studio', 'original', 'compilation', 'recording', 'group', 'early']\n",
      "['man', 'lyric', 'bad', 'listen', 'guy', 'get', 'kid', 'day', 'love', 'people', 'let', '\\n', 'shit', 'line', 'pretty']\n",
      "['house', 'techno', 'mix', 'dance', 'label', 'beat', 'producer', 'bass', 'dj', 'artist', 'synth', 'drum', 'club', 'set', 'rhythm']\n",
      "['dance', 'disco', 'synth', 'single', 'love', 'house', 'group', 'big', 'duo', 'indie', 'remix', 'electro', 'turn', 'act', 'debut']\n",
      "['\\xa0', 'project', 'point', 'year', 'artist', 'title', 'solo', 'ep', 'early', 'synth', 'close', 'place', 'producer', 'feature', 'lyric']\n",
      "['melody', 'debut', 'riff', 'drum', 'band', 'chorus', 'hook', 'line', 'bass', 'group', 'punk', 'rhythm', 'ep', 'opener', 'drummer']\n",
      "['love', 'sing', 'life', 'lyric', 'world', 'word', 'line', 'write', 'feeling', 'close', 'leave', 'light', 'death', 'sense', 'kind']\n",
      "['punk', 'live', 'single', 'love', 'band', 'cover', 'solo', 'early', 'day', 'fan', 'big', 'group', 'classic', 'set', 'smith']\n",
      "['drum', 'melody', 'jazz', 'electronic', 'noise', 'beat', 'bass', 'piece', 'interesting', 'begin', 'disc', 'bit', 'group', 'instrumental', 'live']\n",
      "['life', 'man', 'people', 'world', 'black', 'woman', 'write', 'love', 'story', 'word', 'political', 'live', 'year', 'sing', 'call']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'lyric', 'acoustic', 'record', 'artist', 'man', 'dylan', 'american', 'style', 'love']\n",
      "['beat', 'hip_hop', 'dance', 'funk', 'people', 'sample', 'party', 'mix', 'bad', 'big', 'fun', 'single', 'pretty', 'punk', 'get']\n",
      "['piece', 'piano', 'electronic', 'jazz', 'composition', 'composer', 'world', 'film', 'instrument', 'musician', 'recording', 'create', 'string', 'note', 'score']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'verse', 'production', 'year', 'producer', 'flow', 'style', 'feature', 'line', 'big', 'sample']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'sense', 'debut', 'artist', 'instrumental', 'production', 'project', 'kind', 'drum', 'texture']\n",
      "['lyric', 'title', 'kind', 'band', 'love', 'emo', 'life', 'people', 'indie_rock', 'punk', 'point', 'indie', 'chorus', 'sort', 'hook']\n",
      "['metal', 'riff', 'punk', 'hardcore', 'band', 'black_metal', 'noise', 'death', 'drum', 'heavy', 'doom', 'scream', 'group', 'drummer', 'black']\n",
      "['love', 'r&b', 'artist', 'star', 'singer', 'hit', 'single', 'year', 'producer', 'girl', 'soul', 'debut', 'prince', 'big', 'woman']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5033333333333333\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "tone\n",
      "piece\n",
      "build\n",
      "drum\n",
      "sense\n",
      "space\n",
      "close\n",
      "melody\n",
      "bass\n",
      "open\n",
      "light\n",
      "slow\n",
      "dark\n",
      "coherence c_uci -0.03102570862344466\n",
      "coherence c_npmi 0.012184400376578652\n",
      "coherence c_cv 0.3853524294350838\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   571,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 521\tIter    6\tLoss:2167.5019531\tRec Loss:2158.5302734\tKL Div:8.9716272\n",
      "Epoch 521\tIter   12\tLoss:2073.0458984\tRec Loss:2064.0991211\tKL Div:8.9468861\n",
      "Epoch 521\tIter   18\tLoss:2132.6303711\tRec Loss:2123.8540039\tKL Div:8.7763548\n",
      "Epoch 522\tIter    6\tLoss:2068.7419434\tRec Loss:2059.7319336\tKL Div:9.0101137\n",
      "Epoch 522\tIter   12\tLoss:2139.3371582\tRec Loss:2130.3979492\tKL Div:8.9390945\n",
      "Epoch 522\tIter   18\tLoss:2163.2734375\tRec Loss:2154.3718262\tKL Div:8.9015312\n",
      "Epoch 523\tIter    6\tLoss:2138.0864258\tRec Loss:2129.1723633\tKL Div:8.9140053\n",
      "Epoch 523\tIter   12\tLoss:2187.0493164\tRec Loss:2177.9941406\tKL Div:9.0550690\n",
      "Epoch 523\tIter   18\tLoss:2155.4145508\tRec Loss:2146.4797363\tKL Div:8.9348431\n",
      "Epoch 524\tIter    6\tLoss:2107.3654785\tRec Loss:2098.4106445\tKL Div:8.9549294\n",
      "Epoch 524\tIter   12\tLoss:2152.3029785\tRec Loss:2143.3930664\tKL Div:8.9100046\n",
      "Epoch 524\tIter   18\tLoss:2090.7714844\tRec Loss:2081.9609375\tKL Div:8.8104706\n",
      "Epoch 525\tIter    6\tLoss:2125.4943848\tRec Loss:2116.4445801\tKL Div:9.0498772\n",
      "Epoch 525\tIter   12\tLoss:2104.2773438\tRec Loss:2095.4873047\tKL Div:8.7900810\n",
      "Epoch 525\tIter   18\tLoss:2194.0102539\tRec Loss:2185.0366211\tKL Div:8.9736414\n",
      "Epoch 526\tIter    6\tLoss:2102.8242188\tRec Loss:2093.9301758\tKL Div:8.8939705\n",
      "Epoch 526\tIter   12\tLoss:2136.6525879\tRec Loss:2127.8078613\tKL Div:8.8446693\n",
      "Epoch 526\tIter   18\tLoss:2146.1440430\tRec Loss:2137.0043945\tKL Div:9.1397066\n",
      "Epoch 527\tIter    6\tLoss:2136.4931641\tRec Loss:2127.6025391\tKL Div:8.8905392\n",
      "Epoch 527\tIter   12\tLoss:2111.4755859\tRec Loss:2102.5234375\tKL Div:8.9521523\n",
      "Epoch 527\tIter   18\tLoss:2085.5434570\tRec Loss:2076.6508789\tKL Div:8.8924809\n",
      "Epoch 528\tIter    6\tLoss:2132.6074219\tRec Loss:2123.5222168\tKL Div:9.0851946\n",
      "Epoch 528\tIter   12\tLoss:2109.2604980\tRec Loss:2100.4560547\tKL Div:8.8045158\n",
      "Epoch 528\tIter   18\tLoss:2125.4738770\tRec Loss:2116.4350586\tKL Div:9.0387058\n",
      "Epoch 529\tIter    6\tLoss:2107.0886230\tRec Loss:2098.0869141\tKL Div:9.0017471\n",
      "Epoch 529\tIter   12\tLoss:2125.1967773\tRec Loss:2116.2021484\tKL Div:8.9945440\n",
      "Epoch 529\tIter   18\tLoss:2123.2812500\tRec Loss:2114.4694824\tKL Div:8.8118706\n",
      "Epoch 530\tIter    6\tLoss:2167.2702637\tRec Loss:2158.1472168\tKL Div:9.1230316\n",
      "Epoch 530\tIter   12\tLoss:2143.2089844\tRec Loss:2134.3374023\tKL Div:8.8716040\n",
      "Epoch 530\tIter   18\tLoss:2118.4680176\tRec Loss:2109.4487305\tKL Div:9.0191679\n",
      "Epoch 531\tIter    6\tLoss:2121.5427246\tRec Loss:2112.6406250\tKL Div:8.9021034\n",
      "Epoch 531\tIter   12\tLoss:2162.8796387\tRec Loss:2153.9267578\tKL Div:8.9527712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 531\tIter   18\tLoss:2172.5673828\tRec Loss:2163.7409668\tKL Div:8.8263865\n",
      "Epoch 532\tIter    6\tLoss:2130.0319824\tRec Loss:2121.1191406\tKL Div:8.9128504\n",
      "Epoch 532\tIter   12\tLoss:2099.8518066\tRec Loss:2090.9216309\tKL Div:8.9302864\n",
      "Epoch 532\tIter   18\tLoss:2101.4565430\tRec Loss:2092.6315918\tKL Div:8.8249722\n",
      "Epoch 533\tIter    6\tLoss:2132.4609375\tRec Loss:2123.5795898\tKL Div:8.8814592\n",
      "Epoch 533\tIter   12\tLoss:2117.2846680\tRec Loss:2108.3359375\tKL Div:8.9487877\n",
      "Epoch 533\tIter   18\tLoss:2103.1101074\tRec Loss:2094.2934570\tKL Div:8.8166046\n",
      "Epoch 534\tIter    6\tLoss:2136.7192383\tRec Loss:2127.6235352\tKL Div:9.0956926\n",
      "Epoch 534\tIter   12\tLoss:2133.8256836\tRec Loss:2124.9541016\tKL Div:8.8715820\n",
      "Epoch 534\tIter   18\tLoss:2126.1274414\tRec Loss:2117.2949219\tKL Div:8.8324471\n",
      "Epoch 535\tIter    6\tLoss:2133.4650879\tRec Loss:2124.4194336\tKL Div:9.0455580\n",
      "Epoch 535\tIter   12\tLoss:2140.0605469\tRec Loss:2131.2141113\tKL Div:8.8464937\n",
      "Epoch 535\tIter   18\tLoss:2130.8752441\tRec Loss:2121.9555664\tKL Div:8.9195681\n",
      "Epoch 536\tIter    6\tLoss:2110.7160645\tRec Loss:2101.8032227\tKL Div:8.9128275\n",
      "Epoch 536\tIter   12\tLoss:2163.1184082\tRec Loss:2154.0693359\tKL Div:9.0489559\n",
      "Epoch 536\tIter   18\tLoss:2144.1850586\tRec Loss:2135.3269043\tKL Div:8.8581905\n",
      "Epoch 537\tIter    6\tLoss:2112.9494629\tRec Loss:2104.0407715\tKL Div:8.9085941\n",
      "Epoch 537\tIter   12\tLoss:2127.6228027\tRec Loss:2118.6801758\tKL Div:8.9425507\n",
      "Epoch 537\tIter   18\tLoss:2124.8398438\tRec Loss:2115.8771973\tKL Div:8.9627037\n",
      "Epoch 538\tIter    6\tLoss:2081.3908691\tRec Loss:2072.4499512\tKL Div:8.9409351\n",
      "Epoch 538\tIter   12\tLoss:2089.7543945\tRec Loss:2080.9558105\tKL Div:8.7985172\n",
      "Epoch 538\tIter   18\tLoss:2142.8574219\tRec Loss:2133.8862305\tKL Div:8.9710741\n",
      "Epoch 539\tIter    6\tLoss:2140.3007812\tRec Loss:2131.3828125\tKL Div:8.9180136\n",
      "Epoch 539\tIter   12\tLoss:2075.5351562\tRec Loss:2066.6606445\tKL Div:8.8746033\n",
      "Epoch 539\tIter   18\tLoss:2196.3986816\tRec Loss:2187.3361816\tKL Div:9.0624943\n",
      "Epoch 540\tIter    6\tLoss:2119.4255371\tRec Loss:2110.4436035\tKL Div:8.9818573\n",
      "Epoch 540\tIter   12\tLoss:2103.2094727\tRec Loss:2094.3027344\tKL Div:8.9066248\n",
      "Epoch 540\tIter   18\tLoss:2117.0073242\tRec Loss:2108.2475586\tKL Div:8.7598324\n",
      "Epoch 541\tIter    6\tLoss:2136.7822266\tRec Loss:2127.6484375\tKL Div:9.1338978\n",
      "Epoch 541\tIter   12\tLoss:2125.8950195\tRec Loss:2116.9902344\tKL Div:8.9049006\n",
      "Epoch 541\tIter   18\tLoss:2137.5686035\tRec Loss:2128.4428711\tKL Div:9.1256657\n",
      "Epoch 542\tIter    6\tLoss:2140.8283691\tRec Loss:2131.8044434\tKL Div:9.0240211\n",
      "Epoch 542\tIter   12\tLoss:2126.9235840\tRec Loss:2118.0737305\tKL Div:8.8499451\n",
      "Epoch 542\tIter   18\tLoss:2134.0537109\tRec Loss:2125.0166016\tKL Div:9.0371151\n",
      "Epoch 543\tIter    6\tLoss:2131.3864746\tRec Loss:2122.5473633\tKL Div:8.8391695\n",
      "Epoch 543\tIter   12\tLoss:2123.5163574\tRec Loss:2114.5187988\tKL Div:8.9975204\n",
      "Epoch 543\tIter   18\tLoss:2133.8859863\tRec Loss:2124.9550781\tKL Div:8.9308367\n",
      "Epoch 544\tIter    6\tLoss:2090.7065430\tRec Loss:2081.7934570\tKL Div:8.9130421\n",
      "Epoch 544\tIter   12\tLoss:2124.3920898\tRec Loss:2115.4550781\tKL Div:8.9370031\n",
      "Epoch 544\tIter   18\tLoss:2129.1372070\tRec Loss:2120.0600586\tKL Div:9.0771608\n",
      "Epoch 545\tIter    6\tLoss:2131.8195801\tRec Loss:2122.8527832\tKL Div:8.9669075\n",
      "Epoch 545\tIter   12\tLoss:2132.8640137\tRec Loss:2123.9431152\tKL Div:8.9209747\n",
      "Epoch 545\tIter   18\tLoss:2091.8496094\tRec Loss:2082.8989258\tKL Div:8.9507313\n",
      "Epoch 546\tIter    6\tLoss:2116.0185547\tRec Loss:2106.9980469\tKL Div:9.0206242\n",
      "Epoch 546\tIter   12\tLoss:2132.5329590\tRec Loss:2123.6484375\tKL Div:8.8845253\n",
      "Epoch 546\tIter   18\tLoss:2153.5546875\tRec Loss:2144.5759277\tKL Div:8.9786587\n",
      "Epoch 547\tIter    6\tLoss:2110.6132812\tRec Loss:2101.8593750\tKL Div:8.7539177\n",
      "Epoch 547\tIter   12\tLoss:2099.6955566\tRec Loss:2090.8454590\tKL Div:8.8501663\n",
      "Epoch 547\tIter   18\tLoss:2129.7875977\tRec Loss:2120.9057617\tKL Div:8.8818607\n",
      "Epoch 548\tIter    6\tLoss:2113.9577637\tRec Loss:2104.9597168\tKL Div:8.9980965\n",
      "Epoch 548\tIter   12\tLoss:2170.6799316\tRec Loss:2161.8632812\tKL Div:8.8165722\n",
      "Epoch 548\tIter   18\tLoss:2120.5815430\tRec Loss:2111.4770508\tKL Div:9.1045189\n",
      "Epoch 549\tIter    6\tLoss:2136.7202148\tRec Loss:2127.6806641\tKL Div:9.0395012\n",
      "Epoch 549\tIter   12\tLoss:2112.3212891\tRec Loss:2103.4331055\tKL Div:8.8881035\n",
      "Epoch 549\tIter   18\tLoss:2152.9436035\tRec Loss:2143.9233398\tKL Div:9.0203562\n",
      "Epoch 550\tIter    6\tLoss:2135.5217285\tRec Loss:2126.5681152\tKL Div:8.9535542\n",
      "Epoch 550\tIter   12\tLoss:2135.3889160\tRec Loss:2126.2863770\tKL Div:9.1025677\n",
      "Epoch 550\tIter   18\tLoss:2122.4865723\tRec Loss:2113.8027344\tKL Div:8.6839428\n",
      "Epoch 551\tIter    6\tLoss:2111.3818359\tRec Loss:2102.3271484\tKL Div:9.0547428\n",
      "Epoch 551\tIter   12\tLoss:2163.6469727\tRec Loss:2154.7324219\tKL Div:8.9144974\n",
      "Epoch 551\tIter   18\tLoss:2058.6301270\tRec Loss:2049.8686523\tKL Div:8.7614918\n",
      "Epoch 552\tIter    6\tLoss:2098.7368164\tRec Loss:2089.6669922\tKL Div:9.0698204\n",
      "Epoch 552\tIter   12\tLoss:2130.3835449\tRec Loss:2121.3911133\tKL Div:8.9924622\n",
      "Epoch 552\tIter   18\tLoss:2102.1101074\tRec Loss:2093.3159180\tKL Div:8.7941513\n",
      "Epoch 553\tIter    6\tLoss:2148.3935547\tRec Loss:2139.2490234\tKL Div:9.1444206\n",
      "Epoch 553\tIter   12\tLoss:2134.4086914\tRec Loss:2125.5717773\tKL Div:8.8368549\n",
      "Epoch 553\tIter   18\tLoss:2132.1032715\tRec Loss:2123.0683594\tKL Div:9.0348482\n",
      "Epoch 554\tIter    6\tLoss:2119.6086426\tRec Loss:2110.8105469\tKL Div:8.7981415\n",
      "Epoch 554\tIter   12\tLoss:2124.5107422\tRec Loss:2115.5454102\tKL Div:8.9654503\n",
      "Epoch 554\tIter   18\tLoss:2117.9665527\tRec Loss:2109.0561523\tKL Div:8.9104404\n",
      "Epoch 555\tIter    6\tLoss:2116.5112305\tRec Loss:2107.6098633\tKL Div:8.9013996\n",
      "Epoch 555\tIter   12\tLoss:2092.1623535\tRec Loss:2083.3544922\tKL Div:8.8079147\n",
      "Epoch 555\tIter   18\tLoss:2137.7946777\tRec Loss:2128.7915039\tKL Div:9.0032129\n",
      "Epoch 556\tIter    6\tLoss:2133.6542969\tRec Loss:2124.6286621\tKL Div:9.0255594\n",
      "Epoch 556\tIter   12\tLoss:2141.1511230\tRec Loss:2132.2675781\tKL Div:8.8835354\n",
      "Epoch 556\tIter   18\tLoss:2110.5478516\tRec Loss:2101.6894531\tKL Div:8.8585052\n",
      "Epoch 557\tIter    6\tLoss:2092.9663086\tRec Loss:2083.9057617\tKL Div:9.0604935\n",
      "Epoch 557\tIter   12\tLoss:2158.3386230\tRec Loss:2149.5310059\tKL Div:8.8075075\n",
      "Epoch 557\tIter   18\tLoss:2098.6791992\tRec Loss:2089.7001953\tKL Div:8.9791117\n",
      "Epoch 558\tIter    6\tLoss:2096.7163086\tRec Loss:2087.7690430\tKL Div:8.9473705\n",
      "Epoch 558\tIter   12\tLoss:2116.3203125\tRec Loss:2107.5451660\tKL Div:8.7751141\n",
      "Epoch 558\tIter   18\tLoss:2156.6904297\tRec Loss:2147.6037598\tKL Div:9.0865602\n",
      "Epoch 559\tIter    6\tLoss:2116.8364258\tRec Loss:2107.9606934\tKL Div:8.8756599\n",
      "Epoch 559\tIter   12\tLoss:2139.0827637\tRec Loss:2130.0786133\tKL Div:9.0041580\n",
      "Epoch 559\tIter   18\tLoss:2119.2182617\tRec Loss:2110.3159180\tKL Div:8.9024572\n",
      "Epoch 560\tIter    6\tLoss:2152.9907227\tRec Loss:2144.0847168\tKL Div:8.9059334\n",
      "Epoch 560\tIter   12\tLoss:2119.8632812\tRec Loss:2110.7810059\tKL Div:9.0821562\n",
      "Epoch 560\tIter   18\tLoss:2132.5573730\tRec Loss:2123.8093262\tKL Div:8.7480431\n",
      "Epoch 560\tLoss:2126.0498085\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'tone', 'piece', 'sense', 'build', 'melody', 'drum', 'space', 'close', 'light', 'bass', 'ambient', 'dark', 'note']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'debut', 'piano', 'harmony', 'group', 'chorus', 'line', 'string', 'bit', 'write', 'pretty']\n",
      "['version', 'live', 'disc', 'set', 'include', 'record', 'soul', 'cover', 'studio', 'label', 'recording', 'original', 'group', 'compilation', 'early']\n",
      "['lyric', 'man', 'bad', 'listen', 'guy', 'day', 'get', 'kid', '\\n', 'people', 'love', 'let', 'pretty', 'line', 'kind']\n",
      "['house', 'techno', 'mix', 'dance', 'label', 'beat', 'producer', 'bass', 'dj', 'artist', 'synth', 'drum', 'club', 'rhythm', 'set']\n",
      "['dance', 'disco', 'synth', 'single', 'house', 'love', 'group', 'big', 'duo', 'indie', 'remix', 'electro', 'club', 'act', 'turn']\n",
      "['\\xa0', 'project', 'point', 'artist', 'year', 'title', 'solo', 'early', 'ep', 'synth', 'sense', 'close', 'place', 'producer', 'feature']\n",
      "['melody', 'riff', 'debut', 'chorus', 'drum', 'band', 'hook', 'line', 'punk', 'group', 'bass', 'ep', 'rhythm', 'hard', 'opener']\n",
      "['sing', 'love', 'life', 'lyric', 'line', 'word', 'world', 'write', 'feeling', 'close', 'leave', 'light', 'death', 'sense', 'kind']\n",
      "['punk', 'live', 'single', 'love', 'cover', 'band', 'early', 'solo', 'big', 'day', 'fan', 'set', 'classic', 'group', 'smith']\n",
      "['drum', 'electronic', 'jazz', 'melody', 'noise', 'bass', 'beat', 'piece', 'begin', 'feature', 'live', 'group', 'interesting', 'disc', 'instrumental']\n",
      "['life', 'man', 'world', 'people', 'black', 'woman', 'write', 'love', 'story', 'live', 'sing', 'word', 'call', 'political', 'white']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'acoustic', 'record', 'lyric', 'dylan', 'artist', 'oldham', 'man', 'american', 'love']\n",
      "['beat', 'hip_hop', 'dance', 'funk', 'party', 'sample', 'mix', 'people', 'big', 'bad', 'fun', 'pretty', 'genre', 'get', 'single']\n",
      "['piece', 'piano', 'composer', 'jazz', 'composition', 'film', 'world', 'musician', 'electronic', 'instrument', 'recording', 'create', 'note', 'string', 'group']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'verse', 'production', 'year', 'producer', 'flow', 'style', 'line', 'feature', 'big', 'artist']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'sense', 'debut', 'artist', 'drum', 'kind', 'production', 'instrumental', 'build', 'project']\n",
      "['lyric', 'kind', 'title', 'love', 'band', 'people', 'life', 'indie_rock', 'point', 'emo', 'sort', 'punk', 'chorus', 'indie', 'hook']\n",
      "['metal', 'riff', 'punk', 'hardcore', 'black_metal', 'band', 'noise', 'death', 'drum', 'doom', 'heavy', 'drummer', 'scream', 'group', 'black']\n",
      "['love', 'r&b', 'artist', 'star', 'singer', 'single', 'hit', 'year', 'producer', 'girl', 'debut', 'prince', 'soul', 'woman', 'drake']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5066666666666667\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "tone\n",
      "piece\n",
      "sense\n",
      "build\n",
      "melody\n",
      "drum\n",
      "space\n",
      "close\n",
      "light\n",
      "bass\n",
      "ambient\n",
      "dark\n",
      "note\n",
      "coherence c_uci -0.05151984040704029\n",
      "coherence c_npmi 0.011260357968015958\n",
      "coherence c_cv 0.3867863733261995\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 561\tIter    6\tLoss:2110.3476562\tRec Loss:2101.2404785\tKL Div:9.1072063\n",
      "Epoch 561\tIter   12\tLoss:2105.9023438\tRec Loss:2097.1059570\tKL Div:8.7963371\n",
      "Epoch 561\tIter   18\tLoss:2122.7360840\tRec Loss:2113.8017578\tKL Div:8.9343853\n",
      "Epoch 562\tIter    6\tLoss:2167.7016602\tRec Loss:2158.6274414\tKL Div:9.0743179\n",
      "Epoch 562\tIter   12\tLoss:2141.6630859\tRec Loss:2132.6337891\tKL Div:9.0293255\n",
      "Epoch 562\tIter   18\tLoss:2115.6010742\tRec Loss:2106.8166504\tKL Div:8.7845230\n",
      "Epoch 563\tIter    6\tLoss:2085.2709961\tRec Loss:2076.2312012\tKL Div:9.0398407\n",
      "Epoch 563\tIter   12\tLoss:2138.0603027\tRec Loss:2129.2084961\tKL Div:8.8518724\n",
      "Epoch 563\tIter   18\tLoss:2144.1464844\tRec Loss:2135.1960449\tKL Div:8.9503403\n",
      "Epoch 564\tIter    6\tLoss:2135.0651855\tRec Loss:2126.0485840\tKL Div:9.0165777\n",
      "Epoch 564\tIter   12\tLoss:2119.7524414\tRec Loss:2110.7329102\tKL Div:9.0195103\n",
      "Epoch 564\tIter   18\tLoss:2135.9797363\tRec Loss:2127.1506348\tKL Div:8.8290844\n",
      "Epoch 565\tIter    6\tLoss:2179.4064941\tRec Loss:2170.2036133\tKL Div:9.2028074\n",
      "Epoch 565\tIter   12\tLoss:2100.9956055\tRec Loss:2092.2036133\tKL Div:8.7918758\n",
      "Epoch 565\tIter   18\tLoss:2123.4506836\tRec Loss:2114.6181641\tKL Div:8.8325062\n",
      "Epoch 566\tIter    6\tLoss:2140.0961914\tRec Loss:2131.0478516\tKL Div:9.0484600\n",
      "Epoch 566\tIter   12\tLoss:2147.5761719\tRec Loss:2138.6518555\tKL Div:8.9243746\n",
      "Epoch 566\tIter   18\tLoss:2114.9455566\tRec Loss:2106.0913086\tKL Div:8.8542328\n",
      "Epoch 567\tIter    6\tLoss:2141.3083496\tRec Loss:2132.2709961\tKL Div:9.0372572\n",
      "Epoch 567\tIter   12\tLoss:2116.5561523\tRec Loss:2107.5502930\tKL Div:9.0058575\n",
      "Epoch 567\tIter   18\tLoss:2146.9045410\tRec Loss:2137.9855957\tKL Div:8.9188480\n",
      "Epoch 568\tIter    6\tLoss:2124.8603516\tRec Loss:2115.7631836\tKL Div:9.0971823\n",
      "Epoch 568\tIter   12\tLoss:2144.0251465\tRec Loss:2135.1958008\tKL Div:8.8293629\n",
      "Epoch 568\tIter   18\tLoss:2116.9213867\tRec Loss:2107.8344727\tKL Div:9.0868835\n",
      "Epoch 569\tIter    6\tLoss:2159.7021484\tRec Loss:2150.8291016\tKL Div:8.8729620\n",
      "Epoch 569\tIter   12\tLoss:2143.2937012\tRec Loss:2134.3300781\tKL Div:8.9636192\n",
      "Epoch 569\tIter   18\tLoss:2116.9064941\tRec Loss:2108.1191406\tKL Div:8.7872677\n",
      "Epoch 570\tIter    6\tLoss:2118.4277344\tRec Loss:2109.4509277\tKL Div:8.9768057\n",
      "Epoch 570\tIter   12\tLoss:2132.5751953\tRec Loss:2123.5585938\tKL Div:9.0165443\n",
      "Epoch 570\tIter   18\tLoss:2128.2893066\tRec Loss:2119.3627930\tKL Div:8.9265957\n",
      "Epoch 571\tIter    6\tLoss:2135.7211914\tRec Loss:2126.7011719\tKL Div:9.0201330\n",
      "Epoch 571\tIter   12\tLoss:2097.4628906\tRec Loss:2088.5849609\tKL Div:8.8779297\n",
      "Epoch 571\tIter   18\tLoss:2089.7629395\tRec Loss:2081.0485840\tKL Div:8.7143669\n",
      "Epoch 572\tIter    6\tLoss:2111.7949219\tRec Loss:2102.7880859\tKL Div:9.0067406\n",
      "Epoch 572\tIter   12\tLoss:2154.5820312\tRec Loss:2145.7172852\tKL Div:8.8647337\n",
      "Epoch 572\tIter   18\tLoss:2155.4809570\tRec Loss:2146.3806152\tKL Div:9.1002884\n",
      "Epoch 573\tIter    6\tLoss:2124.3398438\tRec Loss:2115.5769043\tKL Div:8.7630310\n",
      "Epoch 573\tIter   12\tLoss:2109.5205078\tRec Loss:2100.5615234\tKL Div:8.9589100\n",
      "Epoch 573\tIter   18\tLoss:2144.0285645\tRec Loss:2134.9804688\tKL Div:9.0480309\n",
      "Epoch 574\tIter    6\tLoss:2157.2253418\tRec Loss:2148.3935547\tKL Div:8.8317308\n",
      "Epoch 574\tIter   12\tLoss:2109.8486328\tRec Loss:2100.7050781\tKL Div:9.1435947\n",
      "Epoch 574\tIter   18\tLoss:2090.9875488\tRec Loss:2082.2268066\tKL Div:8.7607059\n",
      "Epoch 575\tIter    6\tLoss:2104.3227539\tRec Loss:2095.3120117\tKL Div:9.0106659\n",
      "Epoch 575\tIter   12\tLoss:2146.3083496\tRec Loss:2137.3571777\tKL Div:8.9512596\n",
      "Epoch 575\tIter   18\tLoss:2140.8955078\tRec Loss:2131.8049316\tKL Div:9.0906219\n",
      "Epoch 576\tIter    6\tLoss:2146.3457031\tRec Loss:2137.3579102\tKL Div:8.9876862\n",
      "Epoch 576\tIter   12\tLoss:2122.2353516\tRec Loss:2113.2700195\tKL Div:8.9653177\n",
      "Epoch 576\tIter   18\tLoss:2118.5285645\tRec Loss:2109.6821289\tKL Div:8.8464031\n",
      "Epoch 577\tIter    6\tLoss:2130.0063477\tRec Loss:2120.9023438\tKL Div:9.1041117\n",
      "Epoch 577\tIter   12\tLoss:2087.4863281\tRec Loss:2078.6977539\tKL Div:8.7884665\n",
      "Epoch 577\tIter   18\tLoss:2150.7233887\tRec Loss:2141.7331543\tKL Div:8.9901524\n",
      "Epoch 578\tIter    6\tLoss:2142.5300293\tRec Loss:2133.4860840\tKL Div:9.0440598\n",
      "Epoch 578\tIter   12\tLoss:2138.7043457\tRec Loss:2129.8901367\tKL Div:8.8142891\n",
      "Epoch 578\tIter   18\tLoss:2131.5944824\tRec Loss:2122.3845215\tKL Div:9.2098942\n",
      "Epoch 579\tIter    6\tLoss:2152.7736816\tRec Loss:2143.9067383\tKL Div:8.8669119\n",
      "Epoch 579\tIter   12\tLoss:2149.2512207\tRec Loss:2140.1372070\tKL Div:9.1139679\n",
      "Epoch 579\tIter   18\tLoss:2080.9003906\tRec Loss:2072.0522461\tKL Div:8.8481798\n",
      "Epoch 580\tIter    6\tLoss:2149.0175781\tRec Loss:2140.0300293\tKL Div:8.9876432\n",
      "Epoch 580\tIter   12\tLoss:2137.5358887\tRec Loss:2128.5817871\tKL Div:8.9540405\n",
      "Epoch 580\tIter   18\tLoss:2089.1208496\tRec Loss:2080.2368164\tKL Div:8.8839817\n",
      "Epoch 581\tIter    6\tLoss:2139.6721191\tRec Loss:2130.6174316\tKL Div:9.0545959\n",
      "Epoch 581\tIter   12\tLoss:2108.9946289\tRec Loss:2100.1997070\tKL Div:8.7949276\n",
      "Epoch 581\tIter   18\tLoss:2136.7216797\tRec Loss:2127.8073730\tKL Div:8.9141884\n",
      "Epoch 582\tIter    6\tLoss:2134.9179688\tRec Loss:2125.8383789\tKL Div:9.0796976\n",
      "Epoch 582\tIter   12\tLoss:2073.4035645\tRec Loss:2064.5854492\tKL Div:8.8181076\n",
      "Epoch 582\tIter   18\tLoss:2093.6413574\tRec Loss:2084.7714844\tKL Div:8.8697844\n",
      "Epoch 583\tIter    6\tLoss:2129.0314941\tRec Loss:2120.0747070\tKL Div:8.9568748\n",
      "Epoch 583\tIter   12\tLoss:2079.7285156\tRec Loss:2070.8100586\tKL Div:8.9184017\n",
      "Epoch 583\tIter   18\tLoss:2168.6237793\tRec Loss:2159.5981445\tKL Div:9.0256596\n",
      "Epoch 584\tIter    6\tLoss:2108.5134277\tRec Loss:2099.6157227\tKL Div:8.8978224\n",
      "Epoch 584\tIter   12\tLoss:2158.7360840\tRec Loss:2149.8222656\tKL Div:8.9137115\n",
      "Epoch 584\tIter   18\tLoss:2088.0449219\tRec Loss:2079.2595215\tKL Div:8.7853489\n",
      "Epoch 585\tIter    6\tLoss:2166.0290527\tRec Loss:2156.9716797\tKL Div:9.0574093\n",
      "Epoch 585\tIter   12\tLoss:2130.0786133\tRec Loss:2121.1589355\tKL Div:8.9197083\n",
      "Epoch 585\tIter   18\tLoss:2099.4353027\tRec Loss:2090.5771484\tKL Div:8.8581181\n",
      "Epoch 586\tIter    6\tLoss:2144.7011719\tRec Loss:2135.5864258\tKL Div:9.1147318\n",
      "Epoch 586\tIter   12\tLoss:2095.3491211\tRec Loss:2086.4431152\tKL Div:8.9060612\n",
      "Epoch 586\tIter   18\tLoss:2132.5725098\tRec Loss:2123.7075195\tKL Div:8.8650360\n",
      "Epoch 587\tIter    6\tLoss:2094.1127930\tRec Loss:2085.0952148\tKL Div:9.0175142\n",
      "Epoch 587\tIter   12\tLoss:2088.5917969\tRec Loss:2079.9047852\tKL Div:8.6869812\n",
      "Epoch 587\tIter   18\tLoss:2124.1877441\tRec Loss:2115.1271973\tKL Div:9.0606155\n",
      "Epoch 588\tIter    6\tLoss:2113.2121582\tRec Loss:2104.1801758\tKL Div:9.0319977\n",
      "Epoch 588\tIter   12\tLoss:2125.4853516\tRec Loss:2116.7521973\tKL Div:8.7330914\n",
      "Epoch 588\tIter   18\tLoss:2122.5317383\tRec Loss:2113.4572754\tKL Div:9.0745344\n",
      "Epoch 589\tIter    6\tLoss:2124.2250977\tRec Loss:2115.4467773\tKL Div:8.7782707\n",
      "Epoch 589\tIter   12\tLoss:2125.4885254\tRec Loss:2116.2329102\tKL Div:9.2555666\n",
      "Epoch 589\tIter   18\tLoss:2137.2695312\tRec Loss:2128.4692383\tKL Div:8.8003063\n",
      "Epoch 590\tIter    6\tLoss:2155.9328613\tRec Loss:2146.7077637\tKL Div:9.2250233\n",
      "Epoch 590\tIter   12\tLoss:2086.8391113\tRec Loss:2078.0156250\tKL Div:8.8234434\n",
      "Epoch 590\tIter   18\tLoss:2139.0710449\tRec Loss:2129.9516602\tKL Div:9.1192646\n",
      "Epoch 591\tIter    6\tLoss:2096.6367188\tRec Loss:2087.8183594\tKL Div:8.8184137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591\tIter   12\tLoss:2117.7690430\tRec Loss:2108.7309570\tKL Div:9.0381231\n",
      "Epoch 591\tIter   18\tLoss:2138.0649414\tRec Loss:2129.2106934\tKL Div:8.8542490\n",
      "Epoch 592\tIter    6\tLoss:2129.9497070\tRec Loss:2121.0493164\tKL Div:8.9003716\n",
      "Epoch 592\tIter   12\tLoss:2138.8669434\tRec Loss:2129.7460938\tKL Div:9.1207943\n",
      "Epoch 592\tIter   18\tLoss:2116.9953613\tRec Loss:2108.2338867\tKL Div:8.7615128\n",
      "Epoch 593\tIter    6\tLoss:2065.7929688\tRec Loss:2056.7529297\tKL Div:9.0400677\n",
      "Epoch 593\tIter   12\tLoss:2085.3454590\tRec Loss:2076.4814453\tKL Div:8.8640175\n",
      "Epoch 593\tIter   18\tLoss:2128.8278809\tRec Loss:2119.7758789\tKL Div:9.0519371\n",
      "Epoch 594\tIter    6\tLoss:2115.4467773\tRec Loss:2106.5720215\tKL Div:8.8746414\n",
      "Epoch 594\tIter   12\tLoss:2122.4609375\tRec Loss:2113.5842285\tKL Div:8.8766356\n",
      "Epoch 594\tIter   18\tLoss:2070.2338867\tRec Loss:2061.3232422\tKL Div:8.9107113\n",
      "Epoch 595\tIter    6\tLoss:2124.3845215\tRec Loss:2115.3579102\tKL Div:9.0266418\n",
      "Epoch 595\tIter   12\tLoss:2109.9094238\tRec Loss:2101.0322266\tKL Div:8.8771954\n",
      "Epoch 595\tIter   18\tLoss:2136.8569336\tRec Loss:2128.0136719\tKL Div:8.8431606\n",
      "Epoch 596\tIter    6\tLoss:2106.5239258\tRec Loss:2097.6655273\tKL Div:8.8583889\n",
      "Epoch 596\tIter   12\tLoss:2124.8522949\tRec Loss:2115.6389160\tKL Div:9.2133932\n",
      "Epoch 596\tIter   18\tLoss:2139.5756836\tRec Loss:2130.7778320\tKL Div:8.7977400\n",
      "Epoch 597\tIter    6\tLoss:2150.7583008\tRec Loss:2141.5913086\tKL Div:9.1669350\n",
      "Epoch 597\tIter   12\tLoss:2100.3786621\tRec Loss:2091.5825195\tKL Div:8.7962418\n",
      "Epoch 597\tIter   18\tLoss:2134.6572266\tRec Loss:2125.5095215\tKL Div:9.1476784\n",
      "Epoch 598\tIter    6\tLoss:2131.7888184\tRec Loss:2122.8916016\tKL Div:8.8972063\n",
      "Epoch 598\tIter   12\tLoss:2119.7956543\tRec Loss:2110.7829590\tKL Div:9.0126839\n",
      "Epoch 598\tIter   18\tLoss:2140.1240234\tRec Loss:2131.1801758\tKL Div:8.9438238\n",
      "Epoch 599\tIter    6\tLoss:2144.2663574\tRec Loss:2135.2517090\tKL Div:9.0145292\n",
      "Epoch 599\tIter   12\tLoss:2114.3322754\tRec Loss:2105.2973633\tKL Div:9.0349541\n",
      "Epoch 599\tIter   18\tLoss:2165.7629395\tRec Loss:2156.8288574\tKL Div:8.9339867\n",
      "Epoch 600\tIter    6\tLoss:2087.1254883\tRec Loss:2078.2031250\tKL Div:8.9223385\n",
      "Epoch 600\tIter   12\tLoss:2115.3210449\tRec Loss:2106.3041992\tKL Div:9.0169420\n",
      "Epoch 600\tIter   18\tLoss:2123.7609863\tRec Loss:2114.8718262\tKL Div:8.8892794\n",
      "Epoch 600\tLoss:2128.0804830\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'tone', 'piece', 'build', 'space', 'sense', 'drum', 'close', 'melody', 'bass', 'light', 'electronic', 'open', 'string']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'harmony', 'debut', 'group', 'piano', 'chorus', 'line', 'string', 'bit', 'write', 'pretty']\n",
      "['live', 'version', 'disc', 'set', 'include', 'record', 'soul', 'recording', 'label', 'studio', 'cover', 'compilation', 'original', 'group', 'early']\n",
      "['lyric', 'man', 'guy', 'listen', 'bad', 'get', 'day', 'kid', 'love', 'people', '\\n', 'let', 'girl', 'pretty', 'shit']\n",
      "['house', 'techno', 'mix', 'label', 'dance', 'producer', 'beat', 'bass', 'dj', 'artist', 'synth', 'rhythm', 'drum', 'club', 'set']\n",
      "['dance', 'disco', 'synth', 'single', 'house', 'love', 'group', 'duo', 'big', 'remix', 'indie', 'electro', 'club', 'turn', 'act']\n",
      "['\\xa0', 'project', 'artist', 'point', 'year', 'solo', 'title', 'early', 'synth', 'ep', 'place', 'producer', 'close', 'feature', 'show']\n",
      "['melody', 'riff', 'debut', 'drum', 'chorus', 'band', 'hook', 'line', 'group', 'bass', 'punk', 'rhythm', 'ep', 'opener', 'build']\n",
      "['sing', 'love', 'life', 'lyric', 'line', 'world', 'word', 'write', 'close', 'feeling', 'leave', 'death', 'sense', 'light', 'place']\n",
      "['punk', 'live', 'single', 'love', 'band', 'cover', 'early', 'solo', 'fan', 'day', 'classic', 'big', 'group', 'smith', 'set']\n",
      "['drum', 'electronic', 'jazz', 'melody', 'noise', 'bass', 'beat', 'piece', 'begin', 'group', 'interesting', 'disc', 'feature', 'instrumental', 'live']\n",
      "['life', 'man', 'world', 'black', 'people', 'love', 'write', 'woman', 'story', 'word', 'white', 'call', 'live', 'political', 'sing']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'dylan', 'lyric', 'acoustic', 'artist', 'record', 'man', 'american', 'love', 'style']\n",
      "['beat', 'hip_hop', 'sample', 'funk', 'dance', 'party', 'mix', 'people', 'big', 'bad', 'fun', 'pretty', 'single', 'get', 'genre']\n",
      "['piece', 'piano', 'composer', 'jazz', 'world', 'film', 'electronic', 'composition', 'musician', 'instrument', 'recording', 'create', 'string', 'note', 'group']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'verse', 'mixtape', 'production', 'year', 'producer', 'flow', 'style', 'feature', 'line', 'big', 'artist']\n",
      "['synth', 'beat', 'electronic', 'ep', 'melody', 'sample', 'sense', 'debut', 'artist', 'instrumental', 'production', 'kind', 'project', 'drum', 'build']\n",
      "['lyric', 'kind', 'title', 'band', 'people', 'love', 'life', 'indie_rock', 'point', 'sort', 'punk', 'emo', 'indie', 'chorus', 'hook']\n",
      "['metal', 'riff', 'hardcore', 'punk', 'band', 'noise', 'black_metal', 'death', 'drum', 'heavy', 'drummer', 'doom', 'black', 'scream', 'group']\n",
      "['love', 'r&b', 'artist', 'hit', 'single', 'singer', 'star', 'year', 'producer', 'girl', 'debut', 'soul', 'big', 'prince', 'drake']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5033333333333333\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "tone\n",
      "piece\n",
      "build\n",
      "space\n",
      "sense\n",
      "drum\n",
      "close\n",
      "melody\n",
      "bass\n",
      "light\n",
      "electronic\n",
      "open\n",
      "string\n",
      "coherence c_uci -0.028636426613563\n",
      "coherence c_npmi 0.012836685181979529\n",
      "coherence c_cv 0.3886554449741657\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 601\tIter    6\tLoss:2166.9758301\tRec Loss:2157.9780273\tKL Div:8.9979010\n",
      "Epoch 601\tIter   12\tLoss:2109.3864746\tRec Loss:2100.5822754\tKL Div:8.8041277\n",
      "Epoch 601\tIter   18\tLoss:2105.3903809\tRec Loss:2096.4882812\tKL Div:8.9020653\n",
      "Epoch 602\tIter    6\tLoss:2103.1994629\tRec Loss:2094.2077637\tKL Div:8.9915810\n",
      "Epoch 602\tIter   12\tLoss:2117.7861328\tRec Loss:2108.9289551\tKL Div:8.8571777\n",
      "Epoch 602\tIter   18\tLoss:2138.1960449\tRec Loss:2129.1811523\tKL Div:9.0149727\n",
      "Epoch 603\tIter    6\tLoss:2116.7314453\tRec Loss:2107.7890625\tKL Div:8.9422712\n",
      "Epoch 603\tIter   12\tLoss:2131.0541992\tRec Loss:2122.3666992\tKL Div:8.6875210\n",
      "Epoch 603\tIter   18\tLoss:2099.3388672\tRec Loss:2090.4218750\tKL Div:8.9169388\n",
      "Epoch 604\tIter    6\tLoss:2127.9787598\tRec Loss:2118.9624023\tKL Div:9.0164051\n",
      "Epoch 604\tIter   12\tLoss:2116.9997559\tRec Loss:2108.1386719\tKL Div:8.8610506\n",
      "Epoch 604\tIter   18\tLoss:2165.7102051\tRec Loss:2156.6259766\tKL Div:9.0842667\n",
      "Epoch 605\tIter    6\tLoss:2125.1699219\tRec Loss:2116.1572266\tKL Div:9.0127621\n",
      "Epoch 605\tIter   12\tLoss:2072.2026367\tRec Loss:2063.3295898\tKL Div:8.8729782\n",
      "Epoch 605\tIter   18\tLoss:2083.4143066\tRec Loss:2074.5439453\tKL Div:8.8703871\n",
      "Epoch 606\tIter    6\tLoss:2107.5041504\tRec Loss:2098.4277344\tKL Div:9.0763664\n",
      "Epoch 606\tIter   12\tLoss:2131.0515137\tRec Loss:2122.2395020\tKL Div:8.8118935\n",
      "Epoch 606\tIter   18\tLoss:2115.8686523\tRec Loss:2106.7626953\tKL Div:9.1059141\n",
      "Epoch 607\tIter    6\tLoss:2113.0461426\tRec Loss:2104.1540527\tKL Div:8.8921413\n",
      "Epoch 607\tIter   12\tLoss:2120.9016113\tRec Loss:2111.9423828\tKL Div:8.9591684\n",
      "Epoch 607\tIter   18\tLoss:2152.0957031\tRec Loss:2143.0107422\tKL Div:9.0850687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 608\tIter    6\tLoss:2142.8291016\tRec Loss:2133.8210449\tKL Div:9.0079670\n",
      "Epoch 608\tIter   12\tLoss:2168.6999512\tRec Loss:2159.5678711\tKL Div:9.1321955\n",
      "Epoch 608\tIter   18\tLoss:2123.3151855\tRec Loss:2114.4584961\tKL Div:8.8566093\n",
      "Epoch 609\tIter    6\tLoss:2119.4792480\tRec Loss:2110.5390625\tKL Div:8.9400749\n",
      "Epoch 609\tIter   12\tLoss:2134.8164062\tRec Loss:2125.8308105\tKL Div:8.9854774\n",
      "Epoch 609\tIter   18\tLoss:2130.8867188\tRec Loss:2121.8618164\tKL Div:9.0248775\n",
      "Epoch 610\tIter    6\tLoss:2146.6455078\tRec Loss:2137.6513672\tKL Div:8.9941206\n",
      "Epoch 610\tIter   12\tLoss:2143.5795898\tRec Loss:2134.6904297\tKL Div:8.8890591\n",
      "Epoch 610\tIter   18\tLoss:2154.8115234\tRec Loss:2145.7595215\tKL Div:9.0519505\n",
      "Epoch 611\tIter    6\tLoss:2121.2395020\tRec Loss:2112.2978516\tKL Div:8.9415407\n",
      "Epoch 611\tIter   12\tLoss:2084.5766602\tRec Loss:2075.4833984\tKL Div:9.0933409\n",
      "Epoch 611\tIter   18\tLoss:2155.8623047\tRec Loss:2147.0683594\tKL Div:8.7940235\n",
      "Epoch 612\tIter    6\tLoss:2161.3562012\tRec Loss:2152.3894043\tKL Div:8.9668961\n",
      "Epoch 612\tIter   12\tLoss:2130.3769531\tRec Loss:2121.4111328\tKL Div:8.9658585\n",
      "Epoch 612\tIter   18\tLoss:2069.0048828\tRec Loss:2060.1342773\tKL Div:8.8706284\n",
      "Epoch 613\tIter    6\tLoss:2141.3796387\tRec Loss:2132.3015137\tKL Div:9.0780296\n",
      "Epoch 613\tIter   12\tLoss:2132.6013184\tRec Loss:2123.8566895\tKL Div:8.7445621\n",
      "Epoch 613\tIter   18\tLoss:2090.0781250\tRec Loss:2081.1098633\tKL Div:8.9682465\n",
      "Epoch 614\tIter    6\tLoss:2161.7548828\tRec Loss:2152.6455078\tKL Div:9.1093206\n",
      "Epoch 614\tIter   12\tLoss:2134.4692383\tRec Loss:2125.5314941\tKL Div:8.9377565\n",
      "Epoch 614\tIter   18\tLoss:2165.9489746\tRec Loss:2157.0607910\tKL Div:8.8882122\n",
      "Epoch 615\tIter    6\tLoss:2151.7653809\tRec Loss:2142.7810059\tKL Div:8.9843540\n",
      "Epoch 615\tIter   12\tLoss:2142.8161621\tRec Loss:2133.9184570\tKL Div:8.8977585\n",
      "Epoch 615\tIter   18\tLoss:2111.0498047\tRec Loss:2101.9711914\tKL Div:9.0785551\n",
      "Epoch 616\tIter    6\tLoss:2110.0332031\tRec Loss:2101.1098633\tKL Div:8.9234142\n",
      "Epoch 616\tIter   12\tLoss:2109.3525391\tRec Loss:2100.4116211\tKL Div:8.9408083\n",
      "Epoch 616\tIter   18\tLoss:2101.1584473\tRec Loss:2092.2290039\tKL Div:8.9293232\n",
      "Epoch 617\tIter    6\tLoss:2145.3186035\tRec Loss:2136.3872070\tKL Div:8.9313154\n",
      "Epoch 617\tIter   12\tLoss:2104.7822266\tRec Loss:2096.0209961\tKL Div:8.7612553\n",
      "Epoch 617\tIter   18\tLoss:2175.4226074\tRec Loss:2166.4760742\tKL Div:8.9465332\n",
      "Epoch 618\tIter    6\tLoss:2153.3542480\tRec Loss:2144.2387695\tKL Div:9.1155796\n",
      "Epoch 618\tIter   12\tLoss:2148.7265625\tRec Loss:2139.8901367\tKL Div:8.8364096\n",
      "Epoch 618\tIter   18\tLoss:2128.4501953\tRec Loss:2119.3623047\tKL Div:9.0878267\n",
      "Epoch 619\tIter    6\tLoss:2109.8635254\tRec Loss:2100.8750000\tKL Div:8.9886379\n",
      "Epoch 619\tIter   12\tLoss:2125.3876953\tRec Loss:2116.3923340\tKL Div:8.9953384\n",
      "Epoch 619\tIter   18\tLoss:2110.7167969\tRec Loss:2101.8159180\tKL Div:8.9009304\n",
      "Epoch 620\tIter    6\tLoss:2107.7446289\tRec Loss:2098.8503418\tKL Div:8.8943567\n",
      "Epoch 620\tIter   12\tLoss:2138.5610352\tRec Loss:2129.5908203\tKL Div:8.9701881\n",
      "Epoch 620\tIter   18\tLoss:2124.5959473\tRec Loss:2115.7517090\tKL Div:8.8441305\n",
      "Epoch 621\tIter    6\tLoss:2148.1164551\tRec Loss:2139.0144043\tKL Div:9.1019592\n",
      "Epoch 621\tIter   12\tLoss:2128.2487793\tRec Loss:2119.3828125\tKL Div:8.8660851\n",
      "Epoch 621\tIter   18\tLoss:2092.1528320\tRec Loss:2083.2016602\tKL Div:8.9512711\n",
      "Epoch 622\tIter    6\tLoss:2159.4294434\tRec Loss:2150.3525391\tKL Div:9.0768490\n",
      "Epoch 622\tIter   12\tLoss:2136.8151855\tRec Loss:2128.0312500\tKL Div:8.7838373\n",
      "Epoch 622\tIter   18\tLoss:2120.4150391\tRec Loss:2111.2080078\tKL Div:9.2069178\n",
      "Epoch 623\tIter    6\tLoss:2156.9099121\tRec Loss:2148.0637207\tKL Div:8.8461113\n",
      "Epoch 623\tIter   12\tLoss:2107.7583008\tRec Loss:2098.7084961\tKL Div:9.0498600\n",
      "Epoch 623\tIter   18\tLoss:2127.5639648\tRec Loss:2118.6804199\tKL Div:8.8835478\n",
      "Epoch 624\tIter    6\tLoss:2135.7321777\tRec Loss:2126.6638184\tKL Div:9.0683975\n",
      "Epoch 624\tIter   12\tLoss:2134.8583984\tRec Loss:2126.0185547\tKL Div:8.8399010\n",
      "Epoch 624\tIter   18\tLoss:2114.8898926\tRec Loss:2106.0107422\tKL Div:8.8790884\n",
      "Epoch 625\tIter    6\tLoss:2065.3669434\tRec Loss:2056.3217773\tKL Div:9.0452709\n",
      "Epoch 625\tIter   12\tLoss:2120.4704590\tRec Loss:2111.5537109\tKL Div:8.9167690\n",
      "Epoch 625\tIter   18\tLoss:2092.3840332\tRec Loss:2083.3603516\tKL Div:9.0235748\n",
      "Epoch 626\tIter    6\tLoss:2136.4714355\tRec Loss:2127.3789062\tKL Div:9.0925026\n",
      "Epoch 626\tIter   12\tLoss:2119.5727539\tRec Loss:2110.7075195\tKL Div:8.8651752\n",
      "Epoch 626\tIter   18\tLoss:2123.3801270\tRec Loss:2114.5158691\tKL Div:8.8643208\n",
      "Epoch 627\tIter    6\tLoss:2116.4621582\tRec Loss:2107.5053711\tKL Div:8.9567671\n",
      "Epoch 627\tIter   12\tLoss:2099.3708496\tRec Loss:2090.3732910\tKL Div:8.9974957\n",
      "Epoch 627\tIter   18\tLoss:2161.8989258\tRec Loss:2153.0231934\tKL Div:8.8757648\n",
      "Epoch 628\tIter    6\tLoss:2116.6274414\tRec Loss:2107.4272461\tKL Div:9.2001190\n",
      "Epoch 628\tIter   12\tLoss:2135.1259766\tRec Loss:2126.1577148\tKL Div:8.9681625\n",
      "Epoch 628\tIter   18\tLoss:2138.8913574\tRec Loss:2129.8320312\tKL Div:9.0592089\n",
      "Epoch 629\tIter    6\tLoss:2122.8601074\tRec Loss:2113.8706055\tKL Div:8.9894333\n",
      "Epoch 629\tIter   12\tLoss:2128.7556152\tRec Loss:2119.8486328\tKL Div:8.9070873\n",
      "Epoch 629\tIter   18\tLoss:2103.0085449\tRec Loss:2094.0502930\tKL Div:8.9583416\n",
      "Epoch 630\tIter    6\tLoss:2108.8979492\tRec Loss:2099.8374023\tKL Div:9.0606041\n",
      "Epoch 630\tIter   12\tLoss:2149.2612305\tRec Loss:2140.4028320\tKL Div:8.8584204\n",
      "Epoch 630\tIter   18\tLoss:2135.8764648\tRec Loss:2126.7636719\tKL Div:9.1126776\n",
      "Epoch 631\tIter    6\tLoss:2158.1633301\tRec Loss:2149.1713867\tKL Div:8.9918823\n",
      "Epoch 631\tIter   12\tLoss:2152.7407227\tRec Loss:2143.6818848\tKL Div:9.0588589\n",
      "Epoch 631\tIter   18\tLoss:2126.8457031\tRec Loss:2117.8395996\tKL Div:9.0060215\n",
      "Epoch 632\tIter    6\tLoss:2121.5556641\tRec Loss:2112.5527344\tKL Div:9.0028954\n",
      "Epoch 632\tIter   12\tLoss:2131.9028320\tRec Loss:2122.9257812\tKL Div:8.9771509\n",
      "Epoch 632\tIter   18\tLoss:2127.8037109\tRec Loss:2119.0253906\tKL Div:8.7782497\n",
      "Epoch 633\tIter    6\tLoss:2130.6799316\tRec Loss:2121.7790527\tKL Div:8.9009848\n",
      "Epoch 633\tIter   12\tLoss:2129.4304199\tRec Loss:2120.4499512\tKL Div:8.9803743\n",
      "Epoch 633\tIter   18\tLoss:2137.6010742\tRec Loss:2128.6162109\tKL Div:8.9848337\n",
      "Epoch 634\tIter    6\tLoss:2124.9040527\tRec Loss:2115.9040527\tKL Div:8.9998817\n",
      "Epoch 634\tIter   12\tLoss:2113.8525391\tRec Loss:2105.0158691\tKL Div:8.8367214\n",
      "Epoch 634\tIter   18\tLoss:2147.6054688\tRec Loss:2138.5327148\tKL Div:9.0728292\n",
      "Epoch 635\tIter    6\tLoss:2125.3457031\tRec Loss:2116.3535156\tKL Div:8.9920654\n",
      "Epoch 635\tIter   12\tLoss:2129.1093750\tRec Loss:2120.1906738\tKL Div:8.9185810\n",
      "Epoch 635\tIter   18\tLoss:2111.9370117\tRec Loss:2102.9020996\tKL Div:9.0350208\n",
      "Epoch 636\tIter    6\tLoss:2118.7492676\tRec Loss:2109.8554688\tKL Div:8.8938751\n",
      "Epoch 636\tIter   12\tLoss:2118.3356934\tRec Loss:2109.3896484\tKL Div:8.9459858\n",
      "Epoch 636\tIter   18\tLoss:2149.0258789\tRec Loss:2139.9545898\tKL Div:9.0711746\n",
      "Epoch 637\tIter    6\tLoss:2129.6269531\tRec Loss:2120.6367188\tKL Div:8.9902573\n",
      "Epoch 637\tIter   12\tLoss:2123.0854492\tRec Loss:2114.0546875\tKL Div:9.0307665\n",
      "Epoch 637\tIter   18\tLoss:2122.0639648\tRec Loss:2113.1259766\tKL Div:8.9379101\n",
      "Epoch 638\tIter    6\tLoss:2107.9765625\tRec Loss:2098.9433594\tKL Div:9.0332737\n",
      "Epoch 638\tIter   12\tLoss:2136.9362793\tRec Loss:2127.8864746\tKL Div:9.0497494\n",
      "Epoch 638\tIter   18\tLoss:2136.7480469\tRec Loss:2127.7717285\tKL Div:8.9764347\n",
      "Epoch 639\tIter    6\tLoss:2129.8481445\tRec Loss:2120.8994141\tKL Div:8.9486971\n",
      "Epoch 639\tIter   12\tLoss:2122.2373047\tRec Loss:2113.3286133\tKL Div:8.9087238\n",
      "Epoch 639\tIter   18\tLoss:2124.9243164\tRec Loss:2116.0256348\tKL Div:8.8987331\n",
      "Epoch 640\tIter    6\tLoss:2085.9157715\tRec Loss:2076.9707031\tKL Div:8.9449825\n",
      "Epoch 640\tIter   12\tLoss:2158.9382324\tRec Loss:2149.9384766\tKL Div:8.9998283\n",
      "Epoch 640\tIter   18\tLoss:2111.6325684\tRec Loss:2102.7937012\tKL Div:8.8387699\n",
      "Epoch 640\tLoss:2124.1364246\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'tone', 'piece', 'space', 'sense', 'build', 'melody', 'drum', 'close', 'ambient', 'light', 'bass', 'note', 'electronic']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'piano', 'debut', 'harmony', 'group', 'chorus', 'line', 'string', 'bit', 'write', 'pretty']\n",
      "['version', 'live', 'disc', 'set', 'record', 'include', 'studio', 'soul', 'recording', 'cover', 'label', 'group', 'compilation', 'original', 'early']\n",
      "['lyric', 'guy', 'man', 'bad', 'listen', 'kid', 'get', 'day', 'love', 'people', 'let', '\\n', 'pretty', 'girl', 'line']\n",
      "['house', 'techno', 'mix', 'label', 'dance', 'producer', 'beat', 'bass', 'dj', 'artist', 'synth', 'drum', 'rhythm', 'club', 'set']\n",
      "['dance', 'disco', 'synth', 'house', 'single', 'love', 'group', 'big', 'duo', 'remix', 'indie', 'electro', 'club', 'turn', 'act']\n",
      "['\\xa0', 'project', 'point', 'artist', 'year', 'title', 'solo', 'early', 'ep', 'synth', 'place', 'close', 'producer', 'start', 'sense']\n",
      "['melody', 'riff', 'debut', 'drum', 'band', 'chorus', 'hook', 'punk', 'group', 'line', 'bass', 'ep', 'rhythm', 'opener', 'hard']\n",
      "['love', 'sing', 'life', 'lyric', 'world', 'line', 'word', 'write', 'feeling', 'leave', 'close', 'death', 'light', 'sense', 'relationship']\n",
      "['punk', 'live', 'single', 'love', 'cover', 'band', 'early', 'solo', 'fan', 'day', 'group', 'big', 'classic', 'smith', 'pollard']\n",
      "['drum', 'jazz', 'melody', 'electronic', 'noise', 'bass', 'beat', 'begin', 'piece', 'interesting', 'disc', 'instrumental', 'live', 'group', 'tune']\n",
      "['life', 'man', 'people', 'world', 'black', 'write', 'love', 'woman', 'political', 'story', 'word', 'white', 'call', 'live', 'year']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'dylan', 'lyric', 'acoustic', 'record', 'artist', 'oldham', 'american', 'man', 'love']\n",
      "['beat', 'hip_hop', 'sample', 'funk', 'dance', 'party', 'mix', 'people', 'big', 'bad', 'pretty', 'get', 'fun', 'genre', 'break']\n",
      "['piece', 'composer', 'piano', 'film', 'jazz', 'world', 'composition', 'musician', 'electronic', 'instrument', 'recording', 'score', 'create', 'note', 'group']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'verse', 'production', 'year', 'producer', 'flow', 'style', 'line', 'feature', 'big', 'artist']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'debut', 'sense', 'artist', 'instrumental', 'project', 'kind', 'production', 'drum', 'close']\n",
      "['lyric', 'title', 'kind', 'band', 'people', 'love', 'indie_rock', 'point', 'life', 'sort', 'punk', 'emo', 'indie', 'chorus', 'hook']\n",
      "['metal', 'riff', 'hardcore', 'punk', 'black_metal', 'band', 'noise', 'death', 'drum', 'heavy', 'drummer', 'doom', 'group', 'early', 'year']\n",
      "['love', 'r&b', 'artist', 'singer', 'star', 'hit', 'single', 'year', 'producer', 'prince', 'debut', 'girl', 'drake', 'big', 'woman']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.52\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "tone\n",
      "piece\n",
      "space\n",
      "sense\n",
      "build\n",
      "melody\n",
      "drum\n",
      "close\n",
      "ambient\n",
      "light\n",
      "bass\n",
      "note\n",
      "electronic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence c_uci -0.108167789948454\n",
      "coherence c_npmi 0.00954057066673526\n",
      "coherence c_cv 0.38520414806360004\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 641\tIter    6\tLoss:2177.5598145\tRec Loss:2168.5019531\tKL Div:9.0578785\n",
      "Epoch 641\tIter   12\tLoss:2158.5634766\tRec Loss:2149.3798828\tKL Div:9.1835747\n",
      "Epoch 641\tIter   18\tLoss:2181.0422363\tRec Loss:2172.1948242\tKL Div:8.8474369\n",
      "Epoch 642\tIter    6\tLoss:2121.5439453\tRec Loss:2112.5834961\tKL Div:8.9605522\n",
      "Epoch 642\tIter   12\tLoss:2154.5317383\tRec Loss:2145.5776367\tKL Div:8.9541559\n",
      "Epoch 642\tIter   18\tLoss:2123.0170898\tRec Loss:2113.9648438\tKL Div:9.0522823\n",
      "Epoch 643\tIter    6\tLoss:2108.4694824\tRec Loss:2099.5554199\tKL Div:8.9140282\n",
      "Epoch 643\tIter   12\tLoss:2145.2309570\tRec Loss:2136.1665039\tKL Div:9.0645170\n",
      "Epoch 643\tIter   18\tLoss:2142.2990723\tRec Loss:2133.3203125\tKL Div:8.9787951\n",
      "Epoch 644\tIter    6\tLoss:2159.4536133\tRec Loss:2150.4028320\tKL Div:9.0508499\n",
      "Epoch 644\tIter   12\tLoss:2136.7512207\tRec Loss:2127.8789062\tKL Div:8.8722382\n",
      "Epoch 644\tIter   18\tLoss:2142.5151367\tRec Loss:2133.5141602\tKL Div:9.0010824\n",
      "Epoch 645\tIter    6\tLoss:2131.1237793\tRec Loss:2122.0239258\tKL Div:9.0998402\n",
      "Epoch 645\tIter   12\tLoss:2132.4885254\tRec Loss:2123.5251465\tKL Div:8.9633026\n",
      "Epoch 645\tIter   18\tLoss:2172.1215820\tRec Loss:2163.0815430\tKL Div:9.0399866\n",
      "Epoch 646\tIter    6\tLoss:2094.9460449\tRec Loss:2086.0234375\tKL Div:8.9225674\n",
      "Epoch 646\tIter   12\tLoss:2112.8571777\tRec Loss:2103.9978027\tKL Div:8.8594513\n",
      "Epoch 646\tIter   18\tLoss:2121.0571289\tRec Loss:2112.2094727\tKL Div:8.8477230\n",
      "Epoch 647\tIter    6\tLoss:2084.0339355\tRec Loss:2074.9980469\tKL Div:9.0359631\n",
      "Epoch 647\tIter   12\tLoss:2135.4306641\tRec Loss:2126.5507812\tKL Div:8.8798752\n",
      "Epoch 647\tIter   18\tLoss:2099.4389648\tRec Loss:2090.5314941\tKL Div:8.9075584\n",
      "Epoch 648\tIter    6\tLoss:2142.5163574\tRec Loss:2133.5185547\tKL Div:8.9979191\n",
      "Epoch 648\tIter   12\tLoss:2115.1191406\tRec Loss:2106.2250977\tKL Div:8.8940258\n",
      "Epoch 648\tIter   18\tLoss:2094.9414062\tRec Loss:2085.7924805\tKL Div:9.1489420\n",
      "Epoch 649\tIter    6\tLoss:2158.4345703\tRec Loss:2149.5766602\tKL Div:8.8579721\n",
      "Epoch 649\tIter   12\tLoss:2135.2788086\tRec Loss:2126.3164062\tKL Div:8.9623699\n",
      "Epoch 649\tIter   18\tLoss:2131.7644043\tRec Loss:2122.8344727\tKL Div:8.9300060\n",
      "Epoch 650\tIter    6\tLoss:2126.6242676\tRec Loss:2117.7592773\tKL Div:8.8649387\n",
      "Epoch 650\tIter   12\tLoss:2145.5839844\tRec Loss:2136.5136719\tKL Div:9.0702686\n",
      "Epoch 650\tIter   18\tLoss:2092.8383789\tRec Loss:2083.9702148\tKL Div:8.8681831\n",
      "Epoch 651\tIter    6\tLoss:2132.5749512\tRec Loss:2123.3681641\tKL Div:9.2067442\n",
      "Epoch 651\tIter   12\tLoss:2082.2763672\tRec Loss:2073.3657227\tKL Div:8.9105387\n",
      "Epoch 651\tIter   18\tLoss:2138.4562988\tRec Loss:2129.4897461\tKL Div:8.9664707\n",
      "Epoch 652\tIter    6\tLoss:2155.7661133\tRec Loss:2146.6589355\tKL Div:9.1072884\n",
      "Epoch 652\tIter   12\tLoss:2127.9431152\tRec Loss:2119.0571289\tKL Div:8.8859863\n",
      "Epoch 652\tIter   18\tLoss:2098.0893555\tRec Loss:2089.1467285\tKL Div:8.9426374\n",
      "Epoch 653\tIter    6\tLoss:2126.8242188\tRec Loss:2117.7360840\tKL Div:9.0881882\n",
      "Epoch 653\tIter   12\tLoss:2102.7998047\tRec Loss:2094.0122070\tKL Div:8.7876654\n",
      "Epoch 653\tIter   18\tLoss:2119.6911621\tRec Loss:2110.8366699\tKL Div:8.8545399\n",
      "Epoch 654\tIter    6\tLoss:2118.2844238\tRec Loss:2109.1884766\tKL Div:9.0959911\n",
      "Epoch 654\tIter   12\tLoss:2136.1955566\tRec Loss:2127.2255859\tKL Div:8.9699850\n",
      "Epoch 654\tIter   18\tLoss:2103.1262207\tRec Loss:2094.1772461\tKL Div:8.9490061\n",
      "Epoch 655\tIter    6\tLoss:2155.2734375\tRec Loss:2146.2253418\tKL Div:9.0480595\n",
      "Epoch 655\tIter   12\tLoss:2097.0708008\tRec Loss:2088.1745605\tKL Div:8.8962164\n",
      "Epoch 655\tIter   18\tLoss:2106.9226074\tRec Loss:2098.0341797\tKL Div:8.8884830\n",
      "Epoch 656\tIter    6\tLoss:2119.9584961\tRec Loss:2110.9311523\tKL Div:9.0274620\n",
      "Epoch 656\tIter   12\tLoss:2124.2429199\tRec Loss:2115.3178711\tKL Div:8.9249859\n",
      "Epoch 656\tIter   18\tLoss:2174.6953125\tRec Loss:2165.6997070\tKL Div:8.9956017\n",
      "Epoch 657\tIter    6\tLoss:2121.2050781\tRec Loss:2112.1879883\tKL Div:9.0171709\n",
      "Epoch 657\tIter   12\tLoss:2101.3459473\tRec Loss:2092.2949219\tKL Div:9.0511112\n",
      "Epoch 657\tIter   18\tLoss:2126.5270996\tRec Loss:2117.5166016\tKL Div:9.0104704\n",
      "Epoch 658\tIter    6\tLoss:2131.6428223\tRec Loss:2122.6870117\tKL Div:8.9557543\n",
      "Epoch 658\tIter   12\tLoss:2140.8200684\tRec Loss:2131.7404785\tKL Div:9.0796318\n",
      "Epoch 658\tIter   18\tLoss:2075.0808105\tRec Loss:2066.2702637\tKL Div:8.8104391\n",
      "Epoch 659\tIter    6\tLoss:2138.9782715\tRec Loss:2129.9968262\tKL Div:8.9814568\n",
      "Epoch 659\tIter   12\tLoss:2127.9262695\tRec Loss:2118.8066406\tKL Div:9.1195498\n",
      "Epoch 659\tIter   18\tLoss:2082.3967285\tRec Loss:2073.6105957\tKL Div:8.7861214\n",
      "Epoch 660\tIter    6\tLoss:2146.1472168\tRec Loss:2137.1062012\tKL Div:9.0410614\n",
      "Epoch 660\tIter   12\tLoss:2121.1506348\tRec Loss:2112.2587891\tKL Div:8.8918180\n",
      "Epoch 660\tIter   18\tLoss:2102.2929688\tRec Loss:2093.3330078\tKL Div:8.9600697\n",
      "Epoch 661\tIter    6\tLoss:2108.3186035\tRec Loss:2099.3063965\tKL Div:9.0121765\n",
      "Epoch 661\tIter   12\tLoss:2128.6721191\tRec Loss:2119.7795410\tKL Div:8.8926764\n",
      "Epoch 661\tIter   18\tLoss:2160.4265137\tRec Loss:2151.3842773\tKL Div:9.0422783\n",
      "Epoch 662\tIter    6\tLoss:2113.4106445\tRec Loss:2104.3259277\tKL Div:9.0846128\n",
      "Epoch 662\tIter   12\tLoss:2121.6271973\tRec Loss:2112.8686523\tKL Div:8.7585621\n",
      "Epoch 662\tIter   18\tLoss:2122.2070312\tRec Loss:2113.2326660\tKL Div:8.9744663\n",
      "Epoch 663\tIter    6\tLoss:2130.2072754\tRec Loss:2121.1562500\tKL Div:9.0509272\n",
      "Epoch 663\tIter   12\tLoss:2120.5004883\tRec Loss:2111.6274414\tKL Div:8.8729458\n",
      "Epoch 663\tIter   18\tLoss:2140.2507324\tRec Loss:2131.1853027\tKL Div:9.0654602\n",
      "Epoch 664\tIter    6\tLoss:2114.7536621\tRec Loss:2105.7143555\tKL Div:9.0393782\n",
      "Epoch 664\tIter   12\tLoss:2130.7578125\tRec Loss:2121.9350586\tKL Div:8.8226452\n",
      "Epoch 664\tIter   18\tLoss:2090.1801758\tRec Loss:2081.0917969\tKL Div:9.0883484\n",
      "Epoch 665\tIter    6\tLoss:2129.0263672\tRec Loss:2120.0864258\tKL Div:8.9399977\n",
      "Epoch 665\tIter   12\tLoss:2130.5461426\tRec Loss:2121.4916992\tKL Div:9.0544691\n",
      "Epoch 665\tIter   18\tLoss:2113.0270996\tRec Loss:2104.2299805\tKL Div:8.7970238\n",
      "Epoch 666\tIter    6\tLoss:2092.0395508\tRec Loss:2083.1672363\tKL Div:8.8722305\n",
      "Epoch 666\tIter   12\tLoss:2147.5532227\tRec Loss:2138.4726562\tKL Div:9.0804539\n",
      "Epoch 666\tIter   18\tLoss:2137.7497559\tRec Loss:2128.6381836\tKL Div:9.1115370\n",
      "Epoch 667\tIter    6\tLoss:2097.3774414\tRec Loss:2088.4130859\tKL Div:8.9644670\n",
      "Epoch 667\tIter   12\tLoss:2121.5925293\tRec Loss:2112.7646484\tKL Div:8.8277607\n",
      "Epoch 667\tIter   18\tLoss:2106.0754395\tRec Loss:2097.3012695\tKL Div:8.7741261\n",
      "Epoch 668\tIter    6\tLoss:2097.0446777\tRec Loss:2088.0478516\tKL Div:8.9968815\n",
      "Epoch 668\tIter   12\tLoss:2140.1948242\tRec Loss:2131.3017578\tKL Div:8.8931732\n",
      "Epoch 668\tIter   18\tLoss:2155.7133789\tRec Loss:2146.5979004\tKL Div:9.1154442\n",
      "Epoch 669\tIter    6\tLoss:2091.0100098\tRec Loss:2082.0307617\tKL Div:8.9793272\n",
      "Epoch 669\tIter   12\tLoss:2117.4370117\tRec Loss:2108.6166992\tKL Div:8.8203154\n",
      "Epoch 669\tIter   18\tLoss:2132.9082031\tRec Loss:2123.8566895\tKL Div:9.0514965\n",
      "Epoch 670\tIter    6\tLoss:2094.9545898\tRec Loss:2085.9907227\tKL Div:8.9637871\n",
      "Epoch 670\tIter   12\tLoss:2106.5903320\tRec Loss:2097.5595703\tKL Div:9.0308037\n",
      "Epoch 670\tIter   18\tLoss:2136.6889648\tRec Loss:2127.7590332\tKL Div:8.9299574\n",
      "Epoch 671\tIter    6\tLoss:2132.5109863\tRec Loss:2123.4797363\tKL Div:9.0312672\n",
      "Epoch 671\tIter   12\tLoss:2070.3327637\tRec Loss:2061.4777832\tKL Div:8.8549995\n",
      "Epoch 671\tIter   18\tLoss:2124.6223145\tRec Loss:2115.5532227\tKL Div:9.0689716\n",
      "Epoch 672\tIter    6\tLoss:2088.1091309\tRec Loss:2079.1135254\tKL Div:8.9957027\n",
      "Epoch 672\tIter   12\tLoss:2121.6318359\tRec Loss:2112.6757812\tKL Div:8.9560833\n",
      "Epoch 672\tIter   18\tLoss:2164.1948242\tRec Loss:2155.1894531\tKL Div:9.0054874\n",
      "Epoch 673\tIter    6\tLoss:2096.5246582\tRec Loss:2087.5878906\tKL Div:8.9368353\n",
      "Epoch 673\tIter   12\tLoss:2109.5895996\tRec Loss:2100.7475586\tKL Div:8.8419476\n",
      "Epoch 673\tIter   18\tLoss:2165.5891113\tRec Loss:2156.5222168\tKL Div:9.0669041\n",
      "Epoch 674\tIter    6\tLoss:2126.9089355\tRec Loss:2117.9919434\tKL Div:8.9170456\n",
      "Epoch 674\tIter   12\tLoss:2122.9482422\tRec Loss:2113.8720703\tKL Div:9.0762691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 674\tIter   18\tLoss:2136.8071289\tRec Loss:2128.0683594\tKL Div:8.7387667\n",
      "Epoch 675\tIter    6\tLoss:2112.4025879\tRec Loss:2103.2006836\tKL Div:9.2018747\n",
      "Epoch 675\tIter   12\tLoss:2127.8400879\tRec Loss:2119.0507812\tKL Div:8.7894115\n",
      "Epoch 675\tIter   18\tLoss:2116.8715820\tRec Loss:2107.7827148\tKL Div:9.0889778\n",
      "Epoch 676\tIter    6\tLoss:2149.3195801\tRec Loss:2140.3149414\tKL Div:9.0046206\n",
      "Epoch 676\tIter   12\tLoss:2134.8425293\tRec Loss:2125.9560547\tKL Div:8.8865595\n",
      "Epoch 676\tIter   18\tLoss:2123.9230957\tRec Loss:2114.9265137\tKL Div:8.9965973\n",
      "Epoch 677\tIter    6\tLoss:2137.6660156\tRec Loss:2128.6860352\tKL Div:8.9800596\n",
      "Epoch 677\tIter   12\tLoss:2123.0649414\tRec Loss:2114.1718750\tKL Div:8.8931160\n",
      "Epoch 677\tIter   18\tLoss:2165.7294922\tRec Loss:2156.7285156\tKL Div:9.0009556\n",
      "Epoch 678\tIter    6\tLoss:2168.8977051\tRec Loss:2159.8808594\tKL Div:9.0168972\n",
      "Epoch 678\tIter   12\tLoss:2090.4353027\tRec Loss:2081.5009766\tKL Div:8.9343081\n",
      "Epoch 678\tIter   18\tLoss:2116.2814941\tRec Loss:2107.3901367\tKL Div:8.8914204\n",
      "Epoch 679\tIter    6\tLoss:2080.0148926\tRec Loss:2071.1391602\tKL Div:8.8756199\n",
      "Epoch 679\tIter   12\tLoss:2120.3115234\tRec Loss:2111.3811035\tKL Div:8.9304829\n",
      "Epoch 679\tIter   18\tLoss:2162.8312988\tRec Loss:2153.9423828\tKL Div:8.8889351\n",
      "Epoch 680\tIter    6\tLoss:2097.7592773\tRec Loss:2088.8173828\tKL Div:8.9419985\n",
      "Epoch 680\tIter   12\tLoss:2123.8710938\tRec Loss:2114.9782715\tKL Div:8.8927364\n",
      "Epoch 680\tIter   18\tLoss:2119.8657227\tRec Loss:2110.8232422\tKL Div:9.0425749\n",
      "Epoch 680\tLoss:2126.6056376\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'tone', 'piece', 'sense', 'build', 'space', 'close', 'drum', 'melody', 'ambient', 'light', 'note', 'open', 'bass']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'debut', 'harmony', 'group', 'piano', 'chorus', 'bit', 'write', 'string', 'line', 'title']\n",
      "['version', 'live', 'disc', 'set', 'include', 'record', 'cover', 'soul', 'studio', 'label', 'recording', 'original', 'compilation', 'group', 'early']\n",
      "['lyric', 'man', 'bad', 'guy', 'listen', 'day', 'get', 'kid', 'people', 'love', 'let', 'girl', '\\n', 'line', 'pretty']\n",
      "['house', 'label', 'techno', 'mix', 'dance', 'producer', 'bass', 'beat', 'dj', 'artist', 'synth', 'set', 'drum', 'rhythm', 'club']\n",
      "['dance', 'disco', 'synth', 'single', 'love', 'house', 'group', 'big', 'remix', 'duo', 'indie', 'electro', 'club', 'turn', 'act']\n",
      "['\\xa0', 'project', 'point', 'artist', 'title', 'year', 'synth', 'solo', 'early', 'ep', 'close', 'place', 'producer', 'feature', 'start']\n",
      "['melody', 'debut', 'riff', 'chorus', 'drum', 'band', 'hook', 'line', 'group', 'punk', 'bass', 'ep', 'opener', 'rhythm', 'hard']\n",
      "['sing', 'love', 'life', 'lyric', 'line', 'world', 'word', 'write', 'leave', 'close', 'feeling', 'light', 'death', 'sense', 'kind']\n",
      "['punk', 'live', 'single', 'love', 'cover', 'band', 'solo', 'early', 'fan', 'big', 'day', 'group', 'classic', 'set', 'smith']\n",
      "['drum', 'electronic', 'melody', 'jazz', 'noise', 'bass', 'beat', 'piece', 'begin', 'group', 'disc', 'interesting', 'feature', 'bit', 'live']\n",
      "['life', 'man', 'world', 'people', 'black', 'woman', 'love', 'write', 'political', 'word', 'call', 'story', 'live', 'year', 'white']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'dylan', 'lyric', 'acoustic', 'artist', 'record', 'man', 'american', 'love', 'oldham']\n",
      "['beat', 'hip_hop', 'sample', 'funk', 'party', 'mix', 'dance', 'people', 'big', 'break', 'genre', 'dj', 'bad', 'get', 'pretty']\n",
      "['piece', 'film', 'piano', 'jazz', 'composer', 'world', 'composition', 'musician', 'instrument', 'electronic', 'recording', 'group', 'create', 'score', 'string']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'verse', 'production', 'year', 'producer', 'flow', 'style', 'feature', 'line', 'big', 'artist']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'sense', 'debut', 'artist', 'instrumental', 'production', 'kind', 'project', 'drum', 'build']\n",
      "['lyric', 'kind', 'title', 'people', 'band', 'love', 'indie_rock', 'point', 'emo', 'life', 'sort', 'indie', 'chorus', 'big', 'punk']\n",
      "['metal', 'riff', 'hardcore', 'punk', 'black_metal', 'band', 'noise', 'death', 'drum', 'heavy', 'doom', 'close', 'black', 'group', 'year']\n",
      "['love', 'r&b', 'artist', 'hit', 'single', 'star', 'singer', 'year', 'prince', 'producer', 'debut', 'girl', 'soul', 'big', 'drake']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5066666666666667\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "tone\n",
      "piece\n",
      "sense\n",
      "build\n",
      "space\n",
      "close\n",
      "drum\n",
      "melody\n",
      "ambient\n",
      "light\n",
      "note\n",
      "open\n",
      "bass\n",
      "coherence c_uci -0.013403339524305188\n",
      "coherence c_npmi 0.012815646107170092\n",
      "coherence c_cv 0.38479426405468714\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,    99,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 681\tIter    6\tLoss:2092.1237793\tRec Loss:2083.0854492\tKL Div:9.0383625\n",
      "Epoch 681\tIter   12\tLoss:2148.2814941\tRec Loss:2139.1982422\tKL Div:9.0831432\n",
      "Epoch 681\tIter   18\tLoss:2102.7641602\tRec Loss:2093.7873535\tKL Div:8.9769154\n",
      "Epoch 682\tIter    6\tLoss:2082.8352051\tRec Loss:2074.0385742\tKL Div:8.7965155\n",
      "Epoch 682\tIter   12\tLoss:2143.9780273\tRec Loss:2134.8059082\tKL Div:9.1721420\n",
      "Epoch 682\tIter   18\tLoss:2085.6499023\tRec Loss:2076.8852539\tKL Div:8.7645721\n",
      "Epoch 683\tIter    6\tLoss:2137.6403809\tRec Loss:2128.4655762\tKL Div:9.1747551\n",
      "Epoch 683\tIter   12\tLoss:2116.3796387\tRec Loss:2107.6235352\tKL Div:8.7560682\n",
      "Epoch 683\tIter   18\tLoss:2132.6875000\tRec Loss:2123.5305176\tKL Div:9.1570120\n",
      "Epoch 684\tIter    6\tLoss:2153.0258789\tRec Loss:2144.0136719\tKL Div:9.0121584\n",
      "Epoch 684\tIter   12\tLoss:2061.1901855\tRec Loss:2052.4062500\tKL Div:8.7839336\n",
      "Epoch 684\tIter   18\tLoss:2108.4553223\tRec Loss:2099.3320312\tKL Div:9.1233368\n",
      "Epoch 685\tIter    6\tLoss:2121.1787109\tRec Loss:2112.2832031\tKL Div:8.8955097\n",
      "Epoch 685\tIter   12\tLoss:2152.2526855\tRec Loss:2143.1579590\tKL Div:9.0947151\n",
      "Epoch 685\tIter   18\tLoss:2158.7922363\tRec Loss:2149.8317871\tKL Div:8.9604950\n",
      "Epoch 686\tIter    6\tLoss:2140.7795410\tRec Loss:2131.7009277\tKL Div:9.0785732\n",
      "Epoch 686\tIter   12\tLoss:2143.0695801\tRec Loss:2134.2172852\tKL Div:8.8523369\n",
      "Epoch 686\tIter   18\tLoss:2162.7253418\tRec Loss:2153.6665039\tKL Div:9.0588799\n",
      "Epoch 687\tIter    6\tLoss:2093.9584961\tRec Loss:2084.8911133\tKL Div:9.0673590\n",
      "Epoch 687\tIter   12\tLoss:2131.6770020\tRec Loss:2122.7221680\tKL Div:8.9548559\n",
      "Epoch 687\tIter   18\tLoss:2145.8439941\tRec Loss:2136.8437500\tKL Div:9.0001621\n",
      "Epoch 688\tIter    6\tLoss:2116.3488770\tRec Loss:2107.4665527\tKL Div:8.8822575\n",
      "Epoch 688\tIter   12\tLoss:2142.3666992\tRec Loss:2133.1472168\tKL Div:9.2194653\n",
      "Epoch 688\tIter   18\tLoss:2122.1479492\tRec Loss:2113.3750000\tKL Div:8.7730360\n",
      "Epoch 689\tIter    6\tLoss:2160.1870117\tRec Loss:2151.0786133\tKL Div:9.1084394\n",
      "Epoch 689\tIter   12\tLoss:2108.4702148\tRec Loss:2099.5717773\tKL Div:8.8984623\n",
      "Epoch 689\tIter   18\tLoss:2124.7646484\tRec Loss:2115.7963867\tKL Div:8.9682827\n",
      "Epoch 690\tIter    6\tLoss:2137.5112305\tRec Loss:2128.3056641\tKL Div:9.2055817\n",
      "Epoch 690\tIter   12\tLoss:2133.7675781\tRec Loss:2124.8803711\tKL Div:8.8871775\n",
      "Epoch 690\tIter   18\tLoss:2132.8103027\tRec Loss:2123.6486816\tKL Div:9.1615372\n",
      "Epoch 691\tIter    6\tLoss:2165.0141602\tRec Loss:2155.9296875\tKL Div:9.0845490\n",
      "Epoch 691\tIter   12\tLoss:2137.1096191\tRec Loss:2128.1489258\tKL Div:8.9607172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 691\tIter   18\tLoss:2105.2534180\tRec Loss:2096.3276367\tKL Div:8.9258146\n",
      "Epoch 692\tIter    6\tLoss:2082.8366699\tRec Loss:2073.9118652\tKL Div:8.9249182\n",
      "Epoch 692\tIter   12\tLoss:2118.2668457\tRec Loss:2109.3322754\tKL Div:8.9345627\n",
      "Epoch 692\tIter   18\tLoss:2147.8769531\tRec Loss:2138.9233398\tKL Div:8.9536285\n",
      "Epoch 693\tIter    6\tLoss:2166.8994141\tRec Loss:2157.7519531\tKL Div:9.1474428\n",
      "Epoch 693\tIter   12\tLoss:2086.5361328\tRec Loss:2077.7241211\tKL Div:8.8118992\n",
      "Epoch 693\tIter   18\tLoss:2124.5246582\tRec Loss:2115.5388184\tKL Div:8.9858704\n",
      "Epoch 694\tIter    6\tLoss:2143.5241699\tRec Loss:2134.3833008\tKL Div:9.1408119\n",
      "Epoch 694\tIter   12\tLoss:2131.4023438\tRec Loss:2122.3430176\tKL Div:9.0592623\n",
      "Epoch 694\tIter   18\tLoss:2083.8745117\tRec Loss:2075.0622559\tKL Div:8.8123007\n",
      "Epoch 695\tIter    6\tLoss:2141.5126953\tRec Loss:2132.3447266\tKL Div:9.1679916\n",
      "Epoch 695\tIter   12\tLoss:2126.2158203\tRec Loss:2117.4218750\tKL Div:8.7939301\n",
      "Epoch 695\tIter   18\tLoss:2110.8103027\tRec Loss:2101.7995605\tKL Div:9.0107279\n",
      "Epoch 696\tIter    6\tLoss:2144.1882324\tRec Loss:2135.2097168\tKL Div:8.9784470\n",
      "Epoch 696\tIter   12\tLoss:2105.4560547\tRec Loss:2096.6005859\tKL Div:8.8553524\n",
      "Epoch 696\tIter   18\tLoss:2072.7573242\tRec Loss:2063.8103027\tKL Div:8.9469995\n",
      "Epoch 697\tIter    6\tLoss:2129.5268555\tRec Loss:2120.5566406\tKL Div:8.9703255\n",
      "Epoch 697\tIter   12\tLoss:2162.2819824\tRec Loss:2153.3508301\tKL Div:8.9311752\n",
      "Epoch 697\tIter   18\tLoss:2115.7431641\tRec Loss:2106.8576660\tKL Div:8.8855782\n",
      "Epoch 698\tIter    6\tLoss:2119.3068848\tRec Loss:2110.2041016\tKL Div:9.1027145\n",
      "Epoch 698\tIter   12\tLoss:2130.7761230\tRec Loss:2121.7368164\tKL Div:9.0394135\n",
      "Epoch 698\tIter   18\tLoss:2132.2854004\tRec Loss:2123.3395996\tKL Div:8.9457855\n",
      "Epoch 699\tIter    6\tLoss:2166.2629395\tRec Loss:2157.0996094\tKL Div:9.1633043\n",
      "Epoch 699\tIter   12\tLoss:2143.8059082\tRec Loss:2134.8735352\tKL Div:8.9323120\n",
      "Epoch 699\tIter   18\tLoss:2132.7775879\tRec Loss:2123.8259277\tKL Div:8.9515800\n",
      "Epoch 700\tIter    6\tLoss:2117.3603516\tRec Loss:2108.3374023\tKL Div:9.0230160\n",
      "Epoch 700\tIter   12\tLoss:2142.5346680\tRec Loss:2133.4458008\tKL Div:9.0888453\n",
      "Epoch 700\tIter   18\tLoss:2097.2949219\tRec Loss:2088.3474121\tKL Div:8.9475689\n",
      "Epoch 701\tIter    6\tLoss:2129.4047852\tRec Loss:2120.2714844\tKL Div:9.1332283\n",
      "Epoch 701\tIter   12\tLoss:2110.5209961\tRec Loss:2101.7285156\tKL Div:8.7925863\n",
      "Epoch 701\tIter   18\tLoss:2118.2636719\tRec Loss:2109.0979004\tKL Div:9.1656990\n",
      "Epoch 702\tIter    6\tLoss:2157.8916016\tRec Loss:2148.9116211\tKL Div:8.9799500\n",
      "Epoch 702\tIter   12\tLoss:2082.5339355\tRec Loss:2073.6494141\tKL Div:8.8844700\n",
      "Epoch 702\tIter   18\tLoss:2169.0336914\tRec Loss:2159.9973145\tKL Div:9.0362835\n",
      "Epoch 703\tIter    6\tLoss:2149.2524414\tRec Loss:2140.2485352\tKL Div:9.0038738\n",
      "Epoch 703\tIter   12\tLoss:2135.8239746\tRec Loss:2126.8134766\tKL Div:9.0103903\n",
      "Epoch 703\tIter   18\tLoss:2116.1774902\tRec Loss:2107.3107910\tKL Div:8.8666191\n",
      "Epoch 704\tIter    6\tLoss:2108.5898438\tRec Loss:2099.4514160\tKL Div:9.1383562\n",
      "Epoch 704\tIter   12\tLoss:2111.8325195\tRec Loss:2102.9213867\tKL Div:8.9111128\n",
      "Epoch 704\tIter   18\tLoss:2117.5412598\tRec Loss:2108.6005859\tKL Div:8.9405880\n",
      "Epoch 705\tIter    6\tLoss:2140.7646484\tRec Loss:2131.7795410\tKL Div:8.9850731\n",
      "Epoch 705\tIter   12\tLoss:2132.0859375\tRec Loss:2123.1145020\tKL Div:8.9715061\n",
      "Epoch 705\tIter   18\tLoss:2142.0827637\tRec Loss:2133.2285156\tKL Div:8.8543167\n",
      "Epoch 706\tIter    6\tLoss:2068.9843750\tRec Loss:2059.9335938\tKL Div:9.0506868\n",
      "Epoch 706\tIter   12\tLoss:2082.9035645\tRec Loss:2074.1569824\tKL Div:8.7465420\n",
      "Epoch 706\tIter   18\tLoss:2156.3496094\tRec Loss:2147.2814941\tKL Div:9.0681934\n",
      "Epoch 707\tIter    6\tLoss:2174.9147949\tRec Loss:2165.7824707\tKL Div:9.1324320\n",
      "Epoch 707\tIter   12\tLoss:2152.9140625\tRec Loss:2143.9335938\tKL Div:8.9804420\n",
      "Epoch 707\tIter   18\tLoss:2098.2910156\tRec Loss:2089.1777344\tKL Div:9.1133013\n",
      "Epoch 708\tIter    6\tLoss:2104.9729004\tRec Loss:2096.2478027\tKL Div:8.7250214\n",
      "Epoch 708\tIter   12\tLoss:2118.6660156\tRec Loss:2109.5620117\tKL Div:9.1040115\n",
      "Epoch 708\tIter   18\tLoss:2110.2143555\tRec Loss:2101.4387207\tKL Div:8.7756948\n",
      "Epoch 709\tIter    6\tLoss:2101.1403809\tRec Loss:2092.0507812\tKL Div:9.0895061\n",
      "Epoch 709\tIter   12\tLoss:2099.8139648\tRec Loss:2091.0600586\tKL Div:8.7539463\n",
      "Epoch 709\tIter   18\tLoss:2141.9711914\tRec Loss:2132.8447266\tKL Div:9.1265163\n",
      "Epoch 710\tIter    6\tLoss:2139.7145996\tRec Loss:2130.8388672\tKL Div:8.8757820\n",
      "Epoch 710\tIter   12\tLoss:2117.2756348\tRec Loss:2108.1799316\tKL Div:9.0956507\n",
      "Epoch 710\tIter   18\tLoss:2174.6320801\tRec Loss:2165.5864258\tKL Div:9.0457401\n",
      "Epoch 711\tIter    6\tLoss:2181.2348633\tRec Loss:2172.2163086\tKL Div:9.0185814\n",
      "Epoch 711\tIter   12\tLoss:2176.4577637\tRec Loss:2167.4404297\tKL Div:9.0173988\n",
      "Epoch 711\tIter   18\tLoss:2090.8032227\tRec Loss:2081.8840332\tKL Div:8.9192724\n",
      "Epoch 712\tIter    6\tLoss:2142.3745117\tRec Loss:2133.3144531\tKL Div:9.0601454\n",
      "Epoch 712\tIter   12\tLoss:2120.0686035\tRec Loss:2111.0400391\tKL Div:9.0285511\n",
      "Epoch 712\tIter   18\tLoss:2147.6430664\tRec Loss:2138.7060547\tKL Div:8.9369850\n",
      "Epoch 713\tIter    6\tLoss:2096.5983887\tRec Loss:2087.4633789\tKL Div:9.1350899\n",
      "Epoch 713\tIter   12\tLoss:2151.8864746\tRec Loss:2143.0561523\tKL Div:8.8302555\n",
      "Epoch 713\tIter   18\tLoss:2096.2302246\tRec Loss:2087.2543945\tKL Div:8.9757614\n",
      "Epoch 714\tIter    6\tLoss:2093.3759766\tRec Loss:2084.5136719\tKL Div:8.8622732\n",
      "Epoch 714\tIter   12\tLoss:2139.0112305\tRec Loss:2130.0686035\tKL Div:8.9427395\n",
      "Epoch 714\tIter   18\tLoss:2099.1376953\tRec Loss:2090.1596680\tKL Div:8.9779339\n",
      "Epoch 715\tIter    6\tLoss:2055.3757324\tRec Loss:2046.5776367\tKL Div:8.7980528\n",
      "Epoch 715\tIter   12\tLoss:2132.3044434\tRec Loss:2123.2785645\tKL Div:9.0259676\n",
      "Epoch 715\tIter   18\tLoss:2100.3225098\tRec Loss:2091.3413086\tKL Div:8.9812689\n",
      "Epoch 716\tIter    6\tLoss:2105.9316406\tRec Loss:2097.0390625\tKL Div:8.8925476\n",
      "Epoch 716\tIter   12\tLoss:2102.3872070\tRec Loss:2093.3056641\tKL Div:9.0816650\n",
      "Epoch 716\tIter   18\tLoss:2145.3317871\tRec Loss:2136.3315430\tKL Div:9.0002747\n",
      "Epoch 717\tIter    6\tLoss:2146.3120117\tRec Loss:2137.1953125\tKL Div:9.1167221\n",
      "Epoch 717\tIter   12\tLoss:2161.0568848\tRec Loss:2152.1101074\tKL Div:8.9466619\n",
      "Epoch 717\tIter   18\tLoss:2122.9895020\tRec Loss:2114.1088867\tKL Div:8.8805285\n",
      "Epoch 718\tIter    6\tLoss:2132.7775879\tRec Loss:2123.7812500\tKL Div:8.9962406\n",
      "Epoch 718\tIter   12\tLoss:2114.6333008\tRec Loss:2105.6528320\tKL Div:8.9805708\n",
      "Epoch 718\tIter   18\tLoss:2102.7941895\tRec Loss:2093.7514648\tKL Div:9.0427904\n",
      "Epoch 719\tIter    6\tLoss:2125.0810547\tRec Loss:2116.0102539\tKL Div:9.0707417\n",
      "Epoch 719\tIter   12\tLoss:2117.8945312\tRec Loss:2109.0795898\tKL Div:8.8149414\n",
      "Epoch 719\tIter   18\tLoss:2164.1555176\tRec Loss:2154.9384766\tKL Div:9.2171574\n",
      "Epoch 720\tIter    6\tLoss:2130.7111816\tRec Loss:2121.8999023\tKL Div:8.8111916\n",
      "Epoch 720\tIter   12\tLoss:2159.7416992\tRec Loss:2150.6289062\tKL Div:9.1128788\n",
      "Epoch 720\tIter   18\tLoss:2104.1767578\tRec Loss:2095.3071289\tKL Div:8.8695974\n",
      "Epoch 720\tLoss:2126.4962649\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   573,  1307,   131,\n",
      "          526,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'piece', 'tone', 'sense', 'space', 'build', 'melody', 'close', 'ambient', 'drum', 'piano', 'light', 'note', 'electronic']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'debut', 'group', 'harmony', 'piano', 'chorus', 'string', 'write', 'bit', 'line', 'pretty']\n",
      "['version', 'live', 'disc', 'set', 'include', 'record', 'studio', 'label', 'soul', 'recording', 'cover', 'compilation', 'original', 'group', 'early']\n",
      "['bad', 'guy', 'lyric', 'man', 'listen', 'day', 'kid', 'get', 'people', 'love', '\\n', 'let', 'girl', 'pretty', 'line']\n",
      "['house', 'techno', 'mix', 'label', 'dance', 'producer', 'bass', 'beat', 'dj', 'artist', 'synth', 'club', 'set', 'rhythm', 'drum']\n",
      "['dance', 'disco', 'synth', 'single', 'house', 'love', 'big', 'group', 'duo', 'remix', 'indie', 'electro', 'club', 'electronic', 'turn']\n",
      "['\\xa0', 'project', 'point', 'artist', 'year', 'title', 'solo', 'synth', 'early', 'ep', 'place', 'close', 'producer', 'sense', 'feature']\n",
      "['melody', 'debut', 'riff', 'drum', 'band', 'chorus', 'hook', 'line', 'group', 'punk', 'bass', 'ep', 'rhythm', 'opener', 'hard']\n",
      "['sing', 'love', 'life', 'lyric', 'world', 'line', 'word', 'write', 'leave', 'feeling', 'close', 'death', 'light', 'sense', 'story']\n",
      "['punk', 'live', 'single', 'love', 'cover', 'band', 'early', 'solo', 'day', 'big', 'fan', 'set', 'group', 'classic', 'pollard']\n",
      "['electronic', 'drum', 'melody', 'jazz', 'bass', 'noise', 'beat', 'piece', 'interesting', 'begin', 'disc', 'feature', 'live', 'group', 'bit']\n",
      "['life', 'man', 'world', 'people', 'black', 'write', 'woman', 'love', 'political', 'word', 'story', 'call', 'white', 'live', 'year']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'artist', 'dylan', 'acoustic', 'lyric', 'record', 'man', 'american', 'oldham', 'write']\n",
      "['hip_hop', 'beat', 'funk', 'sample', 'mix', 'party', 'dance', 'people', 'genre', 'big', 'break', 'dj', 'bad', 'bass', 'man']\n",
      "['piece', 'piano', 'jazz', 'composer', 'film', 'world', 'composition', 'musician', 'instrument', 'recording', 'electronic', 'string', 'create', 'group', 'note']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'verse', 'production', 'year', 'producer', 'flow', 'style', 'feature', 'line', 'big', 'artist']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'debut', 'sense', 'artist', 'production', 'instrumental', 'project', 'drum', 'kind', 'build']\n",
      "['lyric', 'title', 'kind', 'love', 'band', 'people', 'life', 'point', 'emo', 'indie_rock', 'sort', 'punk', 'indie', 'chorus', 'big']\n",
      "['metal', 'riff', 'hardcore', 'black_metal', 'punk', 'band', 'noise', 'death', 'drum', 'doom', 'heavy', 'drummer', 'bass', 'black', 'early']\n",
      "['love', 'r&b', 'artist', 'singer', 'star', 'single', 'hit', 'year', 'producer', 'girl', 'debut', 'prince', 'big', 'soul', 'woman']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   573,  1307,   131,\n",
      "          526,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   573,  1307,   131,\n",
      "          526,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.49333333333333335\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "piece\n",
      "tone\n",
      "sense\n",
      "space\n",
      "build\n",
      "melody\n",
      "close\n",
      "ambient\n",
      "drum\n",
      "piano\n",
      "light\n",
      "note\n",
      "electronic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence c_uci -0.06781284856378715\n",
      "coherence c_npmi 0.011368713009881527\n",
      "coherence c_cv 0.38796298258406237\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   573,  1307,   131,\n",
      "          526,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 721\tIter    6\tLoss:2156.9306641\tRec Loss:2147.7922363\tKL Div:9.1384935\n",
      "Epoch 721\tIter   12\tLoss:2133.9567871\tRec Loss:2125.0422363\tKL Div:8.9146309\n",
      "Epoch 721\tIter   18\tLoss:2086.1940918\tRec Loss:2077.2631836\tKL Div:8.9308243\n",
      "Epoch 722\tIter    6\tLoss:2139.5761719\tRec Loss:2130.5424805\tKL Div:9.0336952\n",
      "Epoch 722\tIter   12\tLoss:2113.3210449\tRec Loss:2104.4135742\tKL Div:8.9074888\n",
      "Epoch 722\tIter   18\tLoss:2146.2224121\tRec Loss:2137.3437500\tKL Div:8.8786774\n",
      "Epoch 723\tIter    6\tLoss:2110.7441406\tRec Loss:2101.7944336\tKL Div:8.9497395\n",
      "Epoch 723\tIter   12\tLoss:2136.8774414\tRec Loss:2127.8798828\tKL Div:8.9974651\n",
      "Epoch 723\tIter   18\tLoss:2139.6875000\tRec Loss:2130.6652832\tKL Div:9.0222731\n",
      "Epoch 724\tIter    6\tLoss:2099.8398438\tRec Loss:2090.8754883\tKL Div:8.9644012\n",
      "Epoch 724\tIter   12\tLoss:2156.6687012\tRec Loss:2147.8166504\tKL Div:8.8520107\n",
      "Epoch 724\tIter   18\tLoss:2093.7263184\tRec Loss:2084.7353516\tKL Div:8.9908886\n",
      "Epoch 725\tIter    6\tLoss:2142.4367676\tRec Loss:2133.5334473\tKL Div:8.9034004\n",
      "Epoch 725\tIter   12\tLoss:2122.5039062\tRec Loss:2113.5493164\tKL Div:8.9546175\n",
      "Epoch 725\tIter   18\tLoss:2108.8210449\tRec Loss:2099.8012695\tKL Div:9.0196857\n",
      "Epoch 726\tIter    6\tLoss:2160.1108398\tRec Loss:2151.0556641\tKL Div:9.0552216\n",
      "Epoch 726\tIter   12\tLoss:2170.5249023\tRec Loss:2161.5361328\tKL Div:8.9887276\n",
      "Epoch 726\tIter   18\tLoss:2107.9624023\tRec Loss:2099.0717773\tKL Div:8.8907375\n",
      "Epoch 727\tIter    6\tLoss:2180.7343750\tRec Loss:2171.7380371\tKL Div:8.9963264\n",
      "Epoch 727\tIter   12\tLoss:2152.8510742\tRec Loss:2143.7480469\tKL Div:9.1031227\n",
      "Epoch 727\tIter   18\tLoss:2106.4233398\tRec Loss:2097.5461426\tKL Div:8.8771420\n",
      "Epoch 728\tIter    6\tLoss:2106.9848633\tRec Loss:2097.9660645\tKL Div:9.0189123\n",
      "Epoch 728\tIter   12\tLoss:2150.1728516\tRec Loss:2141.1269531\tKL Div:9.0460033\n",
      "Epoch 728\tIter   18\tLoss:2114.4763184\tRec Loss:2105.5268555\tKL Div:8.9495468\n",
      "Epoch 729\tIter    6\tLoss:2116.4450684\tRec Loss:2107.3764648\tKL Div:9.0685806\n",
      "Epoch 729\tIter   12\tLoss:2148.4553223\tRec Loss:2139.4736328\tKL Div:8.9817448\n",
      "Epoch 729\tIter   18\tLoss:2126.6313477\tRec Loss:2117.6735840\tKL Div:8.9578819\n",
      "Epoch 730\tIter    6\tLoss:2093.4680176\tRec Loss:2084.5668945\tKL Div:8.9012079\n",
      "Epoch 730\tIter   12\tLoss:2092.0114746\tRec Loss:2083.1289062\tKL Div:8.8825550\n",
      "Epoch 730\tIter   18\tLoss:2111.0983887\tRec Loss:2102.1169434\tKL Div:8.9813862\n",
      "Epoch 731\tIter    6\tLoss:2130.3645020\tRec Loss:2121.3327637\tKL Div:9.0316391\n",
      "Epoch 731\tIter   12\tLoss:2123.4277344\tRec Loss:2114.5759277\tKL Div:8.8517799\n",
      "Epoch 731\tIter   18\tLoss:2152.9328613\tRec Loss:2143.6494141\tKL Div:9.2833652\n",
      "Epoch 732\tIter    6\tLoss:2174.8557129\tRec Loss:2165.8996582\tKL Div:8.9561272\n",
      "Epoch 732\tIter   12\tLoss:2096.3896484\tRec Loss:2087.3969727\tKL Div:8.9927521\n",
      "Epoch 732\tIter   18\tLoss:2128.0341797\tRec Loss:2119.1467285\tKL Div:8.8874712\n",
      "Epoch 733\tIter    6\tLoss:2159.0991211\tRec Loss:2150.0668945\tKL Div:9.0322504\n",
      "Epoch 733\tIter   12\tLoss:2129.8486328\tRec Loss:2120.8364258\tKL Div:9.0121565\n",
      "Epoch 733\tIter   18\tLoss:2147.1594238\tRec Loss:2138.1901855\tKL Div:8.9691734\n",
      "Epoch 734\tIter    6\tLoss:2110.8427734\tRec Loss:2101.8923340\tKL Div:8.9503965\n",
      "Epoch 734\tIter   12\tLoss:2109.7995605\tRec Loss:2100.8227539\tKL Div:8.9768629\n",
      "Epoch 734\tIter   18\tLoss:2165.7702637\tRec Loss:2156.8796387\tKL Div:8.8906488\n",
      "Epoch 735\tIter    6\tLoss:2145.2739258\tRec Loss:2136.2590332\tKL Div:9.0149918\n",
      "Epoch 735\tIter   12\tLoss:2121.9133301\tRec Loss:2113.0026855\tKL Div:8.9105339\n",
      "Epoch 735\tIter   18\tLoss:2176.1303711\tRec Loss:2167.1108398\tKL Div:9.0195875\n",
      "Epoch 736\tIter    6\tLoss:2170.2260742\tRec Loss:2161.1093750\tKL Div:9.1168098\n",
      "Epoch 736\tIter   12\tLoss:2126.3737793\tRec Loss:2117.4443359\tKL Div:8.9293766\n",
      "Epoch 736\tIter   18\tLoss:2071.3464355\tRec Loss:2062.3964844\tKL Div:8.9499607\n",
      "Epoch 737\tIter    6\tLoss:2134.3044434\tRec Loss:2125.4272461\tKL Div:8.8771324\n",
      "Epoch 737\tIter   12\tLoss:2106.8132324\tRec Loss:2097.8854980\tKL Div:8.9276409\n",
      "Epoch 737\tIter   18\tLoss:2162.3002930\tRec Loss:2153.2902832\tKL Div:9.0099678\n",
      "Epoch 738\tIter    6\tLoss:2135.2561035\tRec Loss:2126.3830566\tKL Div:8.8729477\n",
      "Epoch 738\tIter   12\tLoss:2119.1967773\tRec Loss:2110.2060547\tKL Div:8.9908276\n",
      "Epoch 738\tIter   18\tLoss:2123.1181641\tRec Loss:2114.1574707\tKL Div:8.9605713\n",
      "Epoch 739\tIter    6\tLoss:2157.9504395\tRec Loss:2148.8344727\tKL Div:9.1160393\n",
      "Epoch 739\tIter   12\tLoss:2093.6572266\tRec Loss:2084.8806152\tKL Div:8.7766113\n",
      "Epoch 739\tIter   18\tLoss:2126.1130371\tRec Loss:2117.0991211\tKL Div:9.0138474\n",
      "Epoch 740\tIter    6\tLoss:2097.7355957\tRec Loss:2088.7810059\tKL Div:8.9545822\n",
      "Epoch 740\tIter   12\tLoss:2155.7995605\tRec Loss:2146.8925781\tKL Div:8.9070940\n",
      "Epoch 740\tIter   18\tLoss:2119.7690430\tRec Loss:2110.7802734\tKL Div:8.9886971\n",
      "Epoch 741\tIter    6\tLoss:2124.5964355\tRec Loss:2115.6164551\tKL Div:8.9800253\n",
      "Epoch 741\tIter   12\tLoss:2138.2741699\tRec Loss:2129.3625488\tKL Div:8.9115238\n",
      "Epoch 741\tIter   18\tLoss:2122.1933594\tRec Loss:2113.1660156\tKL Div:9.0272217\n",
      "Epoch 742\tIter    6\tLoss:2098.6318359\tRec Loss:2089.7429199\tKL Div:8.8889408\n",
      "Epoch 742\tIter   12\tLoss:2152.6357422\tRec Loss:2143.7043457\tKL Div:8.9314442\n",
      "Epoch 742\tIter   18\tLoss:2140.0708008\tRec Loss:2131.0830078\tKL Div:8.9877586\n",
      "Epoch 743\tIter    6\tLoss:2134.3183594\tRec Loss:2125.2807617\tKL Div:9.0376225\n",
      "Epoch 743\tIter   12\tLoss:2121.6450195\tRec Loss:2112.6591797\tKL Div:8.9858379\n",
      "Epoch 743\tIter   18\tLoss:2098.3454590\tRec Loss:2089.3510742\tKL Div:8.9943771\n",
      "Epoch 744\tIter    6\tLoss:2164.2561035\tRec Loss:2155.1416016\tKL Div:9.1145420\n",
      "Epoch 744\tIter   12\tLoss:2095.7585449\tRec Loss:2086.9128418\tKL Div:8.8457375\n",
      "Epoch 744\tIter   18\tLoss:2118.1643066\tRec Loss:2109.2683105\tKL Div:8.8961058\n",
      "Epoch 745\tIter    6\tLoss:2160.5483398\tRec Loss:2151.4816895\tKL Div:9.0666800\n",
      "Epoch 745\tIter   12\tLoss:2127.8759766\tRec Loss:2118.9521484\tKL Div:8.9237146\n",
      "Epoch 745\tIter   18\tLoss:2117.9384766\tRec Loss:2109.0502930\tKL Div:8.8881359\n",
      "Epoch 746\tIter    6\tLoss:2122.7097168\tRec Loss:2113.4792480\tKL Div:9.2304335\n",
      "Epoch 746\tIter   12\tLoss:2084.8713379\tRec Loss:2076.1081543\tKL Div:8.7631235\n",
      "Epoch 746\tIter   18\tLoss:2193.5729980\tRec Loss:2184.4665527\tKL Div:9.1064806\n",
      "Epoch 747\tIter    6\tLoss:2109.7897949\tRec Loss:2100.8454590\tKL Div:8.9443474\n",
      "Epoch 747\tIter   12\tLoss:2177.3054199\tRec Loss:2168.2902832\tKL Div:9.0151043\n",
      "Epoch 747\tIter   18\tLoss:2082.2075195\tRec Loss:2073.2421875\tKL Div:8.9653578\n",
      "Epoch 748\tIter    6\tLoss:2125.0236816\tRec Loss:2116.1184082\tKL Div:8.9052505\n",
      "Epoch 748\tIter   12\tLoss:2115.7045898\tRec Loss:2106.7268066\tKL Div:8.9777460\n",
      "Epoch 748\tIter   18\tLoss:2133.9516602\tRec Loss:2124.9106445\tKL Div:9.0410213\n",
      "Epoch 749\tIter    6\tLoss:2142.1875000\tRec Loss:2133.0441895\tKL Div:9.1431942\n",
      "Epoch 749\tIter   12\tLoss:2133.2116699\tRec Loss:2124.2912598\tKL Div:8.9205112\n",
      "Epoch 749\tIter   18\tLoss:2101.8618164\tRec Loss:2093.0383301\tKL Div:8.8233814\n",
      "Epoch 750\tIter    6\tLoss:2125.2663574\tRec Loss:2116.2424316\tKL Div:9.0240021\n",
      "Epoch 750\tIter   12\tLoss:2119.1811523\tRec Loss:2110.1918945\tKL Div:8.9892864\n",
      "Epoch 750\tIter   18\tLoss:2181.4133301\tRec Loss:2172.2539062\tKL Div:9.1593447\n",
      "Epoch 751\tIter    6\tLoss:2097.3232422\tRec Loss:2088.4140625\tKL Div:8.9092922\n",
      "Epoch 751\tIter   12\tLoss:2148.5236816\tRec Loss:2139.4279785\tKL Div:9.0957470\n",
      "Epoch 751\tIter   18\tLoss:2140.0170898\tRec Loss:2131.1440430\tKL Div:8.8731518\n",
      "Epoch 752\tIter    6\tLoss:2117.0434570\tRec Loss:2108.0371094\tKL Div:9.0062637\n",
      "Epoch 752\tIter   12\tLoss:2102.4018555\tRec Loss:2093.4284668\tKL Div:8.9733706\n",
      "Epoch 752\tIter   18\tLoss:2109.5483398\tRec Loss:2100.7250977\tKL Div:8.8232737\n",
      "Epoch 753\tIter    6\tLoss:2131.7324219\tRec Loss:2122.6962891\tKL Div:9.0361233\n",
      "Epoch 753\tIter   12\tLoss:2125.9538574\tRec Loss:2117.0483398\tKL Div:8.9055090\n",
      "Epoch 753\tIter   18\tLoss:2099.0539551\tRec Loss:2090.1757812\tKL Div:8.8780870\n",
      "Epoch 754\tIter    6\tLoss:2087.3940430\tRec Loss:2078.3144531\tKL Div:9.0794744\n",
      "Epoch 754\tIter   12\tLoss:2151.5725098\tRec Loss:2142.6625977\tKL Div:8.9099541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 754\tIter   18\tLoss:2131.6787109\tRec Loss:2122.6572266\tKL Div:9.0213890\n",
      "Epoch 755\tIter    6\tLoss:2119.0568848\tRec Loss:2110.1005859\tKL Div:8.9563608\n",
      "Epoch 755\tIter   12\tLoss:2118.9790039\tRec Loss:2109.8959961\tKL Div:9.0829382\n",
      "Epoch 755\tIter   18\tLoss:2132.4829102\tRec Loss:2123.5205078\tKL Div:8.9625015\n",
      "Epoch 756\tIter    6\tLoss:2111.3493652\tRec Loss:2102.2670898\tKL Div:9.0823622\n",
      "Epoch 756\tIter   12\tLoss:2171.8378906\tRec Loss:2162.7448730\tKL Div:9.0930500\n",
      "Epoch 756\tIter   18\tLoss:2113.8071289\tRec Loss:2104.9951172\tKL Div:8.8119812\n",
      "Epoch 757\tIter    6\tLoss:2119.3532715\tRec Loss:2110.3701172\tKL Div:8.9831886\n",
      "Epoch 757\tIter   12\tLoss:2158.6669922\tRec Loss:2149.6110840\tKL Div:9.0559998\n",
      "Epoch 757\tIter   18\tLoss:2150.3381348\tRec Loss:2141.2485352\tKL Div:9.0897121\n",
      "Epoch 758\tIter    6\tLoss:2078.6955566\tRec Loss:2069.9108887\tKL Div:8.7845860\n",
      "Epoch 758\tIter   12\tLoss:2126.8222656\tRec Loss:2117.7758789\tKL Div:9.0464039\n",
      "Epoch 758\tIter   18\tLoss:2135.5183105\tRec Loss:2126.4780273\tKL Div:9.0402737\n",
      "Epoch 759\tIter    6\tLoss:2105.0866699\tRec Loss:2096.2873535\tKL Div:8.7993889\n",
      "Epoch 759\tIter   12\tLoss:2133.5012207\tRec Loss:2124.2309570\tKL Div:9.2703419\n",
      "Epoch 759\tIter   18\tLoss:2089.5651855\tRec Loss:2080.8525391\tKL Div:8.7125816\n",
      "Epoch 760\tIter    6\tLoss:2142.0561523\tRec Loss:2132.9370117\tKL Div:9.1191530\n",
      "Epoch 760\tIter   12\tLoss:2181.4689941\tRec Loss:2172.5083008\tKL Div:8.9607296\n",
      "Epoch 760\tIter   18\tLoss:2113.4760742\tRec Loss:2104.3923340\tKL Div:9.0836992\n",
      "Epoch 760\tLoss:2123.3541310\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,  2218,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'piece', 'tone', 'sense', 'space', 'build', 'melody', 'ambient', 'drum', 'close', 'piano', 'electronic', 'light', 'open']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'debut', 'piano', 'harmony', 'group', 'chorus', 'string', 'line', 'bit', 'write', 'pretty']\n",
      "['live', 'version', 'disc', 'set', 'include', 'record', 'soul', 'label', 'studio', 'recording', 'cover', 'original', 'compilation', 'group', 'early']\n",
      "['guy', 'bad', 'man', 'lyric', 'listen', 'day', 'get', 'kid', 'people', 'love', 'girl', 'let', '\\n', 'pretty', 'line']\n",
      "['house', 'techno', 'mix', 'label', 'dance', 'bass', 'producer', 'beat', 'dj', 'synth', 'artist', 'set', 'drum', 'club', 'rhythm']\n",
      "['dance', 'disco', 'synth', 'single', 'love', 'house', 'group', 'big', 'duo', 'remix', 'electro', 'indie', 'turn', 'club', 'electronic']\n",
      "['\\xa0', 'project', 'point', 'artist', 'year', 'solo', 'title', 'synth', 'early', 'ep', 'producer', 'close', 'place', 'start', 'lyric']\n",
      "['melody', 'debut', 'riff', 'drum', 'band', 'chorus', 'hook', 'group', 'punk', 'line', 'bass', 'rhythm', 'ep', 'opener', 'style']\n",
      "['sing', 'love', 'life', 'lyric', 'world', 'line', 'word', 'write', 'feeling', 'leave', 'close', 'death', 'light', 'sense', 'kind']\n",
      "['punk', 'live', 'single', 'love', 'cover', 'band', 'early', 'solo', 'day', 'fan', 'big', 'set', 'smith', 'classic', 'group']\n",
      "['drum', 'electronic', 'jazz', 'noise', 'bass', 'melody', 'beat', 'piece', 'interesting', 'disc', 'begin', 'live', 'feature', 'bit', 'group']\n",
      "['life', 'man', 'world', 'people', 'black', 'woman', 'love', 'write', 'political', 'word', 'call', 'white', 'story', 'live', 'year']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'dylan', 'record', 'artist', 'lyric', 'acoustic', 'man', 'oldham', 'american', 'love']\n",
      "['beat', 'hip_hop', 'sample', 'funk', 'mix', 'party', 'dance', 'genre', 'people', 'break', 'big', 'bass', 'rap', 'dj', 'bad']\n",
      "['piece', 'jazz', 'film', 'piano', 'composer', 'musician', 'composition', 'world', 'instrument', 'recording', 'electronic', 'group', 'create', 'note', 'score']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'verse', 'production', 'year', 'producer', 'flow', 'style', 'feature', 'line', 'big', 'artist']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'artist', 'debut', 'sense', 'instrumental', 'production', 'project', 'kind', 'drum', 'build']\n",
      "['lyric', 'title', 'kind', 'band', 'love', 'people', 'emo', 'indie_rock', 'point', 'life', 'sort', 'indie', 'punk', 'hook', 'chorus']\n",
      "['metal', 'riff', 'hardcore', 'black_metal', 'punk', 'band', 'noise', 'death', 'drum', 'heavy', 'doom', 'year', 'close', 'drummer', 'scream']\n",
      "['love', 'r&b', 'artist', 'hit', 'star', 'singer', 'single', 'year', 'producer', 'girl', 'debut', 'prince', 'big', 'soul', 'woman']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,  2218,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,  2218,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5033333333333333\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "piece\n",
      "tone\n",
      "sense\n",
      "space\n",
      "build\n",
      "melody\n",
      "ambient\n",
      "drum\n",
      "close\n",
      "piano\n",
      "electronic\n",
      "light\n",
      "open\n",
      "coherence c_uci -0.038519366711460404\n",
      "coherence c_npmi 0.012108116514196403\n",
      "coherence c_cv 0.3868893736612242\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,  2218,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 761\tIter    6\tLoss:2140.6059570\tRec Loss:2131.6616211\tKL Div:8.9442444\n",
      "Epoch 761\tIter   12\tLoss:2139.0109863\tRec Loss:2129.9946289\tKL Div:9.0163441\n",
      "Epoch 761\tIter   18\tLoss:2144.1228027\tRec Loss:2135.1408691\tKL Div:8.9819221\n",
      "Epoch 762\tIter    6\tLoss:2141.0007324\tRec Loss:2131.9990234\tKL Div:9.0017147\n",
      "Epoch 762\tIter   12\tLoss:2145.8178711\tRec Loss:2136.9272461\tKL Div:8.8906441\n",
      "Epoch 762\tIter   18\tLoss:2079.1354980\tRec Loss:2070.0903320\tKL Div:9.0451145\n",
      "Epoch 763\tIter    6\tLoss:2165.7983398\tRec Loss:2156.9438477\tKL Div:8.8543806\n",
      "Epoch 763\tIter   12\tLoss:2113.8503418\tRec Loss:2105.0544434\tKL Div:8.7957916\n",
      "Epoch 763\tIter   18\tLoss:2080.5544434\tRec Loss:2071.5019531\tKL Div:9.0524855\n",
      "Epoch 764\tIter    6\tLoss:2112.7216797\tRec Loss:2103.8869629\tKL Div:8.8346214\n",
      "Epoch 764\tIter   12\tLoss:2123.0209961\tRec Loss:2113.7556152\tKL Div:9.2654009\n",
      "Epoch 764\tIter   18\tLoss:2172.2185059\tRec Loss:2163.2500000\tKL Div:8.9686222\n",
      "Epoch 765\tIter    6\tLoss:2089.5356445\tRec Loss:2080.4836426\tKL Div:9.0518961\n",
      "Epoch 765\tIter   12\tLoss:2137.6716309\tRec Loss:2128.7924805\tKL Div:8.8790646\n",
      "Epoch 765\tIter   18\tLoss:2097.1164551\tRec Loss:2088.1040039\tKL Div:9.0124550\n",
      "Epoch 766\tIter    6\tLoss:2168.7651367\tRec Loss:2159.7836914\tKL Div:8.9815350\n",
      "Epoch 766\tIter   12\tLoss:2129.0173340\tRec Loss:2119.8635254\tKL Div:9.1537781\n",
      "Epoch 766\tIter   18\tLoss:2138.1606445\tRec Loss:2129.2517090\tKL Div:8.9089737\n",
      "Epoch 767\tIter    6\tLoss:2157.4514160\tRec Loss:2148.1796875\tKL Div:9.2717857\n",
      "Epoch 767\tIter   12\tLoss:2120.8378906\tRec Loss:2111.8889160\tKL Div:8.9490652\n",
      "Epoch 767\tIter   18\tLoss:2090.2341309\tRec Loss:2081.2961426\tKL Div:8.9380026\n",
      "Epoch 768\tIter    6\tLoss:2138.2065430\tRec Loss:2129.1171875\tKL Div:9.0893373\n",
      "Epoch 768\tIter   12\tLoss:2115.5922852\tRec Loss:2106.7456055\tKL Div:8.8467541\n",
      "Epoch 768\tIter   18\tLoss:2147.6430664\tRec Loss:2138.6403809\tKL Div:9.0026608\n",
      "Epoch 769\tIter    6\tLoss:2137.9592285\tRec Loss:2128.9465332\tKL Div:9.0127993\n",
      "Epoch 769\tIter   12\tLoss:2124.6428223\tRec Loss:2115.6313477\tKL Div:9.0114746\n",
      "Epoch 769\tIter   18\tLoss:2174.5546875\tRec Loss:2165.4682617\tKL Div:9.0865231\n",
      "Epoch 770\tIter    6\tLoss:2125.6591797\tRec Loss:2116.7983398\tKL Div:8.8607311\n",
      "Epoch 770\tIter   12\tLoss:2106.9597168\tRec Loss:2097.9965820\tKL Div:8.9632301\n",
      "Epoch 770\tIter   18\tLoss:2160.2685547\tRec Loss:2151.0747070\tKL Div:9.1939697\n",
      "Epoch 771\tIter    6\tLoss:2123.0495605\tRec Loss:2114.0942383\tKL Div:8.9552574\n",
      "Epoch 771\tIter   12\tLoss:2153.1416016\tRec Loss:2144.2741699\tKL Div:8.8674545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 771\tIter   18\tLoss:2177.7136230\tRec Loss:2168.6352539\tKL Div:9.0783834\n",
      "Epoch 772\tIter    6\tLoss:2122.1254883\tRec Loss:2113.0908203\tKL Div:9.0347109\n",
      "Epoch 772\tIter   12\tLoss:2187.2204590\tRec Loss:2178.2094727\tKL Div:9.0109177\n",
      "Epoch 772\tIter   18\tLoss:2131.1135254\tRec Loss:2122.0415039\tKL Div:9.0719452\n",
      "Epoch 773\tIter    6\tLoss:2121.8945312\tRec Loss:2112.8422852\tKL Div:9.0521517\n",
      "Epoch 773\tIter   12\tLoss:2141.3139648\tRec Loss:2132.4753418\tKL Div:8.8385887\n",
      "Epoch 773\tIter   18\tLoss:2161.0725098\tRec Loss:2152.1899414\tKL Div:8.8826637\n",
      "Epoch 774\tIter    6\tLoss:2151.5190430\tRec Loss:2142.2580566\tKL Div:9.2609978\n",
      "Epoch 774\tIter   12\tLoss:2140.9091797\tRec Loss:2132.1367188\tKL Div:8.7723684\n",
      "Epoch 774\tIter   18\tLoss:2157.8718262\tRec Loss:2148.7390137\tKL Div:9.1328697\n",
      "Epoch 775\tIter    6\tLoss:2134.4191895\tRec Loss:2125.5085449\tKL Div:8.9107294\n",
      "Epoch 775\tIter   12\tLoss:2136.5654297\tRec Loss:2127.3979492\tKL Div:9.1675072\n",
      "Epoch 775\tIter   18\tLoss:2085.0817871\tRec Loss:2076.3139648\tKL Div:8.7678280\n",
      "Epoch 776\tIter    6\tLoss:2106.5356445\tRec Loss:2097.5898438\tKL Div:8.9458275\n",
      "Epoch 776\tIter   12\tLoss:2100.5561523\tRec Loss:2091.5839844\tKL Div:8.9720640\n",
      "Epoch 776\tIter   18\tLoss:2132.3767090\tRec Loss:2123.2875977\tKL Div:9.0890770\n",
      "Epoch 777\tIter    6\tLoss:2096.9670410\tRec Loss:2088.1367188\tKL Div:8.8302269\n",
      "Epoch 777\tIter   12\tLoss:2128.8366699\tRec Loss:2119.8901367\tKL Div:8.9466486\n",
      "Epoch 777\tIter   18\tLoss:2125.1657715\tRec Loss:2116.1962891\tKL Div:8.9694481\n",
      "Epoch 778\tIter    6\tLoss:2071.0620117\tRec Loss:2062.1682129\tKL Div:8.8938503\n",
      "Epoch 778\tIter   12\tLoss:2148.9047852\tRec Loss:2140.0361328\tKL Div:8.8686123\n",
      "Epoch 778\tIter   18\tLoss:2122.7409668\tRec Loss:2113.6279297\tKL Div:9.1131306\n",
      "Epoch 779\tIter    6\tLoss:2094.6225586\tRec Loss:2085.6330566\tKL Div:8.9894114\n",
      "Epoch 779\tIter   12\tLoss:2109.2700195\tRec Loss:2100.2941895\tKL Div:8.9757843\n",
      "Epoch 779\tIter   18\tLoss:2128.4160156\tRec Loss:2119.4667969\tKL Div:8.9493141\n",
      "Epoch 780\tIter    6\tLoss:2138.8195801\tRec Loss:2129.9003906\tKL Div:8.9190750\n",
      "Epoch 780\tIter   12\tLoss:2119.4152832\tRec Loss:2110.4409180\tKL Div:8.9744577\n",
      "Epoch 780\tIter   18\tLoss:2109.7858887\tRec Loss:2100.9025879\tKL Div:8.8834095\n",
      "Epoch 781\tIter    6\tLoss:2130.3596191\tRec Loss:2121.2861328\tKL Div:9.0734043\n",
      "Epoch 781\tIter   12\tLoss:2117.2719727\tRec Loss:2108.3549805\tKL Div:8.9170647\n",
      "Epoch 781\tIter   18\tLoss:2068.6604004\tRec Loss:2059.6738281\tKL Div:8.9866505\n",
      "Epoch 782\tIter    6\tLoss:2113.3886719\tRec Loss:2104.4958496\tKL Div:8.8927898\n",
      "Epoch 782\tIter   12\tLoss:2138.4184570\tRec Loss:2129.4045410\tKL Div:9.0139160\n",
      "Epoch 782\tIter   18\tLoss:2154.8498535\tRec Loss:2145.7353516\tKL Div:9.1144753\n",
      "Epoch 783\tIter    6\tLoss:2138.5212402\tRec Loss:2129.6547852\tKL Div:8.8664885\n",
      "Epoch 783\tIter   12\tLoss:2135.2329102\tRec Loss:2126.1923828\tKL Div:9.0404778\n",
      "Epoch 783\tIter   18\tLoss:2087.1035156\tRec Loss:2078.1687012\tKL Div:8.9349089\n",
      "Epoch 784\tIter    6\tLoss:2140.9350586\tRec Loss:2131.9228516\tKL Div:9.0122871\n",
      "Epoch 784\tIter   12\tLoss:2135.8554688\tRec Loss:2126.7890625\tKL Div:9.0665140\n",
      "Epoch 784\tIter   18\tLoss:2093.8286133\tRec Loss:2084.9382324\tKL Div:8.8904686\n",
      "Epoch 785\tIter    6\tLoss:2133.5764160\tRec Loss:2124.4985352\tKL Div:9.0779657\n",
      "Epoch 785\tIter   12\tLoss:2073.7145996\tRec Loss:2064.8442383\tKL Div:8.8703556\n",
      "Epoch 785\tIter   18\tLoss:2124.5278320\tRec Loss:2115.4223633\tKL Div:9.1053543\n",
      "Epoch 786\tIter    6\tLoss:2136.1594238\tRec Loss:2127.1811523\tKL Div:8.9783363\n",
      "Epoch 786\tIter   12\tLoss:2104.5617676\tRec Loss:2095.5178223\tKL Div:9.0439005\n",
      "Epoch 786\tIter   18\tLoss:2131.8457031\tRec Loss:2122.9123535\tKL Div:8.9334373\n",
      "Epoch 787\tIter    6\tLoss:2121.3039551\tRec Loss:2112.3154297\tKL Div:8.9885588\n",
      "Epoch 787\tIter   12\tLoss:2128.1411133\tRec Loss:2119.1491699\tKL Div:8.9919939\n",
      "Epoch 787\tIter   18\tLoss:2122.1025391\tRec Loss:2113.1564941\tKL Div:8.9461517\n",
      "Epoch 788\tIter    6\tLoss:2088.4860840\tRec Loss:2079.4223633\tKL Div:9.0637856\n",
      "Epoch 788\tIter   12\tLoss:2146.0576172\tRec Loss:2136.9155273\tKL Div:9.1420288\n",
      "Epoch 788\tIter   18\tLoss:2149.8378906\tRec Loss:2140.8435059\tKL Div:8.9942989\n",
      "Epoch 789\tIter    6\tLoss:2105.9660645\tRec Loss:2096.8798828\tKL Div:9.0862713\n",
      "Epoch 789\tIter   12\tLoss:2120.0849609\tRec Loss:2111.0959473\tKL Div:8.9889164\n",
      "Epoch 789\tIter   18\tLoss:2114.8374023\tRec Loss:2105.8491211\tKL Div:8.9881926\n",
      "Epoch 790\tIter    6\tLoss:2105.3134766\tRec Loss:2096.1416016\tKL Div:9.1719141\n",
      "Epoch 790\tIter   12\tLoss:2113.6066895\tRec Loss:2104.7160645\tKL Div:8.8905449\n",
      "Epoch 790\tIter   18\tLoss:2128.3542480\tRec Loss:2119.3325195\tKL Div:9.0216169\n",
      "Epoch 791\tIter    6\tLoss:2119.0537109\tRec Loss:2110.1806641\tKL Div:8.8730049\n",
      "Epoch 791\tIter   12\tLoss:2124.2294922\tRec Loss:2115.0839844\tKL Div:9.1454678\n",
      "Epoch 791\tIter   18\tLoss:2130.1525879\tRec Loss:2121.2187500\tKL Div:8.9338369\n",
      "Epoch 792\tIter    6\tLoss:2163.3410645\tRec Loss:2154.3562012\tKL Div:8.9847488\n",
      "Epoch 792\tIter   12\tLoss:2106.9768066\tRec Loss:2097.9062500\tKL Div:9.0704823\n",
      "Epoch 792\tIter   18\tLoss:2143.9545898\tRec Loss:2134.9714355\tKL Div:8.9832344\n",
      "Epoch 793\tIter    6\tLoss:2113.6484375\tRec Loss:2104.6530762\tKL Div:8.9954128\n",
      "Epoch 793\tIter   12\tLoss:2141.8725586\tRec Loss:2132.9965820\tKL Div:8.8759718\n",
      "Epoch 793\tIter   18\tLoss:2120.6354980\tRec Loss:2111.6162109\tKL Div:9.0192471\n",
      "Epoch 794\tIter    6\tLoss:2121.5690918\tRec Loss:2112.5207520\tKL Div:9.0482845\n",
      "Epoch 794\tIter   12\tLoss:2121.7709961\tRec Loss:2112.7739258\tKL Div:8.9970436\n",
      "Epoch 794\tIter   18\tLoss:2136.0427246\tRec Loss:2127.0698242\tKL Div:8.9728718\n",
      "Epoch 795\tIter    6\tLoss:2146.0625000\tRec Loss:2136.9953613\tKL Div:9.0670748\n",
      "Epoch 795\tIter   12\tLoss:2077.8054199\tRec Loss:2068.6333008\tKL Div:9.1720581\n",
      "Epoch 795\tIter   18\tLoss:2115.8398438\tRec Loss:2107.0622559\tKL Div:8.7775192\n",
      "Epoch 796\tIter    6\tLoss:2119.9812012\tRec Loss:2110.9467773\tKL Div:9.0343647\n",
      "Epoch 796\tIter   12\tLoss:2149.8090820\tRec Loss:2140.7548828\tKL Div:9.0543203\n",
      "Epoch 796\tIter   18\tLoss:2141.4660645\tRec Loss:2132.4792480\tKL Div:8.9867458\n",
      "Epoch 797\tIter    6\tLoss:2122.6884766\tRec Loss:2113.5073242\tKL Div:9.1811428\n",
      "Epoch 797\tIter   12\tLoss:2116.9057617\tRec Loss:2108.0268555\tKL Div:8.8788509\n",
      "Epoch 797\tIter   18\tLoss:2117.7453613\tRec Loss:2108.7834473\tKL Div:8.9619179\n",
      "Epoch 798\tIter    6\tLoss:2112.1140137\tRec Loss:2103.1665039\tKL Div:8.9475021\n",
      "Epoch 798\tIter   12\tLoss:2125.6005859\tRec Loss:2116.7893066\tKL Div:8.8113956\n",
      "Epoch 798\tIter   18\tLoss:2092.1660156\tRec Loss:2083.2624512\tKL Div:8.9034653\n",
      "Epoch 799\tIter    6\tLoss:2118.1074219\tRec Loss:2108.8559570\tKL Div:9.2513657\n",
      "Epoch 799\tIter   12\tLoss:2143.7827148\tRec Loss:2134.9084473\tKL Div:8.8741512\n",
      "Epoch 799\tIter   18\tLoss:2163.6088867\tRec Loss:2154.4794922\tKL Div:9.1294699\n",
      "Epoch 800\tIter    6\tLoss:2119.4409180\tRec Loss:2110.4560547\tKL Div:8.9849701\n",
      "Epoch 800\tIter   12\tLoss:2132.5278320\tRec Loss:2123.5651855\tKL Div:8.9625244\n",
      "Epoch 800\tIter   18\tLoss:2100.1982422\tRec Loss:2091.3242188\tKL Div:8.8740597\n",
      "Epoch 800\tLoss:2126.4525135\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   571,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'tone', 'piece', 'sense', 'space', 'build', 'ambient', 'melody', 'close', 'drum', 'piano', 'electronic', 'note', 'light']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'debut', 'group', 'harmony', 'piano', 'bit', 'chorus', 'line', 'string', 'write', 'day']\n",
      "['live', 'version', 'disc', 'set', 'include', 'record', 'studio', 'recording', 'soul', 'cover', 'label', 'group', 'compilation', 'early', 'original']\n",
      "['man', 'guy', 'bad', 'lyric', 'listen', 'get', 'day', 'love', 'kid', 'people', '\\n', 'let', 'girl', 'pretty', 'line']\n",
      "['house', 'techno', 'label', 'dance', 'mix', 'producer', 'bass', 'beat', 'dj', 'artist', 'synth', 'drum', 'set', 'club', 'rhythm']\n",
      "['dance', 'disco', 'synth', 'single', 'house', 'love', 'big', 'remix', 'group', 'duo', 'electro', 'club', 'indie', 'act', 'electronic']\n",
      "['\\xa0', 'project', 'point', 'artist', 'year', 'title', 'solo', 'synth', 'early', 'producer', 'close', 'ep', 'place', 'feature', 'start']\n",
      "['melody', 'riff', 'debut', 'drum', 'chorus', 'band', 'hook', 'group', 'line', 'punk', 'bass', 'rhythm', 'ep', 'opener', 'hard']\n",
      "['sing', 'love', 'life', 'lyric', 'world', 'line', 'word', 'write', 'close', 'leave', 'feeling', 'death', 'sense', 'light', 'kind']\n",
      "['punk', 'live', 'single', 'love', 'cover', 'band', 'early', 'solo', 'set', 'day', 'fan', 'big', 'classic', 'group', 'pollard']\n",
      "['drum', 'electronic', 'jazz', 'noise', 'melody', 'bass', 'beat', 'piece', 'begin', 'disc', 'feature', 'group', 'interesting', 'live', 'bit']\n",
      "['life', 'man', 'world', 'black', 'people', 'woman', 'write', 'love', 'political', 'white', 'word', 'call', 'story', 'live', 'year']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'dylan', 'acoustic', 'man', 'artist', 'record', 'lyric', 'american', 'oldham', 'write']\n",
      "['beat', 'hip_hop', 'sample', 'funk', 'mix', 'party', 'dance', 'break', 'people', 'rap', 'genre', 'bass', 'dj', 'big', 'style']\n",
      "['piece', 'composer', 'jazz', 'piano', 'film', 'composition', 'musician', 'world', 'instrument', 'recording', 'electronic', 'group', 'create', 'string', 'score']\n",
      "['rap', 'rapper', 'beat', 'hip_hop', 'mixtape', 'verse', 'production', 'year', 'producer', 'flow', 'feature', 'line', 'style', 'big', 'artist']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'sense', 'artist', 'debut', 'instrumental', 'project', 'production', 'drum', 'build', 'kind']\n",
      "['lyric', 'title', 'kind', 'band', 'love', 'people', 'point', 'sort', 'life', 'emo', 'indie_rock', 'indie', 'punk', 'hook', 'big']\n",
      "['metal', 'riff', 'hardcore', 'black_metal', 'punk', 'band', 'noise', 'drum', 'death', 'doom', 'heavy', 'scream', 'close', 'black', 'bass']\n",
      "['love', 'r&b', 'artist', 'singer', 'star', 'single', 'hit', 'year', 'producer', 'debut', 'girl', 'prince', 'big', 'soul', 'woman']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   571,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   571,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "tone\n",
      "piece\n",
      "sense\n",
      "space\n",
      "build\n",
      "ambient\n",
      "melody\n",
      "close\n",
      "drum\n",
      "piano\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electronic\n",
      "note\n",
      "light\n",
      "coherence c_uci -0.06359800156633708\n",
      "coherence c_npmi 0.012230241707794388\n",
      "coherence c_cv 0.38853604435257605\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   571,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 801\tIter    6\tLoss:2096.6601562\tRec Loss:2087.4995117\tKL Div:9.1606655\n",
      "Epoch 801\tIter   12\tLoss:2110.2023926\tRec Loss:2101.3627930\tKL Div:8.8396063\n",
      "Epoch 801\tIter   18\tLoss:2131.6767578\tRec Loss:2122.5913086\tKL Div:9.0854473\n",
      "Epoch 802\tIter    6\tLoss:2091.9626465\tRec Loss:2083.0539551\tKL Div:8.9088097\n",
      "Epoch 802\tIter   12\tLoss:2134.5620117\tRec Loss:2125.5844727\tKL Div:8.9774637\n",
      "Epoch 802\tIter   18\tLoss:2135.5876465\tRec Loss:2126.5981445\tKL Div:8.9895916\n",
      "Epoch 803\tIter    6\tLoss:2109.9560547\tRec Loss:2100.9316406\tKL Div:9.0244141\n",
      "Epoch 803\tIter   12\tLoss:2143.2031250\tRec Loss:2134.1772461\tKL Div:9.0259542\n",
      "Epoch 803\tIter   18\tLoss:2118.2170410\tRec Loss:2109.2231445\tKL Div:8.9938602\n",
      "Epoch 804\tIter    6\tLoss:2119.0993652\tRec Loss:2110.0766602\tKL Div:9.0226383\n",
      "Epoch 804\tIter   12\tLoss:2106.9270020\tRec Loss:2098.0302734\tKL Div:8.8966398\n",
      "Epoch 804\tIter   18\tLoss:2105.9897461\tRec Loss:2097.1489258\tKL Div:8.8408909\n",
      "Epoch 805\tIter    6\tLoss:2136.5498047\tRec Loss:2127.4804688\tKL Div:9.0694046\n",
      "Epoch 805\tIter   12\tLoss:2080.1076660\tRec Loss:2071.0942383\tKL Div:9.0134907\n",
      "Epoch 805\tIter   18\tLoss:2134.2551270\tRec Loss:2125.3586426\tKL Div:8.8964024\n",
      "Epoch 806\tIter    6\tLoss:2099.4177246\tRec Loss:2090.2924805\tKL Div:9.1252871\n",
      "Epoch 806\tIter   12\tLoss:2153.9106445\tRec Loss:2144.9624023\tKL Div:8.9481239\n",
      "Epoch 806\tIter   18\tLoss:2131.4851074\tRec Loss:2122.4897461\tKL Div:8.9952469\n",
      "Epoch 807\tIter    6\tLoss:2131.7983398\tRec Loss:2122.5166016\tKL Div:9.2817860\n",
      "Epoch 807\tIter   12\tLoss:2122.0268555\tRec Loss:2113.2761230\tKL Div:8.7507610\n",
      "Epoch 807\tIter   18\tLoss:2129.9206543\tRec Loss:2120.8808594\tKL Div:9.0398769\n",
      "Epoch 808\tIter    6\tLoss:2153.2763672\tRec Loss:2144.2741699\tKL Div:9.0022688\n",
      "Epoch 808\tIter   12\tLoss:2117.9531250\tRec Loss:2108.7631836\tKL Div:9.1899166\n",
      "Epoch 808\tIter   18\tLoss:2120.2888184\tRec Loss:2111.5988770\tKL Div:8.6898756\n",
      "Epoch 809\tIter    6\tLoss:2093.2707520\tRec Loss:2084.0278320\tKL Div:9.2428074\n",
      "Epoch 809\tIter   12\tLoss:2157.4790039\tRec Loss:2148.5458984\tKL Div:8.9331236\n",
      "Epoch 809\tIter   18\tLoss:2119.2160645\tRec Loss:2110.3901367\tKL Div:8.8260117\n",
      "Epoch 810\tIter    6\tLoss:2125.4365234\tRec Loss:2116.4111328\tKL Div:9.0252771\n",
      "Epoch 810\tIter   12\tLoss:2131.2202148\tRec Loss:2122.2250977\tKL Div:8.9951172\n",
      "Epoch 810\tIter   18\tLoss:2124.4489746\tRec Loss:2115.4702148\tKL Div:8.9788113\n",
      "Epoch 811\tIter    6\tLoss:2093.9587402\tRec Loss:2085.1396484\tKL Div:8.8191471\n",
      "Epoch 811\tIter   12\tLoss:2090.8764648\tRec Loss:2081.8254395\tKL Div:9.0510139\n",
      "Epoch 811\tIter   18\tLoss:2149.0153809\tRec Loss:2140.0019531\tKL Div:9.0133553\n",
      "Epoch 812\tIter    6\tLoss:2126.6591797\tRec Loss:2117.5942383\tKL Div:9.0649624\n",
      "Epoch 812\tIter   12\tLoss:2133.8876953\tRec Loss:2124.8962402\tKL Div:8.9914894\n",
      "Epoch 812\tIter   18\tLoss:2112.9201660\tRec Loss:2103.9340820\tKL Div:8.9860954\n",
      "Epoch 813\tIter    6\tLoss:2104.2514648\tRec Loss:2095.2436523\tKL Div:9.0077190\n",
      "Epoch 813\tIter   12\tLoss:2144.2053223\tRec Loss:2135.2263184\tKL Div:8.9790058\n",
      "Epoch 813\tIter   18\tLoss:2135.3137207\tRec Loss:2126.2695312\tKL Div:9.0440960\n",
      "Epoch 814\tIter    6\tLoss:2076.8696289\tRec Loss:2068.1071777\tKL Div:8.7623348\n",
      "Epoch 814\tIter   12\tLoss:2104.9619141\tRec Loss:2095.8969727\tKL Div:9.0650282\n",
      "Epoch 814\tIter   18\tLoss:2122.4916992\tRec Loss:2113.5693359\tKL Div:8.9222794\n",
      "Epoch 815\tIter    6\tLoss:2090.7409668\tRec Loss:2081.8010254\tKL Div:8.9400167\n",
      "Epoch 815\tIter   12\tLoss:2091.1606445\tRec Loss:2082.3291016\tKL Div:8.8315296\n",
      "Epoch 815\tIter   18\tLoss:2142.4392090\tRec Loss:2133.4001465\tKL Div:9.0389719\n",
      "Epoch 816\tIter    6\tLoss:2096.0734863\tRec Loss:2087.2124023\tKL Div:8.8611078\n",
      "Epoch 816\tIter   12\tLoss:2102.0310059\tRec Loss:2093.0590820\tKL Div:8.9720097\n",
      "Epoch 816\tIter   18\tLoss:2156.5834961\tRec Loss:2147.6484375\tKL Div:8.9349823\n",
      "Epoch 817\tIter    6\tLoss:2108.5278320\tRec Loss:2099.4702148\tKL Div:9.0575867\n",
      "Epoch 817\tIter   12\tLoss:2156.5537109\tRec Loss:2147.5229492\tKL Div:9.0307646\n",
      "Epoch 817\tIter   18\tLoss:2149.0058594\tRec Loss:2140.0830078\tKL Div:8.9229355\n",
      "Epoch 818\tIter    6\tLoss:2095.0087891\tRec Loss:2085.9719238\tKL Div:9.0367966\n",
      "Epoch 818\tIter   12\tLoss:2116.9582520\tRec Loss:2108.1733398\tKL Div:8.7847986\n",
      "Epoch 818\tIter   18\tLoss:2109.1203613\tRec Loss:2100.1960449\tKL Div:8.9242878\n",
      "Epoch 819\tIter    6\tLoss:2155.7087402\tRec Loss:2146.5532227\tKL Div:9.1555614\n",
      "Epoch 819\tIter   12\tLoss:2137.0131836\tRec Loss:2127.9960938\tKL Div:9.0171890\n",
      "Epoch 819\tIter   18\tLoss:2086.9858398\tRec Loss:2078.1645508\tKL Div:8.8213100\n",
      "Epoch 820\tIter    6\tLoss:2136.8300781\tRec Loss:2127.9846191\tKL Div:8.8454618\n",
      "Epoch 820\tIter   12\tLoss:2144.4201660\tRec Loss:2135.3144531\tKL Div:9.1057148\n",
      "Epoch 820\tIter   18\tLoss:2128.8950195\tRec Loss:2119.8579102\tKL Div:9.0371408\n",
      "Epoch 821\tIter    6\tLoss:2127.5239258\tRec Loss:2118.3889160\tKL Div:9.1350117\n",
      "Epoch 821\tIter   12\tLoss:2132.2458496\tRec Loss:2123.3125000\tKL Div:8.9332695\n",
      "Epoch 821\tIter   18\tLoss:2140.6369629\tRec Loss:2131.4833984\tKL Div:9.1534615\n",
      "Epoch 822\tIter    6\tLoss:2144.4401855\tRec Loss:2135.4023438\tKL Div:9.0377617\n",
      "Epoch 822\tIter   12\tLoss:2115.4206543\tRec Loss:2106.4697266\tKL Div:8.9509773\n",
      "Epoch 822\tIter   18\tLoss:2105.5480957\tRec Loss:2096.6491699\tKL Div:8.8990250\n",
      "Epoch 823\tIter    6\tLoss:2165.8464355\tRec Loss:2156.7077637\tKL Div:9.1386433\n",
      "Epoch 823\tIter   12\tLoss:2106.4790039\tRec Loss:2097.5358887\tKL Div:8.9431820\n",
      "Epoch 823\tIter   18\tLoss:2129.7915039\tRec Loss:2120.9575195\tKL Div:8.8340158\n",
      "Epoch 824\tIter    6\tLoss:2055.3908691\tRec Loss:2046.4290771\tKL Div:8.9616909\n",
      "Epoch 824\tIter   12\tLoss:2108.0537109\tRec Loss:2099.1630859\tKL Div:8.8906918\n",
      "Epoch 824\tIter   18\tLoss:2177.3522949\tRec Loss:2168.2978516\tKL Div:9.0544224\n",
      "Epoch 825\tIter    6\tLoss:2129.7880859\tRec Loss:2120.7280273\tKL Div:9.0599689\n",
      "Epoch 825\tIter   12\tLoss:2142.9558105\tRec Loss:2134.0278320\tKL Div:8.9279203\n",
      "Epoch 825\tIter   18\tLoss:2183.6564941\tRec Loss:2174.5537109\tKL Div:9.1027966\n",
      "Epoch 826\tIter    6\tLoss:2149.4016113\tRec Loss:2140.2824707\tKL Div:9.1191978\n",
      "Epoch 826\tIter   12\tLoss:2108.0571289\tRec Loss:2099.1752930\tKL Div:8.8819122\n",
      "Epoch 826\tIter   18\tLoss:2113.9438477\tRec Loss:2104.8305664\tKL Div:9.1133356\n",
      "Epoch 827\tIter    6\tLoss:2114.5876465\tRec Loss:2105.8041992\tKL Div:8.7834873\n",
      "Epoch 827\tIter   12\tLoss:2085.0209961\tRec Loss:2075.8957520\tKL Div:9.1253128\n",
      "Epoch 827\tIter   18\tLoss:2135.3920898\tRec Loss:2126.6833496\tKL Div:8.7087440\n",
      "Epoch 828\tIter    6\tLoss:2117.7951660\tRec Loss:2108.5383301\tKL Div:9.2568541\n",
      "Epoch 828\tIter   12\tLoss:2150.9885254\tRec Loss:2142.0778809\tKL Div:8.9107466\n",
      "Epoch 828\tIter   18\tLoss:2147.9501953\tRec Loss:2138.9287109\tKL Div:9.0216064\n",
      "Epoch 829\tIter    6\tLoss:2094.0744629\tRec Loss:2085.1181641\tKL Div:8.9563084\n",
      "Epoch 829\tIter   12\tLoss:2182.5766602\tRec Loss:2173.5156250\tKL Div:9.0610962\n",
      "Epoch 829\tIter   18\tLoss:2177.4033203\tRec Loss:2168.1940918\tKL Div:9.2093430\n",
      "Epoch 830\tIter    6\tLoss:2150.1340332\tRec Loss:2141.1342773\tKL Div:8.9996405\n",
      "Epoch 830\tIter   12\tLoss:2150.1674805\tRec Loss:2141.0371094\tKL Div:9.1304684\n",
      "Epoch 830\tIter   18\tLoss:2130.1982422\tRec Loss:2121.2519531\tKL Div:8.9462614\n",
      "Epoch 831\tIter    6\tLoss:2102.9138184\tRec Loss:2093.9375000\tKL Div:8.9762430\n",
      "Epoch 831\tIter   12\tLoss:2169.9077148\tRec Loss:2160.9506836\tKL Div:8.9570103\n",
      "Epoch 831\tIter   18\tLoss:2150.8386230\tRec Loss:2141.7673340\tKL Div:9.0711899\n",
      "Epoch 832\tIter    6\tLoss:2115.8430176\tRec Loss:2106.8012695\tKL Div:9.0418320\n",
      "Epoch 832\tIter   12\tLoss:2151.4848633\tRec Loss:2142.4797363\tKL Div:9.0052090\n",
      "Epoch 832\tIter   18\tLoss:2108.5734863\tRec Loss:2099.5830078\tKL Div:8.9905443\n",
      "Epoch 833\tIter    6\tLoss:2089.5668945\tRec Loss:2080.7783203\tKL Div:8.7885513\n",
      "Epoch 833\tIter   12\tLoss:2137.7561035\tRec Loss:2128.6301270\tKL Div:9.1260881\n",
      "Epoch 833\tIter   18\tLoss:2130.2395020\tRec Loss:2121.2731934\tKL Div:8.9661903\n",
      "Epoch 834\tIter    6\tLoss:2118.4123535\tRec Loss:2109.3125000\tKL Div:9.0999165\n",
      "Epoch 834\tIter   12\tLoss:2113.7766113\tRec Loss:2104.9135742\tKL Div:8.8630791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 834\tIter   18\tLoss:2125.5590820\tRec Loss:2116.5654297\tKL Div:8.9935570\n",
      "Epoch 835\tIter    6\tLoss:2161.9257812\tRec Loss:2152.9416504\tKL Div:8.9842281\n",
      "Epoch 835\tIter   12\tLoss:2121.2402344\tRec Loss:2112.2983398\tKL Div:8.9418087\n",
      "Epoch 835\tIter   18\tLoss:2124.3916016\tRec Loss:2115.3125000\tKL Div:9.0791311\n",
      "Epoch 836\tIter    6\tLoss:2119.4196777\tRec Loss:2110.5380859\tKL Div:8.8815136\n",
      "Epoch 836\tIter   12\tLoss:2068.0307617\tRec Loss:2059.1350098\tKL Div:8.8958158\n",
      "Epoch 836\tIter   18\tLoss:2198.5913086\tRec Loss:2189.5568848\tKL Div:9.0343266\n",
      "Epoch 837\tIter    6\tLoss:2109.2431641\tRec Loss:2100.2373047\tKL Div:9.0058727\n",
      "Epoch 837\tIter   12\tLoss:2117.3271484\tRec Loss:2108.4057617\tKL Div:8.9214115\n",
      "Epoch 837\tIter   18\tLoss:2110.9228516\tRec Loss:2101.9179688\tKL Div:9.0047779\n",
      "Epoch 838\tIter    6\tLoss:2164.4096680\tRec Loss:2155.2871094\tKL Div:9.1224432\n",
      "Epoch 838\tIter   12\tLoss:2117.1088867\tRec Loss:2108.1982422\tKL Div:8.9105787\n",
      "Epoch 838\tIter   18\tLoss:2119.3527832\tRec Loss:2110.3735352\tKL Div:8.9792614\n",
      "Epoch 839\tIter    6\tLoss:2082.5458984\tRec Loss:2073.6298828\tKL Div:8.9161377\n",
      "Epoch 839\tIter   12\tLoss:2167.3942871\tRec Loss:2158.1774902\tKL Div:9.2167912\n",
      "Epoch 839\tIter   18\tLoss:2144.5693359\tRec Loss:2135.5805664\tKL Div:8.9887867\n",
      "Epoch 840\tIter    6\tLoss:2053.3339844\tRec Loss:2044.3515625\tKL Div:8.9824028\n",
      "Epoch 840\tIter   12\tLoss:2121.4013672\tRec Loss:2112.4619141\tKL Div:8.9393425\n",
      "Epoch 840\tIter   18\tLoss:2189.1530762\tRec Loss:2180.0412598\tKL Div:9.1117191\n",
      "Epoch 840\tLoss:2129.9897288\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   196,  1114,   235, 13238,   573,    97,   131,\n",
      "          526,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'piece', 'tone', 'sense', 'space', 'ambient', 'build', 'melody', 'close', 'piano', 'electronic', 'drum', 'light', 'open']\n",
      "['love', 'melody', 'indie', 'lyric', 'piano', 'arrangement', 'debut', 'harmony', 'group', 'bit', 'chorus', 'string', 'line', 'write', 'pretty']\n",
      "['live', 'version', 'disc', 'set', 'include', 'record', 'soul', 'cover', 'recording', 'studio', 'label', 'original', 'compilation', 'group', 'early']\n",
      "['bad', 'man', 'guy', 'listen', 'lyric', 'people', 'day', 'get', 'love', 'kid', 'let', 'girl', 'pretty', '\\n', 'line']\n",
      "['house', 'techno', 'label', 'dance', 'mix', 'producer', 'bass', 'beat', 'dj', 'artist', 'synth', 'club', 'drum', 'set', 'rhythm']\n",
      "['dance', 'disco', 'synth', 'single', 'house', 'love', 'big', 'remix', 'group', 'electro', 'duo', 'club', 'turn', 'act', 'indie']\n",
      "['\\xa0', 'project', 'point', 'artist', 'year', 'title', 'synth', 'solo', 'early', 'place', 'close', 'ep', 'start', 'producer', 'feature']\n",
      "['melody', 'debut', 'riff', 'drum', 'band', 'chorus', 'punk', 'hook', 'line', 'group', 'bass', 'rhythm', 'opener', 'ep', 'hard']\n",
      "['love', 'sing', 'life', 'lyric', 'world', 'word', 'line', 'write', 'close', 'feeling', 'leave', 'death', 'light', 'sense', 'kind']\n",
      "['punk', 'live', 'single', 'love', 'cover', 'band', 'early', 'solo', 'smith', 'set', 'day', 'classic', 'group', 'pollard', 'fan']\n",
      "['electronic', 'drum', 'jazz', 'noise', 'melody', 'bass', 'beat', 'begin', 'disc', 'piece', 'feature', 'live', 'interesting', 'group', 'bit']\n",
      "['life', 'man', 'world', 'people', 'black', 'woman', 'love', 'write', 'political', 'white', 'word', 'call', 'story', 'live', 'year']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'dylan', 'man', 'artist', 'oldham', 'acoustic', 'lyric', 'american', 'record', 'love']\n",
      "['hip_hop', 'beat', 'sample', 'funk', 'mix', 'party', 'break', 'dj', 'rap', 'genre', 'bass', 'dance', 'people', 'style', 'bit']\n",
      "['piece', 'composer', 'jazz', 'film', 'piano', 'composition', 'musician', 'world', 'instrument', 'recording', 'group', 'electronic', 'score', 'create', 'string']\n",
      "['rap', 'rapper', 'beat', 'mixtape', 'hip_hop', 'verse', 'production', 'year', 'producer', 'flow', 'line', 'feature', 'style', 'big', 'artist']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'artist', 'debut', 'sense', 'instrumental', 'production', 'project', 'drum', 'build', 'kind']\n",
      "['lyric', 'kind', 'title', 'people', 'band', 'love', 'indie_rock', 'point', 'sort', 'emo', 'life', 'indie', 'chorus', 'punk', 'guy']\n",
      "['metal', 'riff', 'hardcore', 'black_metal', 'punk', 'band', 'noise', 'drum', 'death', 'heavy', 'doom', 'bass', 'early', 'black', 'close']\n",
      "['love', 'r&b', 'artist', 'single', 'star', 'singer', 'hit', 'year', 'producer', 'debut', 'girl', 'prince', 'big', 'soul', 'drake']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   196,  1114,   235, 13238,   573,    97,   131,\n",
      "          526,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   196,  1114,   235, 13238,   573,    97,   131,\n",
      "          526,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5066666666666667\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "piece\n",
      "tone\n",
      "sense\n",
      "space\n",
      "ambient\n",
      "build\n",
      "melody\n",
      "close\n",
      "piano\n",
      "electronic\n",
      "drum\n",
      "light\n",
      "open\n",
      "coherence c_uci -0.07917836851001221\n",
      "coherence c_npmi 0.011101335249869377\n",
      "coherence c_cv 0.3870415436374082\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,   196,  1114,   235, 13238,   573,    97,   131,\n",
      "          526,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 841\tIter    6\tLoss:2111.8137207\tRec Loss:2102.8540039\tKL Div:8.9597321\n",
      "Epoch 841\tIter   12\tLoss:2135.5285645\tRec Loss:2126.4565430\tKL Div:9.0720787\n",
      "Epoch 841\tIter   18\tLoss:2153.4621582\tRec Loss:2144.5290527\tKL Div:8.9331627\n",
      "Epoch 842\tIter    6\tLoss:2089.1162109\tRec Loss:2080.1154785\tKL Div:9.0006561\n",
      "Epoch 842\tIter   12\tLoss:2160.8088379\tRec Loss:2151.7919922\tKL Div:9.0168114\n",
      "Epoch 842\tIter   18\tLoss:2134.6025391\tRec Loss:2125.7404785\tKL Div:8.8621120\n",
      "Epoch 843\tIter    6\tLoss:2150.8627930\tRec Loss:2141.7194824\tKL Div:9.1434250\n",
      "Epoch 843\tIter   12\tLoss:2089.4396973\tRec Loss:2080.5126953\tKL Div:8.9269848\n",
      "Epoch 843\tIter   18\tLoss:2132.9575195\tRec Loss:2123.8120117\tKL Div:9.1455002\n",
      "Epoch 844\tIter    6\tLoss:2128.4909668\tRec Loss:2119.5903320\tKL Div:8.9005213\n",
      "Epoch 844\tIter   12\tLoss:2074.6398926\tRec Loss:2065.5839844\tKL Div:9.0559273\n",
      "Epoch 844\tIter   18\tLoss:2166.7277832\tRec Loss:2157.7568359\tKL Div:8.9710407\n",
      "Epoch 845\tIter    6\tLoss:2097.5041504\tRec Loss:2088.4350586\tKL Div:9.0691566\n",
      "Epoch 845\tIter   12\tLoss:2137.7556152\tRec Loss:2128.8662109\tKL Div:8.8894663\n",
      "Epoch 845\tIter   18\tLoss:2122.1992188\tRec Loss:2113.0397949\tKL Div:9.1593075\n",
      "Epoch 846\tIter    6\tLoss:2117.2749023\tRec Loss:2108.2968750\tKL Div:8.9779177\n",
      "Epoch 846\tIter   12\tLoss:2114.6044922\tRec Loss:2105.6328125\tKL Div:8.9717836\n",
      "Epoch 846\tIter   18\tLoss:2157.9023438\tRec Loss:2148.9953613\tKL Div:8.9069290\n",
      "Epoch 847\tIter    6\tLoss:2101.6979980\tRec Loss:2092.7170410\tKL Div:8.9809399\n",
      "Epoch 847\tIter   12\tLoss:2140.9245605\tRec Loss:2131.9233398\tKL Div:9.0011959\n",
      "Epoch 847\tIter   18\tLoss:2102.8632812\tRec Loss:2093.9125977\tKL Div:8.9508057\n",
      "Epoch 848\tIter    6\tLoss:2105.8447266\tRec Loss:2096.8422852\tKL Div:9.0024166\n",
      "Epoch 848\tIter   12\tLoss:2128.3732910\tRec Loss:2119.3759766\tKL Div:8.9973068\n",
      "Epoch 848\tIter   18\tLoss:2154.8203125\tRec Loss:2145.7243652\tKL Div:9.0959492\n",
      "Epoch 849\tIter    6\tLoss:2123.9492188\tRec Loss:2114.9831543\tKL Div:8.9659958\n",
      "Epoch 849\tIter   12\tLoss:2115.4204102\tRec Loss:2106.5161133\tKL Div:8.9042101\n",
      "Epoch 849\tIter   18\tLoss:2125.2485352\tRec Loss:2116.2836914\tKL Div:8.9648075\n",
      "Epoch 850\tIter    6\tLoss:2075.3847656\tRec Loss:2066.4201660\tKL Div:8.9644785\n",
      "Epoch 850\tIter   12\tLoss:2099.5749512\tRec Loss:2090.6914062\tKL Div:8.8834944\n",
      "Epoch 850\tIter   18\tLoss:2106.8540039\tRec Loss:2097.9482422\tKL Div:8.9058180\n",
      "Epoch 851\tIter    6\tLoss:2148.8686523\tRec Loss:2139.8066406\tKL Div:9.0620394\n",
      "Epoch 851\tIter   12\tLoss:2116.0380859\tRec Loss:2107.1833496\tKL Div:8.8546143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 851\tIter   18\tLoss:2134.9511719\tRec Loss:2125.8701172\tKL Div:9.0810852\n",
      "Epoch 852\tIter    6\tLoss:2132.4611816\tRec Loss:2123.4191895\tKL Div:9.0419760\n",
      "Epoch 852\tIter   12\tLoss:2143.9687500\tRec Loss:2135.0270996\tKL Div:8.9415874\n",
      "Epoch 852\tIter   18\tLoss:2121.1569824\tRec Loss:2112.1738281\tKL Div:8.9830894\n",
      "Epoch 853\tIter    6\tLoss:2084.6867676\tRec Loss:2075.6918945\tKL Div:8.9948483\n",
      "Epoch 853\tIter   12\tLoss:2143.9965820\tRec Loss:2134.9240723\tKL Div:9.0726213\n",
      "Epoch 853\tIter   18\tLoss:2105.3535156\tRec Loss:2096.3732910\tKL Div:8.9802818\n",
      "Epoch 854\tIter    6\tLoss:2100.7165527\tRec Loss:2091.8569336\tKL Div:8.8595161\n",
      "Epoch 854\tIter   12\tLoss:2182.9055176\tRec Loss:2173.8452148\tKL Div:9.0603552\n",
      "Epoch 854\tIter   18\tLoss:2108.6022949\tRec Loss:2099.5947266\tKL Div:9.0075111\n",
      "Epoch 855\tIter    6\tLoss:2085.4746094\tRec Loss:2076.5207520\tKL Div:8.9537973\n",
      "Epoch 855\tIter   12\tLoss:2088.9553223\tRec Loss:2080.1213379\tKL Div:8.8340683\n",
      "Epoch 855\tIter   18\tLoss:2151.2810059\tRec Loss:2142.2231445\tKL Div:9.0577450\n",
      "Epoch 856\tIter    6\tLoss:2081.1108398\tRec Loss:2072.1308594\tKL Div:8.9800911\n",
      "Epoch 856\tIter   12\tLoss:2093.8332520\tRec Loss:2084.8652344\tKL Div:8.9680309\n",
      "Epoch 856\tIter   18\tLoss:2143.4301758\tRec Loss:2134.4636230\tKL Div:8.9664726\n",
      "Epoch 857\tIter    6\tLoss:2083.1523438\tRec Loss:2074.2011719\tKL Div:8.9511890\n",
      "Epoch 857\tIter   12\tLoss:2150.7702637\tRec Loss:2141.7766113\tKL Div:8.9937592\n",
      "Epoch 857\tIter   18\tLoss:2137.5747070\tRec Loss:2128.5615234\tKL Div:9.0133057\n",
      "Epoch 858\tIter    6\tLoss:2080.3000488\tRec Loss:2071.4707031\tKL Div:8.8292532\n",
      "Epoch 858\tIter   12\tLoss:2140.6020508\tRec Loss:2131.4860840\tKL Div:9.1160021\n",
      "Epoch 858\tIter   18\tLoss:2138.3879395\tRec Loss:2129.4428711\tKL Div:8.9451351\n",
      "Epoch 859\tIter    6\tLoss:2153.6721191\tRec Loss:2144.7714844\tKL Div:8.9007339\n",
      "Epoch 859\tIter   12\tLoss:2124.8107910\tRec Loss:2115.8525391\tKL Div:8.9583654\n",
      "Epoch 859\tIter   18\tLoss:2170.1616211\tRec Loss:2161.2268066\tKL Div:8.9347000\n",
      "Epoch 860\tIter    6\tLoss:2102.6975098\tRec Loss:2093.7324219\tKL Div:8.9649706\n",
      "Epoch 860\tIter   12\tLoss:2152.9953613\tRec Loss:2144.0175781\tKL Div:8.9778309\n",
      "Epoch 860\tIter   18\tLoss:2147.9396973\tRec Loss:2138.8232422\tKL Div:9.1164799\n",
      "Epoch 861\tIter    6\tLoss:2165.0942383\tRec Loss:2156.1279297\tKL Div:8.9662418\n",
      "Epoch 861\tIter   12\tLoss:2141.2297363\tRec Loss:2132.0485840\tKL Div:9.1811476\n",
      "Epoch 861\tIter   18\tLoss:2081.2504883\tRec Loss:2072.4299316\tKL Div:8.8206291\n",
      "Epoch 862\tIter    6\tLoss:2124.9494629\tRec Loss:2115.9313965\tKL Div:9.0180159\n",
      "Epoch 862\tIter   12\tLoss:2132.8933105\tRec Loss:2123.9228516\tKL Div:8.9705486\n",
      "Epoch 862\tIter   18\tLoss:2113.0322266\tRec Loss:2104.0761719\tKL Div:8.9561367\n",
      "Epoch 863\tIter    6\tLoss:2177.0859375\tRec Loss:2167.8312988\tKL Div:9.2545681\n",
      "Epoch 863\tIter   12\tLoss:2113.7668457\tRec Loss:2104.9316406\tKL Div:8.8352299\n",
      "Epoch 863\tIter   18\tLoss:2141.6748047\tRec Loss:2132.5781250\tKL Div:9.0966492\n",
      "Epoch 864\tIter    6\tLoss:2116.9326172\tRec Loss:2107.9768066\tKL Div:8.9558592\n",
      "Epoch 864\tIter   12\tLoss:2111.7556152\tRec Loss:2102.7375488\tKL Div:9.0179873\n",
      "Epoch 864\tIter   18\tLoss:2105.3999023\tRec Loss:2096.4062500\tKL Div:8.9935455\n",
      "Epoch 865\tIter    6\tLoss:2097.6245117\tRec Loss:2088.6518555\tKL Div:8.9726410\n",
      "Epoch 865\tIter   12\tLoss:2136.2922363\tRec Loss:2127.2592773\tKL Div:9.0330524\n",
      "Epoch 865\tIter   18\tLoss:2124.3679199\tRec Loss:2115.3344727\tKL Div:9.0333319\n",
      "Epoch 866\tIter    6\tLoss:2128.7536621\tRec Loss:2119.8493652\tKL Div:8.9043217\n",
      "Epoch 866\tIter   12\tLoss:2146.6936035\tRec Loss:2137.7687988\tKL Div:8.9247427\n",
      "Epoch 866\tIter   18\tLoss:2096.3837891\tRec Loss:2087.4414062\tKL Div:8.9424210\n",
      "Epoch 867\tIter    6\tLoss:2096.7609863\tRec Loss:2087.7446289\tKL Div:9.0164080\n",
      "Epoch 867\tIter   12\tLoss:2150.6523438\tRec Loss:2141.7290039\tKL Div:8.9234390\n",
      "Epoch 867\tIter   18\tLoss:2159.1882324\tRec Loss:2150.1440430\tKL Div:9.0442209\n",
      "Epoch 868\tIter    6\tLoss:2121.0341797\tRec Loss:2112.0852051\tKL Div:8.9490232\n",
      "Epoch 868\tIter   12\tLoss:2106.3420410\tRec Loss:2097.3608398\tKL Div:8.9811678\n",
      "Epoch 868\tIter   18\tLoss:2107.2348633\tRec Loss:2098.2429199\tKL Div:8.9918480\n",
      "Epoch 869\tIter    6\tLoss:2131.2426758\tRec Loss:2122.3100586\tKL Div:8.9326267\n",
      "Epoch 869\tIter   12\tLoss:2109.6867676\tRec Loss:2100.5688477\tKL Div:9.1178741\n",
      "Epoch 869\tIter   18\tLoss:2133.2143555\tRec Loss:2124.2099609\tKL Div:9.0043354\n",
      "Epoch 870\tIter    6\tLoss:2094.3220215\tRec Loss:2085.1066895\tKL Div:9.2152958\n",
      "Epoch 870\tIter   12\tLoss:2154.4147949\tRec Loss:2145.4780273\tKL Div:8.9367800\n",
      "Epoch 870\tIter   18\tLoss:2125.4443359\tRec Loss:2116.3481445\tKL Div:9.0960712\n",
      "Epoch 871\tIter    6\tLoss:2096.4206543\tRec Loss:2087.4291992\tKL Div:8.9913702\n",
      "Epoch 871\tIter   12\tLoss:2091.4118652\tRec Loss:2082.4785156\tKL Div:8.9332561\n",
      "Epoch 871\tIter   18\tLoss:2097.4262695\tRec Loss:2088.4633789\tKL Div:8.9629478\n",
      "Epoch 872\tIter    6\tLoss:2145.6755371\tRec Loss:2136.6894531\tKL Div:8.9860744\n",
      "Epoch 872\tIter   12\tLoss:2136.0180664\tRec Loss:2127.1250000\tKL Div:8.8930054\n",
      "Epoch 872\tIter   18\tLoss:2108.2697754\tRec Loss:2099.2778320\tKL Div:8.9918985\n",
      "Epoch 873\tIter    6\tLoss:2129.3994141\tRec Loss:2120.5205078\tKL Div:8.8788137\n",
      "Epoch 873\tIter   12\tLoss:2142.9860840\tRec Loss:2133.9375000\tKL Div:9.0485458\n",
      "Epoch 873\tIter   18\tLoss:2145.6479492\tRec Loss:2136.5883789\tKL Div:9.0595560\n",
      "Epoch 874\tIter    6\tLoss:2129.2673340\tRec Loss:2120.1972656\tKL Div:9.0700092\n",
      "Epoch 874\tIter   12\tLoss:2102.5607910\tRec Loss:2093.5883789\tKL Div:8.9724331\n",
      "Epoch 874\tIter   18\tLoss:2162.4975586\tRec Loss:2153.4921875\tKL Div:9.0053110\n",
      "Epoch 875\tIter    6\tLoss:2098.7211914\tRec Loss:2089.6362305\tKL Div:9.0850058\n",
      "Epoch 875\tIter   12\tLoss:2127.7290039\tRec Loss:2118.7136230\tKL Div:9.0153093\n",
      "Epoch 875\tIter   18\tLoss:2089.9865723\tRec Loss:2081.2233887\tKL Div:8.7632504\n",
      "Epoch 876\tIter    6\tLoss:2149.1655273\tRec Loss:2140.0517578\tKL Div:9.1138477\n",
      "Epoch 876\tIter   12\tLoss:2094.1955566\tRec Loss:2085.3564453\tKL Div:8.8391285\n",
      "Epoch 876\tIter   18\tLoss:2123.1831055\tRec Loss:2114.2836914\tKL Div:8.8992920\n",
      "Epoch 877\tIter    6\tLoss:2083.1909180\tRec Loss:2074.1198730\tKL Div:9.0710354\n",
      "Epoch 877\tIter   12\tLoss:2126.9907227\tRec Loss:2118.0571289\tKL Div:8.9336128\n",
      "Epoch 877\tIter   18\tLoss:2127.2712402\tRec Loss:2118.1928711\tKL Div:9.0784416\n",
      "Epoch 878\tIter    6\tLoss:2123.8125000\tRec Loss:2114.8610840\tKL Div:8.9513330\n",
      "Epoch 878\tIter   12\tLoss:2113.7663574\tRec Loss:2104.7214355\tKL Div:9.0448017\n",
      "Epoch 878\tIter   18\tLoss:2114.8811035\tRec Loss:2106.0747070\tKL Div:8.8063822\n",
      "Epoch 879\tIter    6\tLoss:2126.5925293\tRec Loss:2117.5129395\tKL Div:9.0796776\n",
      "Epoch 879\tIter   12\tLoss:2113.5109863\tRec Loss:2104.6831055\tKL Div:8.8279648\n",
      "Epoch 879\tIter   18\tLoss:2121.1188965\tRec Loss:2112.0263672\tKL Div:9.0925446\n",
      "Epoch 880\tIter    6\tLoss:2142.4890137\tRec Loss:2133.3779297\tKL Div:9.1110497\n",
      "Epoch 880\tIter   12\tLoss:2129.3466797\tRec Loss:2120.4294434\tKL Div:8.9172411\n",
      "Epoch 880\tIter   18\tLoss:2096.8146973\tRec Loss:2087.8627930\tKL Div:8.9519367\n",
      "Epoch 880\tLoss:2126.6377742\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,  2218,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'tone', 'piece', 'sense', 'space', 'build', 'melody', 'ambient', 'close', 'piano', 'electronic', 'note', 'light', 'open']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'piano', 'debut', 'group', 'harmony', 'chorus', 'string', 'bit', 'write', 'line', 'solo']\n",
      "['live', 'version', 'disc', 'set', 'include', 'record', 'label', 'cover', 'soul', 'recording', 'studio', 'compilation', 'original', 'group', 'early']\n",
      "['guy', 'bad', 'man', 'lyric', 'listen', 'get', 'day', 'kid', 'people', 'love', 'girl', 'let', '\\n', 'pretty', 'kind']\n",
      "['house', 'techno', 'label', 'dance', 'mix', 'producer', 'bass', 'beat', 'dj', 'artist', 'synth', 'club', 'set', 'drum', 'rhythm']\n",
      "['dance', 'disco', 'synth', 'single', 'house', 'love', 'big', 'remix', 'group', 'electro', 'duo', 'indie', 'club', 'turn', 'act']\n",
      "['\\xa0', 'project', 'point', 'artist', 'year', 'solo', 'title', 'early', 'synth', 'ep', 'place', 'close', 'producer', 'lyric', 'start']\n",
      "['melody', 'debut', 'riff', 'drum', 'band', 'chorus', 'group', 'hook', 'line', 'punk', 'bass', 'ep', 'rhythm', 'hard', 'opener']\n",
      "['love', 'sing', 'life', 'lyric', 'world', 'line', 'word', 'write', 'close', 'feeling', 'leave', 'death', 'sense', 'light', 'kind']\n",
      "['punk', 'live', 'single', 'love', 'band', 'cover', 'early', 'solo', 'smith', 'day', 'set', 'group', 'fan', 'classic', 'big']\n",
      "['drum', 'electronic', 'melody', 'noise', 'bass', 'beat', 'jazz', 'piece', 'begin', 'interesting', 'disc', 'feature', 'group', 'live', 'tune']\n",
      "['life', 'man', 'world', 'people', 'black', 'woman', 'love', 'write', 'political', 'story', 'word', 'white', 'call', 'america', 'live']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'dylan', 'man', 'record', 'lyric', 'artist', 'american', 'acoustic', 'write', 'oldham']\n",
      "['beat', 'hip_hop', 'sample', 'funk', 'mix', 'rap', 'party', 'break', 'bass', 'dj', 'soul', 'genre', 'style', 'big', 'bit']\n",
      "['piece', 'jazz', 'film', 'piano', 'composer', 'composition', 'musician', 'world', 'instrument', 'recording', 'group', 'electronic', 'solo', 'score', 'string']\n",
      "['rap', 'rapper', 'beat', 'mixtape', 'verse', 'hip_hop', 'production', 'year', 'flow', 'producer', 'line', 'feature', 'big', 'style', 'artist']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'debut', 'sense', 'artist', 'production', 'instrumental', 'project', 'drum', 'kind', 'build']\n",
      "['lyric', 'title', 'kind', 'band', 'people', 'love', 'point', 'emo', 'sort', 'indie_rock', 'life', 'indie', 'chorus', 'punk', 'hook']\n",
      "['metal', 'riff', 'hardcore', 'black_metal', 'band', 'death', 'noise', 'punk', 'drum', 'heavy', 'doom', 'year', 'black', 'early', 'close']\n",
      "['love', 'r&b', 'artist', 'hit', 'singer', 'single', 'star', 'year', 'producer', 'girl', 'debut', 'big', 'prince', 'drake', 'production']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,  2218,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,  2218,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.51\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "tone\n",
      "piece\n",
      "sense\n",
      "space\n",
      "build\n",
      "melody\n",
      "ambient\n",
      "close\n",
      "piano\n",
      "electronic\n",
      "note\n",
      "light\n",
      "open\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence c_uci -0.026937966832673944\n",
      "coherence c_npmi 0.012968598688221816\n",
      "coherence c_cv 0.386699468433555\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,    95,  2218,  1114,   235, 13238,   573,    97,   131,\n",
      "          254,   317,  1654,   823,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 881\tIter    6\tLoss:2146.0065918\tRec Loss:2136.9497070\tKL Div:9.0569210\n",
      "Epoch 881\tIter   12\tLoss:2147.4416504\tRec Loss:2138.3847656\tKL Div:9.0568695\n",
      "Epoch 881\tIter   18\tLoss:2121.7482910\tRec Loss:2112.6616211\tKL Div:9.0866680\n",
      "Epoch 882\tIter    6\tLoss:2155.1293945\tRec Loss:2146.1794434\tKL Div:8.9500141\n",
      "Epoch 882\tIter   12\tLoss:2128.8925781\tRec Loss:2119.7502441\tKL Div:9.1422482\n",
      "Epoch 882\tIter   18\tLoss:2145.7756348\tRec Loss:2136.8315430\tKL Div:8.9439983\n",
      "Epoch 883\tIter    6\tLoss:2100.0227051\tRec Loss:2091.0034180\tKL Div:9.0193710\n",
      "Epoch 883\tIter   12\tLoss:2113.3190918\tRec Loss:2104.4416504\tKL Div:8.8775282\n",
      "Epoch 883\tIter   18\tLoss:2120.5476074\tRec Loss:2111.3591309\tKL Div:9.1884270\n",
      "Epoch 884\tIter    6\tLoss:2195.3234863\tRec Loss:2186.3090820\tKL Div:9.0143166\n",
      "Epoch 884\tIter   12\tLoss:2154.5319824\tRec Loss:2145.4208984\tKL Div:9.1111202\n",
      "Epoch 884\tIter   18\tLoss:2132.2414551\tRec Loss:2123.2902832\tKL Div:8.9511414\n",
      "Epoch 885\tIter    6\tLoss:2089.7951660\tRec Loss:2080.7531738\tKL Div:9.0420933\n",
      "Epoch 885\tIter   12\tLoss:2102.0671387\tRec Loss:2093.0502930\tKL Div:9.0167789\n",
      "Epoch 885\tIter   18\tLoss:2140.7758789\tRec Loss:2131.7568359\tKL Div:9.0191011\n",
      "Epoch 886\tIter    6\tLoss:2106.3991699\tRec Loss:2097.4487305\tKL Div:8.9504204\n",
      "Epoch 886\tIter   12\tLoss:2141.0048828\tRec Loss:2131.8940430\tKL Div:9.1107521\n",
      "Epoch 886\tIter   18\tLoss:2114.9553223\tRec Loss:2106.0747070\tKL Div:8.8806572\n",
      "Epoch 887\tIter    6\tLoss:2100.8774414\tRec Loss:2091.8564453\tKL Div:9.0210543\n",
      "Epoch 887\tIter   12\tLoss:2111.4455566\tRec Loss:2102.5498047\tKL Div:8.8957253\n",
      "Epoch 887\tIter   18\tLoss:2174.9753418\tRec Loss:2165.9323730\tKL Div:9.0429630\n",
      "Epoch 888\tIter    6\tLoss:2147.1862793\tRec Loss:2138.1799316\tKL Div:9.0062466\n",
      "Epoch 888\tIter   12\tLoss:2105.9235840\tRec Loss:2097.0441895\tKL Div:8.8793907\n",
      "Epoch 888\tIter   18\tLoss:2123.2192383\tRec Loss:2114.2265625\tKL Div:8.9926672\n",
      "Epoch 889\tIter    6\tLoss:2145.0070801\tRec Loss:2135.9050293\tKL Div:9.1019821\n",
      "Epoch 889\tIter   12\tLoss:2172.8156738\tRec Loss:2163.7160645\tKL Div:9.0995865\n",
      "Epoch 889\tIter   18\tLoss:2158.6962891\tRec Loss:2149.6296387\tKL Div:9.0667362\n",
      "Epoch 890\tIter    6\tLoss:2079.6633301\tRec Loss:2070.6567383\tKL Div:9.0064907\n",
      "Epoch 890\tIter   12\tLoss:2122.5498047\tRec Loss:2113.7805176\tKL Div:8.7692547\n",
      "Epoch 890\tIter   18\tLoss:2128.4985352\tRec Loss:2119.4672852\tKL Div:9.0311661\n",
      "Epoch 891\tIter    6\tLoss:2128.1147461\tRec Loss:2119.1220703\tKL Div:8.9927101\n",
      "Epoch 891\tIter   12\tLoss:2143.1982422\tRec Loss:2134.0815430\tKL Div:9.1165962\n",
      "Epoch 891\tIter   18\tLoss:2143.5502930\tRec Loss:2134.5092773\tKL Div:9.0410080\n",
      "Epoch 892\tIter    6\tLoss:2115.5048828\tRec Loss:2106.4812012\tKL Div:9.0237665\n",
      "Epoch 892\tIter   12\tLoss:2169.4545898\tRec Loss:2160.3461914\tKL Div:9.1084547\n",
      "Epoch 892\tIter   18\tLoss:2136.5681152\tRec Loss:2127.6313477\tKL Div:8.9367952\n",
      "Epoch 893\tIter    6\tLoss:2113.9440918\tRec Loss:2104.9699707\tKL Div:8.9742413\n",
      "Epoch 893\tIter   12\tLoss:2104.1665039\tRec Loss:2095.3825684\tKL Div:8.7840424\n",
      "Epoch 893\tIter   18\tLoss:2168.2358398\tRec Loss:2159.0844727\tKL Div:9.1513281\n",
      "Epoch 894\tIter    6\tLoss:2115.1525879\tRec Loss:2106.1997070\tKL Div:8.9529333\n",
      "Epoch 894\tIter   12\tLoss:2124.4060059\tRec Loss:2115.3774414\tKL Div:9.0286627\n",
      "Epoch 894\tIter   18\tLoss:2110.9133301\tRec Loss:2101.9907227\tKL Div:8.9226389\n",
      "Epoch 895\tIter    6\tLoss:2141.9262695\tRec Loss:2132.8298340\tKL Div:9.0964069\n",
      "Epoch 895\tIter   12\tLoss:2121.8684082\tRec Loss:2112.7663574\tKL Div:9.1019402\n",
      "Epoch 895\tIter   18\tLoss:2142.7258301\tRec Loss:2133.5363770\tKL Div:9.1895247\n",
      "Epoch 896\tIter    6\tLoss:2161.1262207\tRec Loss:2152.0703125\tKL Div:9.0559483\n",
      "Epoch 896\tIter   12\tLoss:2138.5727539\tRec Loss:2129.6350098\tKL Div:8.9376812\n",
      "Epoch 896\tIter   18\tLoss:2134.6362305\tRec Loss:2125.6403809\tKL Div:8.9959545\n",
      "Epoch 897\tIter    6\tLoss:2164.8564453\tRec Loss:2155.8393555\tKL Div:9.0170507\n",
      "Epoch 897\tIter   12\tLoss:2169.4548340\tRec Loss:2160.3549805\tKL Div:9.0998840\n",
      "Epoch 897\tIter   18\tLoss:2087.7001953\tRec Loss:2078.7980957\tKL Div:8.9021244\n",
      "Epoch 898\tIter    6\tLoss:2123.5869141\tRec Loss:2114.5783691\tKL Div:9.0084743\n",
      "Epoch 898\tIter   12\tLoss:2137.0561523\tRec Loss:2128.1618652\tKL Div:8.8943062\n",
      "Epoch 898\tIter   18\tLoss:2152.9096680\tRec Loss:2143.7272949\tKL Div:9.1824932\n",
      "Epoch 899\tIter    6\tLoss:2155.1928711\tRec Loss:2146.3100586\tKL Div:8.8827324\n",
      "Epoch 899\tIter   12\tLoss:2112.1987305\tRec Loss:2103.1035156\tKL Div:9.0951195\n",
      "Epoch 899\tIter   18\tLoss:2119.3959961\tRec Loss:2110.2897949\tKL Div:9.1062489\n",
      "Epoch 900\tIter    6\tLoss:2116.9008789\tRec Loss:2107.8696289\tKL Div:9.0311966\n",
      "Epoch 900\tIter   12\tLoss:2114.8139648\tRec Loss:2105.9941406\tKL Div:8.8197985\n",
      "Epoch 900\tIter   18\tLoss:2164.0483398\tRec Loss:2154.9792480\tKL Div:9.0691862\n",
      "Epoch 901\tIter    6\tLoss:2126.8122559\tRec Loss:2117.7246094\tKL Div:9.0877190\n",
      "Epoch 901\tIter   12\tLoss:2115.6135254\tRec Loss:2106.6298828\tKL Div:8.9836454\n",
      "Epoch 901\tIter   18\tLoss:2125.1225586\tRec Loss:2116.1357422\tKL Div:8.9868279\n",
      "Epoch 902\tIter    6\tLoss:2150.1711426\tRec Loss:2141.1093750\tKL Div:9.0618162\n",
      "Epoch 902\tIter   12\tLoss:2124.1428223\tRec Loss:2115.2209473\tKL Div:8.9219933\n",
      "Epoch 902\tIter   18\tLoss:2126.4960938\tRec Loss:2117.3349609\tKL Div:9.1611176\n",
      "Epoch 903\tIter    6\tLoss:2135.1174316\tRec Loss:2126.1335449\tKL Div:8.9839525\n",
      "Epoch 903\tIter   12\tLoss:2090.0173340\tRec Loss:2081.0541992\tKL Div:8.9632206\n",
      "Epoch 903\tIter   18\tLoss:2169.5825195\tRec Loss:2160.4851074\tKL Div:9.0975189\n",
      "Epoch 904\tIter    6\tLoss:2125.2929688\tRec Loss:2116.2856445\tKL Div:9.0072384\n",
      "Epoch 904\tIter   12\tLoss:2112.3017578\tRec Loss:2103.2851562\tKL Div:9.0165834\n",
      "Epoch 904\tIter   18\tLoss:2101.6406250\tRec Loss:2092.6928711\tKL Div:8.9477062\n",
      "Epoch 905\tIter    6\tLoss:2108.1691895\tRec Loss:2099.2644043\tKL Div:8.9048471\n",
      "Epoch 905\tIter   12\tLoss:2106.8239746\tRec Loss:2097.8447266\tKL Div:8.9791975\n",
      "Epoch 905\tIter   18\tLoss:2123.7387695\tRec Loss:2114.7221680\tKL Div:9.0167112\n",
      "Epoch 906\tIter    6\tLoss:2092.3444824\tRec Loss:2083.2312012\tKL Div:9.1132622\n",
      "Epoch 906\tIter   12\tLoss:2132.3635254\tRec Loss:2123.4206543\tKL Div:8.9429684\n",
      "Epoch 906\tIter   18\tLoss:2137.5666504\tRec Loss:2128.6240234\tKL Div:8.9425287\n",
      "Epoch 907\tIter    6\tLoss:2106.8640137\tRec Loss:2097.9580078\tKL Div:8.9059067\n",
      "Epoch 907\tIter   12\tLoss:2117.4499512\tRec Loss:2108.4350586\tKL Div:9.0148239\n",
      "Epoch 907\tIter   18\tLoss:2121.8034668\tRec Loss:2112.8979492\tKL Div:8.9054546\n",
      "Epoch 908\tIter    6\tLoss:2155.1159668\tRec Loss:2146.0756836\tKL Div:9.0403252\n",
      "Epoch 908\tIter   12\tLoss:2119.6835938\tRec Loss:2110.6188965\tKL Div:9.0646944\n",
      "Epoch 908\tIter   18\tLoss:2111.2175293\tRec Loss:2102.2934570\tKL Div:8.9240303\n",
      "Epoch 909\tIter    6\tLoss:2140.5993652\tRec Loss:2131.6511230\tKL Div:8.9483490\n",
      "Epoch 909\tIter   12\tLoss:2120.5969238\tRec Loss:2111.5839844\tKL Div:9.0129232\n",
      "Epoch 909\tIter   18\tLoss:2149.2541504\tRec Loss:2140.2844238\tKL Div:8.9696293\n",
      "Epoch 910\tIter    6\tLoss:2120.5671387\tRec Loss:2111.3945312\tKL Div:9.1727047\n",
      "Epoch 910\tIter   12\tLoss:2089.0319824\tRec Loss:2080.1079102\tKL Div:8.9240837\n",
      "Epoch 910\tIter   18\tLoss:2170.0488281\tRec Loss:2160.8916016\tKL Div:9.1572762\n",
      "Epoch 911\tIter    6\tLoss:2103.6379395\tRec Loss:2094.6110840\tKL Div:9.0268116\n",
      "Epoch 911\tIter   12\tLoss:2132.4494629\tRec Loss:2123.5446777\tKL Div:8.9047709\n",
      "Epoch 911\tIter   18\tLoss:2126.0539551\tRec Loss:2117.1462402\tKL Div:8.9076052\n",
      "Epoch 912\tIter    6\tLoss:2128.5820312\tRec Loss:2119.6254883\tKL Div:8.9566393\n",
      "Epoch 912\tIter   12\tLoss:2144.4204102\tRec Loss:2135.2958984\tKL Div:9.1245766\n",
      "Epoch 912\tIter   18\tLoss:2139.6821289\tRec Loss:2130.6682129\tKL Div:9.0138950\n",
      "Epoch 913\tIter    6\tLoss:2131.4147949\tRec Loss:2122.4731445\tKL Div:8.9417639\n",
      "Epoch 913\tIter   12\tLoss:2088.8029785\tRec Loss:2079.9096680\tKL Div:8.8933010\n",
      "Epoch 913\tIter   18\tLoss:2144.1591797\tRec Loss:2135.1948242\tKL Div:8.9642916\n",
      "Epoch 914\tIter    6\tLoss:2090.9157715\tRec Loss:2081.9555664\tKL Div:8.9602852\n",
      "Epoch 914\tIter   12\tLoss:2154.4184570\tRec Loss:2145.4223633\tKL Div:8.9961758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 914\tIter   18\tLoss:2109.4338379\tRec Loss:2100.4948730\tKL Div:8.9389439\n",
      "Epoch 915\tIter    6\tLoss:2125.5185547\tRec Loss:2116.4697266\tKL Div:9.0487518\n",
      "Epoch 915\tIter   12\tLoss:2133.3063965\tRec Loss:2124.4467773\tKL Div:8.8595963\n",
      "Epoch 915\tIter   18\tLoss:2100.0693359\tRec Loss:2090.9174805\tKL Div:9.1519032\n",
      "Epoch 916\tIter    6\tLoss:2095.9248047\tRec Loss:2087.0771484\tKL Div:8.8476067\n",
      "Epoch 916\tIter   12\tLoss:2111.9416504\tRec Loss:2102.7099609\tKL Div:9.2316380\n",
      "Epoch 916\tIter   18\tLoss:2108.3078613\tRec Loss:2099.5227051\tKL Div:8.7852058\n",
      "Epoch 917\tIter    6\tLoss:2113.4848633\tRec Loss:2104.4340820\tKL Div:9.0507298\n",
      "Epoch 917\tIter   12\tLoss:2136.0070801\tRec Loss:2126.8825684\tKL Div:9.1244373\n",
      "Epoch 917\tIter   18\tLoss:2160.8076172\tRec Loss:2151.9106445\tKL Div:8.8970013\n",
      "Epoch 918\tIter    6\tLoss:2081.5146484\tRec Loss:2072.6132812\tKL Div:8.9013796\n",
      "Epoch 918\tIter   12\tLoss:2088.9797363\tRec Loss:2080.0017090\tKL Div:8.9780388\n",
      "Epoch 918\tIter   18\tLoss:2144.7807617\tRec Loss:2135.8403320\tKL Div:8.9404879\n",
      "Epoch 919\tIter    6\tLoss:2128.7607422\tRec Loss:2119.6645508\tKL Div:9.0961914\n",
      "Epoch 919\tIter   12\tLoss:2126.9904785\tRec Loss:2118.1457520\tKL Div:8.8446550\n",
      "Epoch 919\tIter   18\tLoss:2120.0144043\tRec Loss:2111.0654297\tKL Div:8.9490433\n",
      "Epoch 920\tIter    6\tLoss:2088.1796875\tRec Loss:2079.2331543\tKL Div:8.9465141\n",
      "Epoch 920\tIter   12\tLoss:2136.2795410\tRec Loss:2127.2829590\tKL Div:8.9965496\n",
      "Epoch 920\tIter   18\tLoss:2147.8977051\tRec Loss:2138.9641113\tKL Div:8.9335213\n",
      "Epoch 920\tLoss:2126.9531204\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   571,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'piece', 'tone', 'sense', 'space', 'ambient', 'build', 'melody', 'close', 'electronic', 'piano', 'note', 'light', 'listen']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'piano', 'debut', 'harmony', 'group', 'string', 'chorus', 'bit', 'write', 'line', 'pretty']\n",
      "['version', 'live', 'disc', 'set', 'record', 'include', 'soul', 'label', 'recording', 'cover', 'studio', 'compilation', 'original', 'group', 'early']\n",
      "['bad', 'guy', 'man', 'lyric', 'listen', 'day', 'people', 'get', 'love', 'kid', 'girl', 'let', 'pretty', '\\n', 'line']\n",
      "['house', 'techno', 'label', 'dance', 'mix', 'producer', 'bass', 'beat', 'dj', 'synth', 'artist', 'club', 'drum', 'set', 'rhythm']\n",
      "['dance', 'disco', 'synth', 'single', 'house', 'love', 'group', 'remix', 'big', 'electro', 'duo', 'club', 'indie', 'act', 'turn']\n",
      "['\\xa0', 'project', 'artist', 'point', 'year', 'synth', 'early', 'solo', 'title', 'place', 'close', 'feature', 'producer', 'ep', 'show']\n",
      "['melody', 'riff', 'debut', 'drum', 'band', 'chorus', 'punk', 'group', 'hook', 'line', 'bass', 'rhythm', 'ep', 'hard', 'opener']\n",
      "['sing', 'love', 'life', 'lyric', 'world', 'line', 'word', 'write', 'leave', 'close', 'feeling', 'death', 'kind', 'sense', 'light']\n",
      "['punk', 'live', 'single', 'love', 'cover', 'band', 'early', 'solo', 'day', 'big', 'smith', 'group', 'set', 'fan', 'classic']\n",
      "['drum', 'electronic', 'melody', 'noise', 'bass', 'beat', 'jazz', 'piece', 'begin', 'disc', 'feature', 'group', 'interesting', 'live', 'tune']\n",
      "['man', 'life', 'world', 'black', 'people', 'woman', 'write', 'love', 'political', 'word', 'call', 'white', 'live', 'america', 'story']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'dylan', 'man', 'artist', 'record', 'lyric', 'american', 'oldham', 'acoustic', 'write']\n",
      "['hip_hop', 'beat', 'sample', 'funk', 'rap', 'mix', 'break', 'soul', 'dj', 'style', 'party', 'bass', 'genre', 'mc', 'bit']\n",
      "['piece', 'jazz', 'piano', 'composer', 'film', 'musician', 'composition', 'world', 'instrument', 'group', 'recording', 'electronic', 'score', 'string', 'solo']\n",
      "['rap', 'rapper', 'beat', 'mixtape', 'verse', 'hip_hop', 'year', 'production', 'flow', 'producer', 'feature', 'line', 'style', 'big', 'hit']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'debut', 'sense', 'artist', 'production', 'project', 'kind', 'instrumental', 'close', 'drum']\n",
      "['lyric', 'kind', 'title', 'band', 'people', 'love', 'emo', 'point', 'indie_rock', 'sort', 'indie', 'life', 'chorus', 'big', 'hook']\n",
      "['metal', 'riff', 'hardcore', 'black_metal', 'band', 'noise', 'drum', 'punk', 'death', 'heavy', 'doom', 'close', 'year', 'early', 'bass']\n",
      "['love', 'r&b', 'singer', 'artist', 'star', 'hit', 'single', 'year', 'producer', 'debut', 'girl', 'prince', 'big', 'drake', 'soul']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   571,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   571,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.51\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "piece\n",
      "tone\n",
      "sense\n",
      "space\n",
      "ambient\n",
      "build\n",
      "melody\n",
      "close\n",
      "electronic\n",
      "piano\n",
      "note\n",
      "light\n",
      "listen\n",
      "coherence c_uci -0.043377267135072166\n",
      "coherence c_npmi 0.012478450771971988\n",
      "coherence c_cv 0.38836098839059047\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   573,  1307,   131,\n",
      "          254,   571,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 921\tIter    6\tLoss:2144.3842773\tRec Loss:2135.3178711\tKL Div:9.0663147\n",
      "Epoch 921\tIter   12\tLoss:2142.5732422\tRec Loss:2133.6303711\tKL Div:8.9428616\n",
      "Epoch 921\tIter   18\tLoss:2101.6347656\tRec Loss:2092.6413574\tKL Div:8.9933786\n",
      "Epoch 922\tIter    6\tLoss:2128.4497070\tRec Loss:2119.4404297\tKL Div:9.0091887\n",
      "Epoch 922\tIter   12\tLoss:2109.6022949\tRec Loss:2100.6860352\tKL Div:8.9162312\n",
      "Epoch 922\tIter   18\tLoss:2135.2504883\tRec Loss:2126.2792969\tKL Div:8.9711266\n",
      "Epoch 923\tIter    6\tLoss:2081.0500488\tRec Loss:2072.0334473\tKL Div:9.0165968\n",
      "Epoch 923\tIter   12\tLoss:2097.4531250\tRec Loss:2088.4995117\tKL Div:8.9537315\n",
      "Epoch 923\tIter   18\tLoss:2141.1779785\tRec Loss:2132.3110352\tKL Div:8.8668900\n",
      "Epoch 924\tIter    6\tLoss:2133.0471191\tRec Loss:2123.9702148\tKL Div:9.0769405\n",
      "Epoch 924\tIter   12\tLoss:2141.0000000\tRec Loss:2131.9721680\tKL Div:9.0278816\n",
      "Epoch 924\tIter   18\tLoss:2127.5915527\tRec Loss:2118.6037598\tKL Div:8.9877205\n",
      "Epoch 925\tIter    6\tLoss:2132.3549805\tRec Loss:2123.1916504\tKL Div:9.1634407\n",
      "Epoch 925\tIter   12\tLoss:2123.3200684\tRec Loss:2114.2041016\tKL Div:9.1158705\n",
      "Epoch 925\tIter   18\tLoss:2150.4511719\tRec Loss:2141.5744629\tKL Div:8.8767891\n",
      "Epoch 926\tIter    6\tLoss:2149.9831543\tRec Loss:2140.8032227\tKL Div:9.1798763\n",
      "Epoch 926\tIter   12\tLoss:2069.0161133\tRec Loss:2060.3049316\tKL Div:8.7111187\n",
      "Epoch 926\tIter   18\tLoss:2091.1716309\tRec Loss:2082.1450195\tKL Div:9.0267220\n",
      "Epoch 927\tIter    6\tLoss:2107.1345215\tRec Loss:2098.1613770\tKL Div:8.9731646\n",
      "Epoch 927\tIter   12\tLoss:2132.3103027\tRec Loss:2123.4020996\tKL Div:8.9082785\n",
      "Epoch 927\tIter   18\tLoss:2140.9145508\tRec Loss:2131.8769531\tKL Div:9.0376625\n",
      "Epoch 928\tIter    6\tLoss:2143.1171875\tRec Loss:2134.1704102\tKL Div:8.9467258\n",
      "Epoch 928\tIter   12\tLoss:2139.9565430\tRec Loss:2130.9062500\tKL Div:9.0502071\n",
      "Epoch 928\tIter   18\tLoss:2185.5498047\tRec Loss:2176.5556641\tKL Div:8.9941158\n",
      "Epoch 929\tIter    6\tLoss:2156.4169922\tRec Loss:2147.2939453\tKL Div:9.1229305\n",
      "Epoch 929\tIter   12\tLoss:2139.2482910\tRec Loss:2130.2998047\tKL Div:8.9485073\n",
      "Epoch 929\tIter   18\tLoss:2137.7133789\tRec Loss:2128.5903320\tKL Div:9.1231232\n",
      "Epoch 930\tIter    6\tLoss:2106.3535156\tRec Loss:2097.3540039\tKL Div:8.9996128\n",
      "Epoch 930\tIter   12\tLoss:2156.1401367\tRec Loss:2147.2275391\tKL Div:8.9126682\n",
      "Epoch 930\tIter   18\tLoss:2096.8354492\tRec Loss:2087.9326172\tKL Div:8.9027348\n",
      "Epoch 931\tIter    6\tLoss:2121.9440918\tRec Loss:2112.8750000\tKL Div:9.0690250\n",
      "Epoch 931\tIter   12\tLoss:2121.4248047\tRec Loss:2112.3842773\tKL Div:9.0406351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 931\tIter   18\tLoss:2127.4479980\tRec Loss:2118.5590820\tKL Div:8.8889809\n",
      "Epoch 932\tIter    6\tLoss:2115.2275391\tRec Loss:2106.1176758\tKL Div:9.1099586\n",
      "Epoch 932\tIter   12\tLoss:2147.8317871\tRec Loss:2138.8220215\tKL Div:9.0098467\n",
      "Epoch 932\tIter   18\tLoss:2119.7575684\tRec Loss:2110.7314453\tKL Div:9.0261898\n",
      "Epoch 933\tIter    6\tLoss:2123.9973145\tRec Loss:2114.9262695\tKL Div:9.0710831\n",
      "Epoch 933\tIter   12\tLoss:2144.1606445\tRec Loss:2135.2358398\tKL Div:8.9248695\n",
      "Epoch 933\tIter   18\tLoss:2137.3083496\tRec Loss:2128.3569336\tKL Div:8.9514685\n",
      "Epoch 934\tIter    6\tLoss:2063.8818359\tRec Loss:2055.0092773\tKL Div:8.8726501\n",
      "Epoch 934\tIter   12\tLoss:2112.3928223\tRec Loss:2103.4741211\tKL Div:8.9185925\n",
      "Epoch 934\tIter   18\tLoss:2149.0537109\tRec Loss:2139.8886719\tKL Div:9.1649513\n",
      "Epoch 935\tIter    6\tLoss:2094.3869629\tRec Loss:2085.4877930\tKL Div:8.8991632\n",
      "Epoch 935\tIter   12\tLoss:2130.5161133\tRec Loss:2121.4272461\tKL Div:9.0889893\n",
      "Epoch 935\tIter   18\tLoss:2152.7751465\tRec Loss:2143.7260742\tKL Div:9.0489521\n",
      "Epoch 936\tIter    6\tLoss:2143.7165527\tRec Loss:2134.7436523\tKL Div:8.9729452\n",
      "Epoch 936\tIter   12\tLoss:2098.6450195\tRec Loss:2089.6320801\tKL Div:9.0129614\n",
      "Epoch 936\tIter   18\tLoss:2096.5485840\tRec Loss:2087.6601562\tKL Div:8.8883247\n",
      "Epoch 937\tIter    6\tLoss:2145.7050781\tRec Loss:2136.7045898\tKL Div:9.0003815\n",
      "Epoch 937\tIter   12\tLoss:2168.1401367\tRec Loss:2159.1677246\tKL Div:8.9725237\n",
      "Epoch 937\tIter   18\tLoss:2131.7048340\tRec Loss:2122.6896973\tKL Div:9.0150900\n",
      "Epoch 938\tIter    6\tLoss:2148.3859863\tRec Loss:2139.3515625\tKL Div:9.0344782\n",
      "Epoch 938\tIter   12\tLoss:2077.0795898\tRec Loss:2068.1965332\tKL Div:8.8830795\n",
      "Epoch 938\tIter   18\tLoss:2148.0046387\tRec Loss:2138.8876953\tKL Div:9.1169195\n",
      "Epoch 939\tIter    6\tLoss:2100.6821289\tRec Loss:2091.5639648\tKL Div:9.1182289\n",
      "Epoch 939\tIter   12\tLoss:2103.3666992\tRec Loss:2094.5737305\tKL Div:8.7928638\n",
      "Epoch 939\tIter   18\tLoss:2132.7822266\tRec Loss:2123.5566406\tKL Div:9.2254734\n",
      "Epoch 940\tIter    6\tLoss:2131.4597168\tRec Loss:2122.6342773\tKL Div:8.8255539\n",
      "Epoch 940\tIter   12\tLoss:2146.3723145\tRec Loss:2137.4111328\tKL Div:8.9611454\n",
      "Epoch 940\tIter   18\tLoss:2072.0021973\tRec Loss:2063.0871582\tKL Div:8.9149418\n",
      "Epoch 941\tIter    6\tLoss:2115.7019043\tRec Loss:2106.7148438\tKL Div:8.9871635\n",
      "Epoch 941\tIter   12\tLoss:2123.7465820\tRec Loss:2114.8852539\tKL Div:8.8612499\n",
      "Epoch 941\tIter   18\tLoss:2102.1250000\tRec Loss:2093.2729492\tKL Div:8.8521109\n",
      "Epoch 942\tIter    6\tLoss:2111.5366211\tRec Loss:2102.4160156\tKL Div:9.1206970\n",
      "Epoch 942\tIter   12\tLoss:2108.5476074\tRec Loss:2099.7358398\tKL Div:8.8118801\n",
      "Epoch 942\tIter   18\tLoss:2127.5744629\tRec Loss:2118.3662109\tKL Div:9.2081642\n",
      "Epoch 943\tIter    6\tLoss:2121.0749512\tRec Loss:2112.0678711\tKL Div:9.0069599\n",
      "Epoch 943\tIter   12\tLoss:2113.8666992\tRec Loss:2104.9829102\tKL Div:8.8836823\n",
      "Epoch 943\tIter   18\tLoss:2116.9523926\tRec Loss:2107.9230957\tKL Div:9.0294075\n",
      "Epoch 944\tIter    6\tLoss:2132.2968750\tRec Loss:2123.4267578\tKL Div:8.8700228\n",
      "Epoch 944\tIter   12\tLoss:2098.1782227\tRec Loss:2089.1838379\tKL Div:8.9943752\n",
      "Epoch 944\tIter   18\tLoss:2159.3022461\tRec Loss:2150.3569336\tKL Div:8.9453735\n",
      "Epoch 945\tIter    6\tLoss:2128.2145996\tRec Loss:2119.2778320\tKL Div:8.9368391\n",
      "Epoch 945\tIter   12\tLoss:2148.9489746\tRec Loss:2139.8059082\tKL Div:9.1431360\n",
      "Epoch 945\tIter   18\tLoss:2091.6853027\tRec Loss:2082.9628906\tKL Div:8.7224998\n",
      "Epoch 946\tIter    6\tLoss:2145.3295898\tRec Loss:2136.2145996\tKL Div:9.1149864\n",
      "Epoch 946\tIter   12\tLoss:2117.9790039\tRec Loss:2109.0578613\tKL Div:8.9212456\n",
      "Epoch 946\tIter   18\tLoss:2106.3166504\tRec Loss:2097.4919434\tKL Div:8.8246231\n",
      "Epoch 947\tIter    6\tLoss:2166.0112305\tRec Loss:2156.8383789\tKL Div:9.1727409\n",
      "Epoch 947\tIter   12\tLoss:2101.1391602\tRec Loss:2092.3139648\tKL Div:8.8250980\n",
      "Epoch 947\tIter   18\tLoss:2130.0100098\tRec Loss:2121.0114746\tKL Div:8.9986534\n",
      "Epoch 948\tIter    6\tLoss:2059.7358398\tRec Loss:2050.8701172\tKL Div:8.8656082\n",
      "Epoch 948\tIter   12\tLoss:2117.1274414\tRec Loss:2108.1157227\tKL Div:9.0116596\n",
      "Epoch 948\tIter   18\tLoss:2142.5522461\tRec Loss:2133.5209961\tKL Div:9.0313225\n",
      "Epoch 949\tIter    6\tLoss:2108.0202637\tRec Loss:2099.1479492\tKL Div:8.8723345\n",
      "Epoch 949\tIter   12\tLoss:2149.0502930\tRec Loss:2139.9445801\tKL Div:9.1057625\n",
      "Epoch 949\tIter   18\tLoss:2130.9780273\tRec Loss:2121.9807129\tKL Div:8.9972162\n",
      "Epoch 950\tIter    6\tLoss:2154.8024902\tRec Loss:2145.7519531\tKL Div:9.0506001\n",
      "Epoch 950\tIter   12\tLoss:2111.7023926\tRec Loss:2102.6845703\tKL Div:9.0177250\n",
      "Epoch 950\tIter   18\tLoss:2168.2238770\tRec Loss:2159.2133789\tKL Div:9.0105438\n",
      "Epoch 951\tIter    6\tLoss:2137.6127930\tRec Loss:2128.5283203\tKL Div:9.0843964\n",
      "Epoch 951\tIter   12\tLoss:2122.5957031\tRec Loss:2113.5610352\tKL Div:9.0346193\n",
      "Epoch 951\tIter   18\tLoss:2102.8576660\tRec Loss:2094.0170898\tKL Div:8.8406525\n",
      "Epoch 952\tIter    6\tLoss:2095.2221680\tRec Loss:2086.0805664\tKL Div:9.1415730\n",
      "Epoch 952\tIter   12\tLoss:2124.0563965\tRec Loss:2115.1520996\tKL Div:8.9043922\n",
      "Epoch 952\tIter   18\tLoss:2102.8774414\tRec Loss:2093.8737793\tKL Div:9.0037537\n",
      "Epoch 953\tIter    6\tLoss:2119.3764648\tRec Loss:2110.3977051\tKL Div:8.9788389\n",
      "Epoch 953\tIter   12\tLoss:2133.9138184\tRec Loss:2124.9687500\tKL Div:8.9451199\n",
      "Epoch 953\tIter   18\tLoss:2105.7197266\tRec Loss:2096.5310059\tKL Div:9.1887035\n",
      "Epoch 954\tIter    6\tLoss:2114.6801758\tRec Loss:2105.7773438\tKL Div:8.9028625\n",
      "Epoch 954\tIter   12\tLoss:2158.1760254\tRec Loss:2149.0541992\tKL Div:9.1217365\n",
      "Epoch 954\tIter   18\tLoss:2158.9606934\tRec Loss:2150.0166016\tKL Div:8.9441395\n",
      "Epoch 955\tIter    6\tLoss:2133.2846680\tRec Loss:2124.1484375\tKL Div:9.1362448\n",
      "Epoch 955\tIter   12\tLoss:2151.3051758\tRec Loss:2142.3666992\tKL Div:8.9383764\n",
      "Epoch 955\tIter   18\tLoss:2136.2521973\tRec Loss:2127.1027832\tKL Div:9.1493320\n",
      "Epoch 956\tIter    6\tLoss:2138.5288086\tRec Loss:2129.5275879\tKL Div:9.0012980\n",
      "Epoch 956\tIter   12\tLoss:2118.3312988\tRec Loss:2109.1086426\tKL Div:9.2227192\n",
      "Epoch 956\tIter   18\tLoss:2114.0446777\tRec Loss:2105.1047363\tKL Div:8.9399509\n",
      "Epoch 957\tIter    6\tLoss:2097.5268555\tRec Loss:2088.6594238\tKL Div:8.8673372\n",
      "Epoch 957\tIter   12\tLoss:2090.3789062\tRec Loss:2081.4147949\tKL Div:8.9640350\n",
      "Epoch 957\tIter   18\tLoss:2120.0217285\tRec Loss:2111.1311035\tKL Div:8.8905554\n",
      "Epoch 958\tIter    6\tLoss:2161.8764648\tRec Loss:2152.7077637\tKL Div:9.1687498\n",
      "Epoch 958\tIter   12\tLoss:2100.1811523\tRec Loss:2091.2021484\tKL Div:8.9790897\n",
      "Epoch 958\tIter   18\tLoss:2150.8725586\tRec Loss:2141.9721680\tKL Div:8.9003229\n",
      "Epoch 959\tIter    6\tLoss:2166.1083984\tRec Loss:2156.9453125\tKL Div:9.1630888\n",
      "Epoch 959\tIter   12\tLoss:2097.4250488\tRec Loss:2088.5698242\tKL Div:8.8552818\n",
      "Epoch 959\tIter   18\tLoss:2112.2351074\tRec Loss:2103.2131348\tKL Div:9.0219946\n",
      "Epoch 960\tIter    6\tLoss:2096.3046875\tRec Loss:2087.2932129\tKL Div:9.0113525\n",
      "Epoch 960\tIter   12\tLoss:2105.7722168\tRec Loss:2096.7958984\tKL Div:8.9763746\n",
      "Epoch 960\tIter   18\tLoss:2142.6892090\tRec Loss:2133.6945801\tKL Div:8.9946718\n",
      "Epoch 960\tLoss:2129.9801873\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,  2218,  1114,   235, 13238,   609,  1307,   131,\n",
      "          254,   571,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'tone', 'piece', 'sense', 'ambient', 'space', 'build', 'melody', 'close', 'piano', 'electronic', 'light', 'drum', 'listen']\n",
      "['love', 'melody', 'indie', 'lyric', 'arrangement', 'piano', 'harmony', 'debut', 'group', 'chorus', 'bit', 'string', 'line', 'write', 'pretty']\n",
      "['version', 'live', 'disc', 'set', 'include', 'record', 'studio', 'soul', 'recording', 'label', 'cover', 'group', 'compilation', 'original', 'early']\n",
      "['guy', 'bad', 'listen', 'man', 'lyric', 'get', 'day', 'people', 'kid', 'love', 'girl', 'pretty', 'let', 'line', '\\n']\n",
      "['house', 'techno', 'dance', 'label', 'producer', 'mix', 'bass', 'beat', 'dj', 'artist', 'synth', 'set', 'club', 'rhythm', 'drum']\n",
      "['dance', 'disco', 'synth', 'single', 'house', 'love', 'big', 'remix', 'group', 'electro', 'duo', 'club', 'turn', 'indie', 'mix']\n",
      "['\\xa0', 'project', 'artist', 'point', 'year', 'title', 'solo', 'synth', 'early', 'producer', 'close', 'ep', 'feature', 'place', 'sense']\n",
      "['riff', 'melody', 'debut', 'drum', 'band', 'punk', 'chorus', 'group', 'line', 'hook', 'bass', 'ep', 'rhythm', 'hard', 'opener']\n",
      "['sing', 'love', 'life', 'lyric', 'world', 'line', 'word', 'write', 'feeling', 'leave', 'close', 'death', 'light', 'sense', 'story']\n",
      "['punk', 'live', 'single', 'love', 'cover', 'band', 'early', 'solo', 'smith', 'day', 'set', 'pollard', 'classic', 'big', 'fan']\n",
      "['drum', 'electronic', 'noise', 'melody', 'bass', 'beat', 'jazz', 'piece', 'begin', 'live', 'interesting', 'disc', 'group', 'feature', 'instrumental']\n",
      "['man', 'life', 'people', 'black', 'world', 'woman', 'love', 'write', 'political', 'white', 'word', 'call', 'story', 'live', 'day']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'dylan', 'man', 'artist', 'record', 'acoustic', 'american', 'lyric', 'oldham', 'love']\n",
      "['hip_hop', 'beat', 'sample', 'funk', 'rap', 'mix', 'soul', 'dj', 'break', 'style', 'bass', 'mc', 'genre', 'party', 'cut']\n",
      "['piece', 'jazz', 'composer', 'film', 'piano', 'composition', 'musician', 'world', 'instrument', 'group', 'recording', 'solo', 'score', 'electronic', 'string']\n",
      "['rap', 'rapper', 'beat', 'mixtape', 'verse', 'hip_hop', 'production', 'year', 'flow', 'producer', 'feature', 'line', 'style', 'big', 'artist']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'debut', 'artist', 'sense', 'production', 'instrumental', 'project', 'build', 'kind', 'drum']\n",
      "['lyric', 'kind', 'title', 'band', 'people', 'love', 'emo', 'indie_rock', 'point', 'life', 'indie', 'sort', 'chorus', 'punk', 'hook']\n",
      "['metal', 'riff', 'black_metal', 'hardcore', 'band', 'noise', 'drum', 'death', 'punk', 'heavy', 'doom', 'year', 'close', 'drummer', 'black']\n",
      "['love', 'r&b', 'artist', 'singer', 'single', 'star', 'hit', 'year', 'producer', 'debut', 'girl', 'prince', 'big', 'soul', 'drake']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,  2218,  1114,   235, 13238,   609,  1307,   131,\n",
      "          254,   571,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,  2218,  1114,   235, 13238,   609,  1307,   131,\n",
      "          254,   571,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5033333333333333\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "tone\n",
      "piece\n",
      "sense\n",
      "ambient\n",
      "space\n",
      "build\n",
      "melody\n",
      "close\n",
      "piano\n",
      "electronic\n",
      "light\n",
      "drum\n",
      "listen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence c_uci -0.08450339092662869\n",
      "coherence c_npmi 0.010984092188086922\n",
      "coherence c_cv 0.38886585336410245\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,  2218,  1114,   235, 13238,   609,  1307,   131,\n",
      "          254,   571,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "Epoch 961\tIter    6\tLoss:2147.3894043\tRec Loss:2138.2827148\tKL Div:9.1067734\n",
      "Epoch 961\tIter   12\tLoss:2157.0786133\tRec Loss:2148.0241699\tKL Div:9.0544720\n",
      "Epoch 961\tIter   18\tLoss:2119.5297852\tRec Loss:2110.5820312\tKL Div:8.9477615\n",
      "Epoch 962\tIter    6\tLoss:2168.5114746\tRec Loss:2159.5053711\tKL Div:9.0060902\n",
      "Epoch 962\tIter   12\tLoss:2142.3122559\tRec Loss:2133.3632812\tKL Div:8.9488907\n",
      "Epoch 962\tIter   18\tLoss:2135.8818359\tRec Loss:2126.9743652\tKL Div:8.9075451\n",
      "Epoch 963\tIter    6\tLoss:2094.0417480\tRec Loss:2085.1442871\tKL Div:8.8974571\n",
      "Epoch 963\tIter   12\tLoss:2126.5009766\tRec Loss:2117.4055176\tKL Div:9.0953875\n",
      "Epoch 963\tIter   18\tLoss:2132.2685547\tRec Loss:2123.2622070\tKL Div:9.0064621\n",
      "Epoch 964\tIter    6\tLoss:2121.7255859\tRec Loss:2112.8310547\tKL Div:8.8945484\n",
      "Epoch 964\tIter   12\tLoss:2109.0102539\tRec Loss:2100.0661621\tKL Div:8.9442129\n",
      "Epoch 964\tIter   18\tLoss:2118.1752930\tRec Loss:2109.3286133\tKL Div:8.8465996\n",
      "Epoch 965\tIter    6\tLoss:2131.1367188\tRec Loss:2122.0827637\tKL Div:9.0540228\n",
      "Epoch 965\tIter   12\tLoss:2071.6320801\tRec Loss:2062.7431641\tKL Div:8.8888874\n",
      "Epoch 965\tIter   18\tLoss:2119.2773438\tRec Loss:2110.2770996\tKL Div:9.0002394\n",
      "Epoch 966\tIter    6\tLoss:2140.1813965\tRec Loss:2131.1289062\tKL Div:9.0525627\n",
      "Epoch 966\tIter   12\tLoss:2141.2551270\tRec Loss:2132.1154785\tKL Div:9.1395702\n",
      "Epoch 966\tIter   18\tLoss:2119.2470703\tRec Loss:2110.3393555\tKL Div:8.9077034\n",
      "Epoch 967\tIter    6\tLoss:2098.3261719\tRec Loss:2089.3764648\tKL Div:8.9496479\n",
      "Epoch 967\tIter   12\tLoss:2123.0874023\tRec Loss:2114.0771484\tKL Div:9.0101643\n",
      "Epoch 967\tIter   18\tLoss:2127.5297852\tRec Loss:2118.4682617\tKL Div:9.0614500\n",
      "Epoch 968\tIter    6\tLoss:2132.2482910\tRec Loss:2123.1040039\tKL Div:9.1442833\n",
      "Epoch 968\tIter   12\tLoss:2138.9167480\tRec Loss:2130.1132812\tKL Div:8.8034096\n",
      "Epoch 968\tIter   18\tLoss:2098.4924316\tRec Loss:2089.5480957\tKL Div:8.9442368\n",
      "Epoch 969\tIter    6\tLoss:2130.8300781\tRec Loss:2121.9316406\tKL Div:8.8983555\n",
      "Epoch 969\tIter   12\tLoss:2129.0778809\tRec Loss:2120.1044922\tKL Div:8.9733124\n",
      "Epoch 969\tIter   18\tLoss:2127.2121582\tRec Loss:2118.1010742\tKL Div:9.1110535\n",
      "Epoch 970\tIter    6\tLoss:2124.0549316\tRec Loss:2115.1730957\tKL Div:8.8817720\n",
      "Epoch 970\tIter   12\tLoss:2070.2314453\tRec Loss:2061.2187500\tKL Div:9.0127859\n",
      "Epoch 970\tIter   18\tLoss:2109.0805664\tRec Loss:2100.3818359\tKL Div:8.6986914\n",
      "Epoch 971\tIter    6\tLoss:2127.0559082\tRec Loss:2117.9333496\tKL Div:9.1225672\n",
      "Epoch 971\tIter   12\tLoss:2176.6782227\tRec Loss:2167.5234375\tKL Div:9.1548624\n",
      "Epoch 971\tIter   18\tLoss:2112.0949707\tRec Loss:2103.0087891\tKL Div:9.0861950\n",
      "Epoch 972\tIter    6\tLoss:2123.8652344\tRec Loss:2114.9755859\tKL Div:8.8897018\n",
      "Epoch 972\tIter   12\tLoss:2121.6826172\tRec Loss:2112.7177734\tKL Div:8.9648781\n",
      "Epoch 972\tIter   18\tLoss:2088.0708008\tRec Loss:2079.1298828\tKL Div:8.9409714\n",
      "Epoch 973\tIter    6\tLoss:2138.9348145\tRec Loss:2129.9118652\tKL Div:9.0230293\n",
      "Epoch 973\tIter   12\tLoss:2108.6613770\tRec Loss:2099.6665039\tKL Div:8.9948759\n",
      "Epoch 973\tIter   18\tLoss:2141.5524902\tRec Loss:2132.5610352\tKL Div:8.9913425\n",
      "Epoch 974\tIter    6\tLoss:2126.9360352\tRec Loss:2117.8669434\tKL Div:9.0690155\n",
      "Epoch 974\tIter   12\tLoss:2153.9936523\tRec Loss:2144.9902344\tKL Div:9.0033855\n",
      "Epoch 974\tIter   18\tLoss:2169.3393555\tRec Loss:2160.2336426\tKL Div:9.1057081\n",
      "Epoch 975\tIter    6\tLoss:2150.6386719\tRec Loss:2141.5454102\tKL Div:9.0933485\n",
      "Epoch 975\tIter   12\tLoss:2140.1850586\tRec Loss:2131.0415039\tKL Div:9.1434898\n",
      "Epoch 975\tIter   18\tLoss:2115.0383301\tRec Loss:2106.0534668\tKL Div:8.9847507\n",
      "Epoch 976\tIter    6\tLoss:2098.0664062\tRec Loss:2088.9780273\tKL Div:9.0884018\n",
      "Epoch 976\tIter   12\tLoss:2099.4270020\tRec Loss:2090.5439453\tKL Div:8.8830833\n",
      "Epoch 976\tIter   18\tLoss:2117.4929199\tRec Loss:2108.5249023\tKL Div:8.9680367\n",
      "Epoch 977\tIter    6\tLoss:2193.1962891\tRec Loss:2184.0319824\tKL Div:9.1643238\n",
      "Epoch 977\tIter   12\tLoss:2081.8325195\tRec Loss:2072.8598633\tKL Div:8.9726839\n",
      "Epoch 977\tIter   18\tLoss:2160.8259277\tRec Loss:2151.6186523\tKL Div:9.2071772\n",
      "Epoch 978\tIter    6\tLoss:2095.4873047\tRec Loss:2086.5998535\tKL Div:8.8875618\n",
      "Epoch 978\tIter   12\tLoss:2106.9941406\tRec Loss:2097.9975586\tKL Div:8.9964867\n",
      "Epoch 978\tIter   18\tLoss:2106.5827637\tRec Loss:2097.7133789\tKL Div:8.8694468\n",
      "Epoch 979\tIter    6\tLoss:2100.3942871\tRec Loss:2091.4453125\tKL Div:8.9490623\n",
      "Epoch 979\tIter   12\tLoss:2188.2629395\tRec Loss:2179.2167969\tKL Div:9.0461235\n",
      "Epoch 979\tIter   18\tLoss:2092.1469727\tRec Loss:2083.2099609\tKL Div:8.9369888\n",
      "Epoch 980\tIter    6\tLoss:2103.1647949\tRec Loss:2094.1401367\tKL Div:9.0246506\n",
      "Epoch 980\tIter   12\tLoss:2139.8266602\tRec Loss:2130.9804688\tKL Div:8.8462715\n",
      "Epoch 980\tIter   18\tLoss:2138.9421387\tRec Loss:2129.7631836\tKL Div:9.1788731\n",
      "Epoch 981\tIter    6\tLoss:2128.4082031\tRec Loss:2119.4135742\tKL Div:8.9946804\n",
      "Epoch 981\tIter   12\tLoss:2103.5605469\tRec Loss:2094.7487793\tKL Div:8.8118353\n",
      "Epoch 981\tIter   18\tLoss:2127.6228027\tRec Loss:2118.6074219\tKL Div:9.0153818\n",
      "Epoch 982\tIter    6\tLoss:2118.9960938\tRec Loss:2109.9812012\tKL Div:9.0148907\n",
      "Epoch 982\tIter   12\tLoss:2157.7167969\tRec Loss:2148.7995605\tKL Div:8.9172430\n",
      "Epoch 982\tIter   18\tLoss:2143.6054688\tRec Loss:2134.5639648\tKL Div:9.0414867\n",
      "Epoch 983\tIter    6\tLoss:2170.2026367\tRec Loss:2161.1953125\tKL Div:9.0073395\n",
      "Epoch 983\tIter   12\tLoss:2216.6118164\tRec Loss:2207.5158691\tKL Div:9.0960474\n",
      "Epoch 983\tIter   18\tLoss:2099.8154297\tRec Loss:2090.9497070\tKL Div:8.8656635\n",
      "Epoch 984\tIter    6\tLoss:2077.5178223\tRec Loss:2068.5280762\tKL Div:8.9897680\n",
      "Epoch 984\tIter   12\tLoss:2189.0065918\tRec Loss:2179.8603516\tKL Div:9.1461887\n",
      "Epoch 984\tIter   18\tLoss:2156.6904297\tRec Loss:2147.7089844\tKL Div:8.9815655\n",
      "Epoch 985\tIter    6\tLoss:2078.3271484\tRec Loss:2069.3176270\tKL Div:9.0096149\n",
      "Epoch 985\tIter   12\tLoss:2156.6003418\tRec Loss:2147.5314941\tKL Div:9.0688114\n",
      "Epoch 985\tIter   18\tLoss:2139.3513184\tRec Loss:2130.3991699\tKL Div:8.9520931\n",
      "Epoch 986\tIter    6\tLoss:2115.8747559\tRec Loss:2106.7131348\tKL Div:9.1616573\n",
      "Epoch 986\tIter   12\tLoss:2120.7390137\tRec Loss:2111.8090820\tKL Div:8.9299717\n",
      "Epoch 986\tIter   18\tLoss:2130.0102539\tRec Loss:2120.9916992\tKL Div:9.0186195\n",
      "Epoch 987\tIter    6\tLoss:2123.2880859\tRec Loss:2114.2985840\tKL Div:8.9894342\n",
      "Epoch 987\tIter   12\tLoss:2165.2006836\tRec Loss:2156.1186523\tKL Div:9.0820732\n",
      "Epoch 987\tIter   18\tLoss:2144.2487793\tRec Loss:2135.2868652\tKL Div:8.9618301\n",
      "Epoch 988\tIter    6\tLoss:2100.4443359\tRec Loss:2091.4584961\tKL Div:8.9858160\n",
      "Epoch 988\tIter   12\tLoss:2138.5461426\tRec Loss:2129.5598145\tKL Div:8.9863815\n",
      "Epoch 988\tIter   18\tLoss:2155.5585938\tRec Loss:2146.5124512\tKL Div:9.0460806\n",
      "Epoch 989\tIter    6\tLoss:2082.4338379\tRec Loss:2073.4814453\tKL Div:8.9524231\n",
      "Epoch 989\tIter   12\tLoss:2146.7514648\tRec Loss:2137.7416992\tKL Div:9.0098228\n",
      "Epoch 989\tIter   18\tLoss:2174.9074707\tRec Loss:2165.7729492\tKL Div:9.1345119\n",
      "Epoch 990\tIter    6\tLoss:2104.1997070\tRec Loss:2095.2988281\tKL Div:8.9009418\n",
      "Epoch 990\tIter   12\tLoss:2137.1308594\tRec Loss:2128.0852051\tKL Div:9.0455437\n",
      "Epoch 990\tIter   18\tLoss:2108.0524902\tRec Loss:2099.1215820\tKL Div:8.9309711\n",
      "Epoch 991\tIter    6\tLoss:2142.4711914\tRec Loss:2133.5012207\tKL Div:8.9698811\n",
      "Epoch 991\tIter   12\tLoss:2160.9597168\tRec Loss:2152.0327148\tKL Div:8.9269161\n",
      "Epoch 991\tIter   18\tLoss:2160.9658203\tRec Loss:2151.9392090\tKL Div:9.0265703\n",
      "Epoch 992\tIter    6\tLoss:2148.3703613\tRec Loss:2139.2763672\tKL Div:9.0939083\n",
      "Epoch 992\tIter   12\tLoss:2094.1608887\tRec Loss:2085.2023926\tKL Div:8.9585190\n",
      "Epoch 992\tIter   18\tLoss:2145.0983887\tRec Loss:2136.1503906\tKL Div:8.9479923\n",
      "Epoch 993\tIter    6\tLoss:2113.9226074\tRec Loss:2104.9096680\tKL Div:9.0129471\n",
      "Epoch 993\tIter   12\tLoss:2132.8413086\tRec Loss:2123.7790527\tKL Div:9.0621872\n",
      "Epoch 993\tIter   18\tLoss:2129.0036621\tRec Loss:2120.0078125\tKL Div:8.9958515\n",
      "Epoch 994\tIter    6\tLoss:2164.0268555\tRec Loss:2155.0488281\tKL Div:8.9781218\n",
      "Epoch 994\tIter   12\tLoss:2088.7380371\tRec Loss:2079.7001953\tKL Div:9.0377846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 994\tIter   18\tLoss:2146.9338379\tRec Loss:2138.0710449\tKL Div:8.8628283\n",
      "Epoch 995\tIter    6\tLoss:2118.4912109\tRec Loss:2109.3779297\tKL Div:9.1133080\n",
      "Epoch 995\tIter   12\tLoss:2149.4191895\tRec Loss:2140.4875488\tKL Div:8.9316139\n",
      "Epoch 995\tIter   18\tLoss:2111.7099609\tRec Loss:2102.7214355\tKL Div:8.9885540\n",
      "Epoch 996\tIter    6\tLoss:2094.9948730\tRec Loss:2086.1069336\tKL Div:8.8880291\n",
      "Epoch 996\tIter   12\tLoss:2157.1257324\tRec Loss:2147.9526367\tKL Div:9.1730833\n",
      "Epoch 996\tIter   18\tLoss:2148.9843750\tRec Loss:2139.9819336\tKL Div:9.0024853\n",
      "Epoch 997\tIter    6\tLoss:2153.3674316\tRec Loss:2144.2421875\tKL Div:9.1252251\n",
      "Epoch 997\tIter   12\tLoss:2123.2509766\tRec Loss:2114.3432617\tKL Div:8.9076433\n",
      "Epoch 997\tIter   18\tLoss:2126.5507812\tRec Loss:2117.5507812\tKL Div:8.9999790\n",
      "Epoch 998\tIter    6\tLoss:2113.6635742\tRec Loss:2104.7333984\tKL Div:8.9302464\n",
      "Epoch 998\tIter   12\tLoss:2170.6987305\tRec Loss:2161.5666504\tKL Div:9.1320419\n",
      "Epoch 998\tIter   18\tLoss:2174.1992188\tRec Loss:2165.3405762\tKL Div:8.8586655\n",
      "Epoch 999\tIter    6\tLoss:2169.1647949\tRec Loss:2160.0791016\tKL Div:9.0855827\n",
      "Epoch 999\tIter   12\tLoss:2152.2333984\tRec Loss:2143.2114258\tKL Div:9.0219049\n",
      "Epoch 999\tIter   18\tLoss:2102.7683105\tRec Loss:2093.5688477\tKL Div:9.1994362\n",
      "Epoch 1000\tIter    6\tLoss:2128.9614258\tRec Loss:2120.1469727\tKL Div:8.8143997\n",
      "Epoch 1000\tIter   12\tLoss:2096.0432129\tRec Loss:2086.9838867\tKL Div:9.0593739\n",
      "Epoch 1000\tIter   18\tLoss:2110.3750000\tRec Loss:2101.3361816\tKL Div:9.0388050\n",
      "Epoch 1000\tLoss:2125.8132382\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   609,    97,   131,\n",
      "          254,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "['drone', 'noise', 'piece', 'tone', 'sense', 'space', 'ambient', 'build', 'melody', 'piano', 'close', 'electronic', 'light', 'note', 'open']\n",
      "['love', 'melody', 'lyric', 'indie', 'arrangement', 'piano', 'debut', 'harmony', 'string', 'group', 'chorus', 'bit', 'line', 'write', 'turn']\n",
      "['version', 'live', 'disc', 'set', 'record', 'include', 'recording', 'cover', 'studio', 'label', 'soul', 'original', 'compilation', 'group', 'early']\n",
      "['bad', 'guy', 'man', 'listen', 'lyric', 'people', 'get', 'day', 'kid', 'love', 'girl', 'let', '\\n', 'pretty', 'kind']\n",
      "['house', 'techno', 'dance', 'label', 'mix', 'producer', 'bass', 'beat', 'dj', 'synth', 'artist', 'set', 'club', 'drum', 'electronic']\n",
      "['dance', 'disco', 'synth', 'single', 'house', 'love', 'remix', 'big', 'electro', 'group', 'duo', 'club', 'mix', 'turn', 'indie']\n",
      "['\\xa0', 'project', 'point', 'artist', 'year', 'title', 'synth', 'solo', 'early', 'producer', 'close', 'love', 'place', 'ep', 'sense']\n",
      "['riff', 'melody', 'debut', 'drum', 'band', 'chorus', 'punk', 'group', 'line', 'hook', 'bass', 'ep', 'rhythm', 'hard', 'opener']\n",
      "['love', 'sing', 'life', 'lyric', 'world', 'line', 'word', 'write', 'leave', 'close', 'feeling', 'death', 'light', 'kind', 'sense']\n",
      "['punk', 'live', 'single', 'love', 'cover', 'early', 'band', 'smith', 'solo', 'day', 'set', 'classic', 'big', 'fan', 'group']\n",
      "['drum', 'electronic', 'noise', 'bass', 'melody', 'beat', 'jazz', 'begin', 'piece', 'interesting', 'disc', 'live', 'feature', 'group', 'tune']\n",
      "['life', 'man', 'world', 'black', 'people', 'woman', 'write', 'love', 'political', 'white', 'word', 'call', 'america', 'story', 'live']\n",
      "['folk', 'country', 'blue', 'cover', 'solo', 'sing', 'dylan', 'man', 'artist', 'lyric', 'record', 'acoustic', 'american', 'oldham', 'love']\n",
      "['hip_hop', 'beat', 'sample', 'funk', 'rap', 'mix', 'dj', 'style', 'break', 'soul', 'mc', 'bass', 'production', 'cut', 'bit']\n",
      "['piece', 'jazz', 'composer', 'film', 'piano', 'composition', 'musician', 'world', 'group', 'instrument', 'recording', 'electronic', 'solo', 'score', 'string']\n",
      "['rap', 'rapper', 'beat', 'mixtape', 'verse', 'year', 'production', 'hip_hop', 'flow', 'producer', 'line', 'feature', 'big', 'style', 'artist']\n",
      "['synth', 'beat', 'ep', 'electronic', 'melody', 'sample', 'artist', 'sense', 'debut', 'production', 'instrumental', 'project', 'kind', 'close', 'drum']\n",
      "['lyric', 'title', 'kind', 'people', 'band', 'love', 'point', 'indie_rock', 'indie', 'life', 'sort', 'emo', 'chorus', 'big', 'guy']\n",
      "['metal', 'riff', 'hardcore', 'black_metal', 'band', 'noise', 'drum', 'death', 'punk', 'heavy', 'doom', 'close', 'bass', 'year', 'black']\n",
      "['love', 'r&b', 'artist', 'singer', 'single', 'hit', 'star', 'year', 'producer', 'girl', 'debut', 'prince', 'big', 'soul', 'drake']\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   609,    97,   131,\n",
      "          254,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   609,    97,   131,\n",
      "          254,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n",
      "0.5033333333333333\n",
      "calculating topic dist\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "drone\n",
      "noise\n",
      "piece\n",
      "tone\n",
      "sense\n",
      "space\n",
      "ambient\n",
      "build\n",
      "melody\n",
      "piano\n",
      "close\n",
      "electronic\n",
      "light\n",
      "note\n",
      "open\n",
      "coherence c_uci -0.04203413958594037\n",
      "coherence c_npmi 0.01258903570635907\n",
      "coherence c_cv 0.3883756880629772\n",
      "shape of idxes is torch.Size([20, 20])\n",
      "tensor([  253,    97,   454,   196,  1114,   235, 13238,   609,    97,   131,\n",
      "          254,   317,  1654,  1402,   751,  1295,   433,    99,  3078,    97],\n",
      "       device='cuda:0')\n",
      "shape of vals is torch.Size([20, 15]) shape of index is torch.Size([20, 15])\n",
      "iterating over topics\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "wandb.watch(model, log=\"all\")\n",
    "#training(model = model, epochs=60, optimizer=optimizer, vocab = dictionary)\n",
    "train(model,batch_size=1024,\n",
    "      learning_rate=9e-4,test_data=None,\n",
    "      num_epochs=1000,is_evaluate=False,log_every=40,beta=1.0, \n",
    "      criterion='cross_entropy',ckpt=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6c33364",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6ca05c9698ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#topic_words = show_topic_words(model,topic_id=None,topK=15, dictionary=dictionary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv_coherence_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopic_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_tokens_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_uci'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcv_coherence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#CoherenceModel(model=model, corpus=docs, coherence='c_v', dictionary = dictionary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topic_words' is not defined"
     ]
    }
   ],
   "source": [
    "#topic_words = show_topic_words(model,topic_id=None,topK=15, dictionary=dictionary)\n",
    "cv_coherence_model = CoherenceModel(topics=topic_words,texts=x_tokens_train,dictionary=dictionary,coherence='c_uci')\n",
    "cv_coherence_model.get_coherence()\n",
    "#CoherenceModel(model=model, corpus=docs, coherence='c_v', dictionary = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb342f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38799502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size = get_topic_word_dist(model,normalize=True)\n",
    "#size.shape\n",
    "for i in np.argsort(size[0], axis = 0)[::-1][:15]:\n",
    "    print(dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = show_topic_words(model, topic_id = None, topK=15, id2token=None, dictionary=dictionary)\n",
    "#np.argsort(a[1][0].detach().cpu().numpy(),axis=0)\n",
    "#dictionary[2336]#id2token = {v:k for k,v in dictionary.token2id.items()}\n",
    "#np.argsort(a[1][0].detach().cpu().numpy(),axis=0)[0]\n",
    "assert a[1][0][5405]<a[1][0][2336]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921074c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_words(model,topn=15,n_topic=20, vocab=None,fix_topic=None, showWght=False):\n",
    "    # define topic list\n",
    "    topics = []\n",
    "    def show_one_tp(tp_idx):\n",
    "        if showWght:\n",
    "            return [(vocab.id2token[t[0]],t[1]) for t in model.get_topic_terms(tp_idx,topn=topn)]\n",
    "        else:\n",
    "            return [vocab.id2token[t[0]] for t in model.get_topic_terms(tp_idx,topn=topn)]\n",
    "    if fix_topic is None:\n",
    "        for i in range(n_topic):\n",
    "            topics.append(show_one_tp(i))\n",
    "    else:\n",
    "        topics.append(show_one_tp(fix_topic))\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_words(model,topn=15,n_topic=10,vocab=None,fix_topic=None,showWght=False):\n",
    "    topics = []\n",
    "    def show_one_tp(tp_idx):\n",
    "        if showWght:\n",
    "            return [(vocab.id2token[t[0]],t[1]) for t in model.get_topic_terms(tp_idx,topn=topn)]\n",
    "        else:\n",
    "            return [vocab.id2token[t[0]] for t in model.get_topic_terms(tp_idx,topn=topn)]\n",
    "    if fix_topic is None:\n",
    "        for i in range(n_topic):\n",
    "            topics.append(show_one_tp(i))\n",
    "    else:\n",
    "        topics.append(show_one_tp(fix_topic))\n",
    "    return topics\n",
    "get_topic_words(model, topn=15, n_topic=20, vocab=dictionary,fix_topic=None,showWght=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0877467",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(show_topic_words(model,topic_id=i,topK=15, dictionary=dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "a\n",
    "get_topics(model = mod, num_topics = 25, top_n_words= 5, vocabulary = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69cd0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    topics = []\n",
    "    gammas = model.get_beta()\n",
    "    for k in range(25):\n",
    "        gamma = gammas[k]\n",
    "        top_words = list(gamma.cpu().numpy().argsort())\n",
    "        topic_words = [vocab[a] for a in top_words]\n",
    "        topics.append(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas[3].cpu().numpy().argsort()\n",
    "[vocab[a] for a in gammas[2].cpu().numpy().argsort()][-10:-1]\n",
    "topics_words = []\n",
    "top_words = list(gammas[2].cpu().numpy().argsort()[-5+1:][::-1])\n",
    "topic_words = [vocab[a].strip() for a in top_words]\n",
    "topics_words.append(' '.join(topic_words))\n",
    "topics_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(self, args, vocabulary, show_emb=False):\n",
    "    Path.cwd().joinpath(\"results\").mkdir(parents=True, exist_ok=True)\n",
    "    self.eval()\n",
    "    model_path = Path.home().joinpath(\"Projects\", \n",
    "                                        \"Personal\", \n",
    "                                        \"balobi_nini\", \n",
    "                                        'models', \n",
    "                                        'embeddings_one_gram_fast_tweets_only').__str__()\n",
    "    model_gensim = FT_gensim.load(model_path)\n",
    "\n",
    "        # need to update this .. \n",
    "    queries = ['jazz','rock','radiohead']\n",
    "\n",
    "        ## visualize topics using monte carlo\n",
    "    with torch.no_grad():\n",
    "        print('#'*100)\n",
    "        print('Visualize topics...')\n",
    "        topics_words = []\n",
    "        gammas = self.get_beta()\n",
    "        for k in range(args.num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-args.num_words+1:][::-1])\n",
    "            topic_words = [vocabulary[a].strip() for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            if show_emb:\n",
    "                ## visualize word embeddings by using V to get nearest neighbors\n",
    "                print('#'*100)\n",
    "                print('Visualize word embeddings by using output embedding matrix')\n",
    "                try:\n",
    "                    embeddings = model.rho.weight  # Vocab_size x E\n",
    "                except:\n",
    "                    embeddings = self.rho         # Vocab_size x E\n",
    "                neighbors = []\n",
    "                for word in queries:\n",
    "                    print('word: {} .. neighbors: {}'.format(\n",
    "                        word, nearest_neighbors(model_gensim, word)))\n",
    "                print('#'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encode_dims=[2000,1024,512,20],decode_dims=[20,1024,2000],dropout=0.0):\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.ModuleDict({\n",
    "            f'enc_{i}':nn.Linear(encode_dims[i],encode_dims[i+1]) \n",
    "            for i in range(len(encode_dims)-2)\n",
    "        })\n",
    "        self.fc_mu = nn.Linear(encode_dims[-2],encode_dims[-1])\n",
    "        self.fc_logvar = nn.Linear(encode_dims[-2],encode_dims[-1])\n",
    "\n",
    "        self.decoder = nn.ModuleDict({\n",
    "            f'dec_{i}':nn.Linear(decode_dims[i],decode_dims[i+1])\n",
    "            for i in range(len(decode_dims)-1)\n",
    "        })\n",
    "        self.latent_dim = encode_dims[-1]\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc1 = nn.Linear(encode_dims[-1],encode_dims[-1])\n",
    "        \n",
    "        \n",
    "    def encode(self, x):\n",
    "        hid = x\n",
    "        for i,layer in self.encoder.items():\n",
    "            hid = F.relu(self.dropout(layer(hid)))\n",
    "        mu, log_var = self.fc_mu(hid), self.fc_logvar(hid)\n",
    "        return mu, log_var\n",
    "\n",
    "    def inference(self,x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        theta = torch.softmax(x,dim=1)\n",
    "        return theta\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        hid = z\n",
    "        for i,(_,layer) in enumerate(self.decoder.items()):\n",
    "            hid = layer(hid)\n",
    "            if i<len(self.decoder)-1:\n",
    "                hid = F.relu(self.dropout(hid))\n",
    "        return hid\n",
    "    \n",
    "    def forward(self, x, collate_fn=None):\n",
    "        mu, log_var = self.encode(x)\n",
    "        _theta = self.reparameterize(mu, log_var)\n",
    "        _theta = self.fc1(_theta) \n",
    "        if collate_fn!=None:\n",
    "            theta = collate_fn(_theta)\n",
    "        else:\n",
    "            theta = _theta\n",
    "        x_reconst = self.decode(theta)\n",
    "        return x_reconst, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVAE(VAE):\n",
    "    def __init__(self, encode_dims=[2000,1024,512,20],decode_dims=[20,1024,2000],dropout=0.0,emb_dim=300):\n",
    "        super(EVAE,self).__init__(encode_dims=encode_dims,decode_dims=decode_dims,dropout=dropout)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vocab_size = encode_dims[0]\n",
    "        self.n_topic = encode_dims[-1]\n",
    "        self.rho = nn.Linear(emb_dim,self.vocab_size)\n",
    "        self.alpha = nn.Linear(emb_dim,self.n_topic)\n",
    "        self.decoder = None\n",
    "\n",
    "    def decode(self,z):\n",
    "        wght_dec = self.alpha(self.rho.weight) #[K,V]\n",
    "        beta = F.softmax(wght_dec,dim=0).transpose(1,0)\n",
    "        res = torch.mm(z,beta)\n",
    "        logits = torch.log(res+1e-6)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ETM:\n",
    "    def __init__(self,bow_dim=10000,n_topic=20,taskname=None,device=None,emb_dim=300):\n",
    "        self.bow_dim = bow_dim\n",
    "        self.n_topic = n_topic\n",
    "        self.emb_dim = emb_dim\n",
    "        #TBD_fc1\n",
    "        self.vae = EVAE(encode_dims=[bow_dim,1024,512,n_topic],decode_dims=[n_topic,512,bow_dim],dropout=0.0,emb_dim=emb_dim)\n",
    "        self.device = device\n",
    "        self.id2token = None\n",
    "        self.taskname = taskname\n",
    "        if device!=None:\n",
    "            self.vae = self.vae.to(device)\n",
    "\n",
    "    def train(self,train_data,batch_size=256,learning_rate=1e-3,test_data=None,num_epochs=100,is_evaluate=False,log_every=5,beta=1.0,criterion='cross_entropy',ckpt=None):\n",
    "        self.vae.train()\n",
    "        self.id2token = {v:k for k,v in train_data.dictionary.token2id.items()}\n",
    "        data_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,num_workers=4,collate_fn=train_data.collate_fn)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.vae.parameters(),lr=learning_rate)\n",
    "        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        if ckpt:\n",
    "            self.load_model(ckpt[\"net\"])\n",
    "            optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "            start_epoch = ckpt[\"epoch\"] + 1\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "\n",
    "        trainloss_lst, valloss_lst = [], []\n",
    "        recloss_lst, klloss_lst = [],[]\n",
    "        c_v_lst, c_w2v_lst, c_uci_lst, c_npmi_lst, mimno_tc_lst, td_lst = [], [], [], [], [], []\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            epochloss_lst = []\n",
    "            for iter,data in enumerate(data_loader):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                txts,bows = data\n",
    "                bows = bows.to(self.device)\n",
    "                '''\n",
    "                n_samples = 20\n",
    "                rec_loss = torch.tensor(0.0).to(self.device)\n",
    "                for i in range(n_samples):\n",
    "                    bows_recon,mus,log_vars = self.vae(bows,lambda x:torch.softmax(x,dim=1))\n",
    "                    \n",
    "                    logsoftmax = torch.log_softmax(bows_recon,dim=1)\n",
    "                    _rec_loss = -1.0 * torch.sum(bows*logsoftmax)\n",
    "                    rec_loss += _rec_loss\n",
    "                rec_loss = rec_loss / n_samples\n",
    "                '''\n",
    "                bows_recon,mus,log_vars = self.vae(bows,lambda x:torch.softmax(x,dim=1))\n",
    "                if criterion=='cross_entropy':\n",
    "                    logsoftmax = torch.log_softmax(bows_recon,dim=1)\n",
    "                    rec_loss = -1.0 * torch.sum(bows*logsoftmax)\n",
    "                elif criterion=='bce_softmax':\n",
    "                    rec_loss = F.binary_cross_entropy(torch.softmax(bows_recon,dim=1),bows,reduction='sum')\n",
    "                elif criterion=='bce_sigmoid':\n",
    "                    rec_loss = F.binary_cross_entropy(torch.sigmoid(bows_recon),bows,reduction='sum')\n",
    "                \n",
    "                kl_div = -0.5 * torch.sum(1+log_vars-mus.pow(2)-log_vars.exp())\n",
    "                \n",
    "                loss = rec_loss + kl_div * beta\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                trainloss_lst.append(loss.item()/len(bows))\n",
    "                epochloss_lst.append(loss.item()/len(bows))\n",
    "                if (iter+1) % 10==0:\n",
    "                    print(f'Epoch {(epoch+1):>3d}\\tIter {(iter+1):>4d}\\tLoss:{loss.item()/len(bows):<.7f}\\tRec Loss:{rec_loss.item()/len(bows):<.7f}\\tKL Div:{kl_div.item()/len(bows):<.7f}')\n",
    "            #scheduler.step()\n",
    "            if (epoch+1) % log_every==0:\n",
    "                save_name = f'./ckpt/ETM_{self.taskname}_tp{self.n_topic}_{time.strftime(\"%Y-%m-%d-%H-%M\", time.localtime())}_ep{epoch+1}.ckpt'\n",
    "                checkpoint = {\n",
    "                    \"net\": self.vae.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"param\": {\n",
    "                        \"bow_dim\": self.bow_dim,\n",
    "                        \"n_topic\": self.n_topic,\n",
    "                        \"taskname\": self.taskname,\n",
    "                        \"emb_dim\": self.emb_dim\n",
    "                    }\n",
    "                }\n",
    "                torch.save(checkpoint,save_name)\n",
    "                # The code lines between this and the next comment lines are duplicated with WLDA.py, consider to simpify them.\n",
    "                print(f'Epoch {(epoch+1):>3d}\\tLoss:{sum(epochloss_lst)/len(epochloss_lst):<.7f}')\n",
    "                print('\\n'.join([str(lst) for lst in self.show_topic_words()]))\n",
    "                print('='*30)\n",
    "                smth_pts = smooth_curve(trainloss_lst)\n",
    "                plt.plot(np.array(range(len(smth_pts)))*log_every,smth_pts)\n",
    "                plt.xlabel('epochs')\n",
    "                plt.title('Train Loss')\n",
    "                plt.savefig('gsm_trainloss.png')\n",
    "                if test_data!=None:\n",
    "                    c_v,c_w2v,c_uci,c_npmi,mimno_tc, td = self.evaluate(test_data,calc4each=False)\n",
    "                    c_v_lst.append(c_v), c_w2v_lst.append(c_w2v), c_uci_lst.append(c_uci),c_npmi_lst.append(c_npmi), mimno_tc_lst.append(mimno_tc), td_lst.append(td)\n",
    "                save_name = f'./ckpt/ETM_{self.taskname}_tp{self.n_topic}_{time.strftime(\"%Y-%m-%d-%H-%M\", time.localtime())}.ckpt'\n",
    "                torch.save(self.vae.state_dict(),save_name)\n",
    "        scrs = {'c_v':c_v_lst,'c_w2v':c_w2v_lst,'c_uci':c_uci_lst,'c_npmi':c_npmi_lst,'mimno_tc':mimno_tc_lst,'td':td_lst}\n",
    "        '''\n",
    "        for scr_name,scr_lst in scrs.items():\n",
    "            plt.cla()\n",
    "            plt.plot(np.array(range(len(scr_lst)))*log_every,scr_lst)\n",
    "            plt.savefig(f'wlda_{scr_name}.png')\n",
    "        '''\n",
    "        plt.cla()\n",
    "        for scr_name,scr_lst in scrs.items():\n",
    "            if scr_name in ['c_v','c_w2v','td']:\n",
    "                plt.plot(np.array(range(len(scr_lst)))*log_every,scr_lst,label=scr_name)\n",
    "        plt.title('Topic Coherence')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'gsm_tc_scores.png')\n",
    "        # The code lines between this and the last comment lines are duplicated with WLDA.py, consider to simpify them.\n",
    "\n",
    "\n",
    "    def evaluate(self,test_data,calc4each=False):\n",
    "        topic_words = self.show_topic_words()\n",
    "        return evaluate_topic_quality(topic_words, test_data, taskname=self.taskname, calc4each=calc4each)\n",
    "\n",
    "\n",
    "    def inference_by_bow(self,doc_bow):\n",
    "        # doc_bow: torch.tensor [vocab_size]; optional: np.array [vocab_size]\n",
    "        if isinstance(doc_bow,np.ndarray):\n",
    "            doc_bow = torch.from_numpy(doc_bow)\n",
    "        doc_bow = doc_bow.reshape(-1,self.bow_dim).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            mu,log_var = self.vae.encode(doc_bow)\n",
    "            mu = self.vae.fc1(mu) \n",
    "            theta = F.softmax(mu,dim=1)\n",
    "            return theta.detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "\n",
    "    def inference(self, doc_tokenized, dictionary,normalize=True):\n",
    "        doc_bow = torch.zeros(1,self.bow_dim)\n",
    "        for token in doc_tokenized:\n",
    "            try:\n",
    "                idx = dictionary.token2id[token]\n",
    "                doc_bow[0][idx] += 1.0\n",
    "            except:\n",
    "                print(f'{token} not in the vocabulary.')\n",
    "        doc_bow = doc_bow.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            mu,log_var = self.vae.encode(doc_bow)\n",
    "            mu = self.vae.fc1(mu)\n",
    "            if normalize:\n",
    "                theta = F.softmax(mu,dim=1)\n",
    "            return theta.detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "    def get_embed(self,train_data, num=1000):\n",
    "        self.vae.eval()\n",
    "        data_loader = DataLoader(train_data, batch_size=512,shuffle=False, num_workers=4, collate_fn=train_data.collate_fn)\n",
    "        embed_lst = []\n",
    "        txt_lst = []\n",
    "        cnt = 0\n",
    "        for data_batch in data_loader:\n",
    "            txts, bows = data_batch\n",
    "            embed = self.inference_by_bow(bows)\n",
    "            embed_lst.append(embed)\n",
    "            txt_lst.append(txts)\n",
    "            cnt += embed.shape[0]\n",
    "            if cnt>=num:\n",
    "                break\n",
    "        embed_lst = np.concatenate(embed_lst,axis=0)[:num]\n",
    "        txt_lst = np.concatenate(txt_lst,axis=0)[:num]\n",
    "        return txt_lst, embed_lst\n",
    "\n",
    "    def get_topic_word_dist(self,normalize=True):\n",
    "        self.vae.eval()\n",
    "        with torch.no_grad():\n",
    "            idxes = torch.eye(self.n_topic).to(self.device)\n",
    "            word_dist = self.vae.decode(idxes)  # word_dist: [n_topic, vocab.size]\n",
    "            if normalize:\n",
    "                word_dist = F.softmax(word_dist,dim=1)\n",
    "            return word_dist.detach().cpu().numpy()\n",
    "\n",
    "    def show_topic_words(self,topic_id=None,topK=15, dictionary=None):\n",
    "        topic_words = []\n",
    "        idxes = torch.eye(self.n_topic).to(self.device)\n",
    "        word_dist = self.vae.decode(idxes)\n",
    "        word_dist = torch.softmax(word_dist,dim=1)\n",
    "        vals,indices = torch.topk(word_dist,topK,dim=1)\n",
    "        vals = vals.cpu().tolist()\n",
    "        indices = indices.cpu().tolist()\n",
    "        if self.id2token==None and dictionary!=None:\n",
    "            self.id2token = {v:k for k,v in dictionary.token2id.items()}\n",
    "        if topic_id==None:\n",
    "            for i in range(self.n_topic):\n",
    "                topic_words.append([self.id2token[idx] for idx in indices[i]])\n",
    "        else:\n",
    "            topic_words.append([self.id2token[idx] for idx in indices[topic_id]])\n",
    "        return topic_words\n",
    "\n",
    "    def load_model(self, model):\n",
    "        self.vae.load_state_dict(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = EVAE(encode_dims=[1024,512,256,20],decode_dims=[20,128,768,1024],emb_dim=300)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347381b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ETM(dropout=0.0,num_topics = 20, vocab_size = len(dictionary), t_hidden_size = 800, emb_dim = 300,  \n",
    "          embeddings=None, train_embeddings=True, drop=0.0)\n",
    "\n",
    "mod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
