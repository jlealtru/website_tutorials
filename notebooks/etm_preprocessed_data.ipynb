{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5a3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Tuple, List\n",
    "\n",
    "from smart_open import smart_open\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import multiprocessing\n",
    "#from gensim import \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "# explore the topics discussed in the reviews and compare how they have evolved over time\n",
    "# We will define a series of functions to achieve that objective\n",
    "from gensim.models import phrases\n",
    "from smart_open import smart_open\n",
    "from gensim.utils import simple_preprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the reviews\n",
    "pitchfork = pd.read_csv('../data/pitchfork/pitchfork.csv', low_memory=False, \n",
    "                       encoding = 'utf-8')\n",
    "pitchfork.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7115e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the stopwords\n",
    "with open('../data/pitchfork/stop.txt', 'r') as f:\n",
    "    stop_words = f.read().split('\\n')\n",
    "stop_words = stop_words+['songs', 'albums','record','records', 'album', 'sound', 'music',\n",
    "                        'band', 'song', 'time', 'years','fuck', '\\\\xa0', 'kind','single','shit',\n",
    "                         'good', '\\n', 'great','\\xa0', 'sing','rock','bad','guy','lyric',\n",
    "                        'lot', 'sing','band','rock','man','girl','listen','day','bands','record',\n",
    "                         'records', 'guitars','thing','pretty','artist','things',\n",
    "                         'people','stuff']\n",
    "#stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pitchfork['review'] = pitchfork['review'].values.astype('str')\n",
    "documents = pitchfork['review'].tolist()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [str(doc) for doc in documents if doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca62cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "documents = [doc for doc in documents if len(doc)>200]\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2068a744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of vocabulary is  7291\n"
     ]
    }
   ],
   "source": [
    "# load dictionary\n",
    "dictionary = Dictionary.load('../data/pitchfork/dict.pkl')\n",
    "bows = gensim.corpora.MmCorpus('../data/pitchfork/corpus.mm')\n",
    "docs = pickle.load(open('../data/pitchfork/docs.pkl','rb'))\n",
    "print('len of vocabulary is ',len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1433984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_bows_train, x_bows_test = train_test_split(bows, test_size=0.15, random_state=192)\n",
    "x_tokens_train, x_tokens_test = train_test_split(docs, test_size=0.15, random_state=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "365d501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of training size is 5757 and tokes are 5757 len of test size is 1017 and tokens are 1017)\n"
     ]
    }
   ],
   "source": [
    "#item = list(zip(*bows[0]))\n",
    "#torch.tensor(list(item[1])).float()\n",
    "print(f'len of training size is {len(x_bows_train)} and tokes are {len(x_tokens_train)} '\n",
    "      f'len of test size is {len(x_bows_test)} and tokens are {len(x_tokens_test)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3304a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "\n",
    "class Data_Processing(object):\n",
    "    def __init__(self, docs, bows, vocab):\n",
    "        self.docs = docs\n",
    "        self.bows = bows\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        bow = torch.zeros(len(self.vocab))\n",
    "        # create token and frequency\n",
    "        item = list(zip(*self.bows[idx])) # bow = [[token_id1,token_id2,...],[freq1,freq2,...]]\n",
    "        # create\n",
    "        bow[list(item[0])] = torch.tensor(list(item[1])).float()\n",
    "        txt = self.docs[idx]\n",
    "        #print(f'shape of bow before being put together in data loader {bow.shape} {type(bow)}')\n",
    "        return txt, bow\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "    \n",
    "    def collate_fn1(self,batch_data):\n",
    "        texts,bows = list(zip(*batch_data))\n",
    "        #print(f'what happens with collate function {torch.stack(bows,dim=0)}, {torch.stack(bows,dim=0).shape}')\n",
    "        return texts,torch.stack(bows,dim=0)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs:\n",
    "            yield doc\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# create a class to process the traininga and test data\n",
    "training_data = Data_Processing(x_tokens_train, x_bows_train, dictionary)\n",
    "test_data = Data_Processing(x_tokens_test, x_bows_test, dictionary)\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4,\n",
    "                                        collate_fn=training_data.collate_fn1),\n",
    "                    'test': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4,\n",
    "                                        collate_fn=test_data.collate_fn1)}\n",
    "dataset_sizes = {'train':len(training_data)}\n",
    "example = next(iter(dataloaders_dict.get('train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30badf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7291])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0df65449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class ETM(nn.Module):\n",
    "    # instantiate the model\n",
    "    def __init__(self, device = None, num_topics = 20, vocab_size = None, t_hidden_size = 1024, \n",
    "                 rho_size = 300, emb_size=300, theta_act='relu',\n",
    "                 embeddings=None, train_embeddings=True, enc_drop=0.5, debug_mode=False):\n",
    "        '''\n",
    "        Creates an embedded topic model instance. The model hyperparameters are:\n",
    "        device: if using cuda or cpu\n",
    "        num_topics: number of topics to be used\n",
    "        vocab_size: size of the vocabulary\n",
    "        t_hidden_size:  \n",
    "        embeddings (str or KeyedVectors): KeyedVectors instance containing word-vector mapping for embeddings, \n",
    "                                          or its path\n",
    "        use_c_format_w2vec (bool): wheter input embeddings use word2vec C format. Both BIN and TXT formats are \n",
    "                                   supported\n",
    "        model_path (str): path to save trained model. If None, the model won't be automatically saved\n",
    "        batch_size (int): input batch size for training\n",
    "        num_topics (int): number of topics\n",
    "        rho_size (int): dimension of rho\n",
    "        emb_size (int): dimens-ion of embeddingsfno\n",
    "        t_hidden_size (int): dimension of hidden space of q(theta)\n",
    "        theta_act (str): activation function can be tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu, etc.\n",
    "        train_embeddings (bool): whether to fix rho or train it, defaults to True\n",
    "        \n",
    "        # separate this part\n",
    "        lr (float): learning rate\n",
    "        lr_factor (float): divide learning rate by this...\n",
    "        epochs (int): number of epochs to train. 150 for 20ng 100 for others\n",
    "        optimizer_type (str): choice of optimizer\n",
    "        seed (int): random seed (default: 1)\n",
    "        enc_drop (float): dropout rate on encoder\n",
    "        clip (float): gradient clipping\n",
    "        nonmono (int): number of bad hits allowed\n",
    "        wdecay (float): some l2 regularization\n",
    "        anneal_lr (bool): whether to anneal the learning rate or not\n",
    "        bow_norm (bool): normalize the bows or not\n",
    "        num_words (int): number of words for topic viz\n",
    "        log_interval (int): when to log training\n",
    "        visualize_every (int): when to visualize results\n",
    "        eval_batch_size (int): input batch size for evaluation\n",
    "        eval_perplexity (bool): whether to compute perplexity on document completion task\n",
    "        debug_mode (bool): wheter or not should log model operations\n",
    "        '''\n",
    "        super(ETM, self).__init__()\n",
    "        # define hyperparameters\n",
    "        self.num_topics = num_topics\n",
    "        self.vocab_size = vocab_size\n",
    "        # in case we want to train the embeddings this parameter controls the hidden size of the embeddings\n",
    "        self.rho_size = rho_size\n",
    "        # dimension of the matrix for the topic mixture (topics hidden size)\n",
    "        self.t_hidden_size = t_hidden_size\n",
    "        # dropout to be applied to the encoder\n",
    "        self.enc_drop = enc_drop\n",
    "        # size of the embeddings in case a pretrained embedding is provided\n",
    "        self.emb_size = emb_size\n",
    "        # dropout rate\n",
    "        self.t_drop = nn.Dropout(enc_drop)\n",
    "        self.debug_mode = debug_mode\n",
    "        # activation for theta\n",
    "        self.theta_act = self.get_activation(theta_act)\n",
    "        # device to use for training\n",
    "        self.device = device\n",
    "\n",
    "        # define the word embedding matrix \\rho\n",
    "        if train_embeddings:\n",
    "            self.rho = nn.Linear(rho_size, vocab_size, bias=True)\n",
    "        else:\n",
    "            # get the dimension of the embeddings\n",
    "            print(embeddings.size())\n",
    "            num_embeddings, emb_size = embeddings.size()\n",
    "            # instantiate rho embeddings\n",
    "            self.rho = embeddings.clone().float().to(self.device)\n",
    "\n",
    "        # define the matrix containing the topic embeddings\n",
    "        self.alphas = nn.Linear(rho_size, num_topics, bias=False)\n",
    "\n",
    "        # define variational distribution for \\theta_{1:D} via amortization\n",
    "        # this takes the vocabulary size, creates the hidden size for the\n",
    "        # topics matrix, applies activation function, applies another linear\n",
    "        # layer followed by additional activation function\n",
    "        self.q_theta = nn.Sequential(nn.Linear(vocab_size, t_hidden_size),\n",
    "            self.theta_act,\n",
    "            nn.Linear(t_hidden_size, t_hidden_size),\n",
    "            self.theta_act,\n",
    "        )\n",
    "        \n",
    "        # linear transformation of the topics hidden size to the number of topics\n",
    "        self.mu_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
    "        \n",
    "        # another linear transformation from hidden size to number of topics\n",
    "        self.logsigma_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
    "\n",
    "    def get_activation(self, act = 'relu'):\n",
    "        '''\n",
    "        Define the activation function options available are tahn, relu, softplus, rrelu, \n",
    "        leakyrelu, elu, selu and glu.\n",
    "        '''\n",
    "        if act == 'tanh':\n",
    "            act = nn.Tanh()\n",
    "        elif act == 'relu':\n",
    "            act = nn.ReLU()\n",
    "        elif act == 'softplus':\n",
    "            act = nn.Softplus()\n",
    "        elif act == 'rrelu':\n",
    "            act = nn.RReLU()\n",
    "        elif act == 'leakyrelu':\n",
    "            act = nn.LeakyReLU()\n",
    "        elif act == 'elu':\n",
    "            act = nn.ELU()\n",
    "        elif act == 'selu':\n",
    "            act = nn.SELU()\n",
    "        elif act == 'glu':\n",
    "            act = nn.GLU()\n",
    "        else:\n",
    "            act = nn.Tanh()\n",
    "            if self.debug_mode:\n",
    "                print('Defaulting to tanh activation')\n",
    "        return act\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Returns a sample from a Gaussian distribution via reparameterization.\n",
    "        mu stands for\n",
    "        logvar stands for \n",
    "        reference for reparametrizetion trick. \n",
    "        https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul_(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def encode(self, bows):\n",
    "        \"\"\"Returns paramters of the variational distribution for \\theta.\n",
    "        input: bows batch of bag-of-words...tensor of shape bsz x V (batch size* vocabulary)\n",
    "        output: mu_theta, log_sigma_theta\n",
    "        \"\"\"\n",
    "        \n",
    "        q_theta = self.q_theta(bows)\n",
    "        if self.enc_drop > 0:\n",
    "            q_theta = self.t_drop(q_theta)\n",
    "        # mu_q_theta takes the topic hidden size and outputs a vector with the dimension of the topics\n",
    "        mu_theta = self.mu_q_theta(q_theta)\n",
    "        # takes the topic hidden size and outpus another vector equal to the size of the vectors\n",
    "        logsigma_theta = self.logsigma_q_theta(q_theta)\n",
    "        # returns kl theta which is a normalized number\n",
    "        kl_theta = -0.5 * torch.sum(1 + logsigma_theta - mu_theta.pow(2) - logsigma_theta.exp(), dim=-1).mean()\n",
    "        #print(f'this is kl_theta {kl_theta}')\n",
    "        return mu_theta, logsigma_theta, kl_theta\n",
    "       \n",
    "    def get_beta(self):\n",
    "        \"\"\"\n",
    "        This generate the description as a defintion over words\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logit = self.alphas(self.rho.weight) # torch.mm(self.rho, self.alphas)\n",
    "        except:\n",
    "            logit = self.alphas(self.rho)\n",
    "        # gets logits over vocabulary dimension when fitting to the topic\n",
    "        beta = F.softmax(logit, dim=0).transpose(1, 0) ## softmax over vocab dimension\n",
    "        #print(f'this is beta {beta}')\n",
    "        return beta\n",
    "\n",
    "    def get_theta(self, normalized_bows):\n",
    "        \"\"\"\n",
    "        getting the topic poportion for the document passed in the normalixe bow or tf-idf\n",
    "        \"\"\"\n",
    "        # gets mu_theta (linear layer for topic dimension), logsima_theta another liner layer output\n",
    "        # and the kld_tjet\n",
    "        mu_theta, logsigma_theta, kld_theta = self.encode(normalized_bows)\n",
    "        # gets variable to backpropagate, returns  reparametized mu theta\n",
    "        z = self.reparameterize(mu_theta, logsigma_theta)\n",
    "        # applied softmax and returns squeezed logits\n",
    "        theta = F.softmax(z, dim=-1) \n",
    "        #print(f'this is theta {theta}')\n",
    "        return theta, kld_theta\n",
    "\n",
    "    def decode(self, theta, beta):\n",
    "        \"\"\"compute the probability of topic given the document which is equal to theta^T ** B\n",
    "        Args:\n",
    "            theta ([type]): vector of proabilities of belonging to a given topic\n",
    "            beta ([type]): probability of belonging to a given topic\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        # multiplication of softmax with beta (likelihood of belonging to a topic given likelihood a word\n",
    "        # belongs to another topic)\n",
    "        res = torch.mm(theta, beta)\n",
    "        # normalizes predictions\n",
    "        almost_zeros = torch.full_like(res, 1e-6)\n",
    "        results_without_zeros = res.add(almost_zeros)\n",
    "        predictions = torch.log(results_without_zeros)\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, bows, normalized_bows, theta=None, aggregate=True):\n",
    "        # get \\theta\n",
    "        if theta is None:\n",
    "            theta, kld_theta = self.get_theta(normalized_bows)\n",
    "        else:\n",
    "            kld_theta = None\n",
    "\n",
    "        # get \\beta\n",
    "        beta = self.get_beta()\n",
    "\n",
    "        # get prediction loss\n",
    "        preds = self.decode(theta, beta)\n",
    "        recon_loss = -(preds * bows).sum(1)\n",
    "        if aggregate:\n",
    "            recon_loss = recon_loss.mean()\n",
    "        return recon_loss, kld_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "924172fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dacb94b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ETM(device = device, num_topics = 20, vocab_size =len(dictionary), \n",
    "            t_hidden_size = 1024, rho_size = 300, \n",
    "            emb_size=300, theta_act='tahn',\n",
    "            embeddings=None, train_embeddings=True, enc_drop=0.4, debug_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "466798d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ETM(\n",
       "  (t_drop): Dropout(p=0.4, inplace=False)\n",
       "  (theta_act): Tanh()\n",
       "  (rho): Linear(in_features=300, out_features=7291, bias=True)\n",
       "  (alphas): Linear(in_features=300, out_features=20, bias=False)\n",
       "  (q_theta): Sequential(\n",
       "    (0): Linear(in_features=7291, out_features=1024, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (mu_q_theta): Linear(in_features=1024, out_features=20, bias=True)\n",
       "  (logsigma_q_theta): Linear(in_features=1024, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b56170f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def expand_array(vocab,index_array,values_array):\n",
    "#    a = np.zeros(len(vocab))\n",
    "#    a[train_dataset['tokens'][0]] = [x for x in train_dataset['counts'][0].tolist()]\n",
    "#    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5536b6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54d1717b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.003\n",
       "    weight_decay: 0.001\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler, AdamW, SGD,Adam\n",
    "lr = 0.003\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay = 0.001)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a917cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.forward()\n",
    "def get_topics(model = None, num_topics = 25, top_n_words=10, vocabulary = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Gets topics. By default, returns the 10 most relevant terms for each topic.\n",
    "        Parameters:\n",
    "        ===\n",
    "            top_n_words (int): number of top words per topic to return\n",
    "        Returns:\n",
    "        ===\n",
    "            list of str: topic list\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            topics = []\n",
    "            gammas = model.get_beta()\n",
    "\n",
    "            for k in range(model.num_topics):\n",
    "                gamma = gammas[k]\n",
    "                top_words = list(gamma.cpu().numpy().argsort()\n",
    "                                 [-top_n_words:][::-1])\n",
    "                topic_words = [vocabulary[a] for a in top_words]\n",
    "                topics.append(topic_words)\n",
    "\n",
    "            return topics\n",
    "\n",
    "def get_topic_words(model,dictionary):\n",
    "    with torch.no_grad():\n",
    "        topic_words = []\n",
    "        gammas = model.get_beta()\n",
    "        \n",
    "        for k in range(model.num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[::-1])\n",
    "            tp = [dictionary[a] for a in top_words]\n",
    "            topic_words.append(tp)\n",
    "    \n",
    "    return topic_words\n",
    "        \n",
    "def calc_topic_diversity(topic_words):\n",
    "    '''topic_words is in the form of [[w11,w12,...],[w21,w22,...]]'''\n",
    "    vocab = set(sum(topic_words,[]))\n",
    "    n_total = len(topic_words) * len(topic_words[0])\n",
    "    print(len(topic_words))\n",
    "    topic_div = len(vocab) / n_total\n",
    "    return topic_div\n",
    "\n",
    "\n",
    "def get_topic_diversity(model, topk=50):\n",
    "    with torch.no_grad():\n",
    "        beta = model.get_beta()\n",
    "        num_topics = model.num_topics\n",
    "        print(beta.shape, num_topics)\n",
    "        list_w = np.zeros((num_topics, topk))\n",
    "        print(list_w.shape)\n",
    "        for k in range(num_topics):\n",
    "            idx = beta[k,:].cpu().numpy().argsort()[-topk:][::-1]\n",
    "            list_w[k,:] = idx\n",
    "        n_unique = len(np.unique(list_w))\n",
    "        TD = n_unique / (topk * num_topics)\n",
    "        #print('Topic diveristy is: {}'.format(TD))\n",
    "        return TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3fa021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def nearest_neighbors(word, embeddings, vocab, n_most_similar=5):\n",
    "    vectors = embeddings.data.cpu().numpy()\n",
    "    index = dictionary.token2id[word]\n",
    "    query = vectors[index]\n",
    "    ranks = vectors.dot(query).squeeze()\n",
    "    denom = query.T.dot(query).squeeze()\n",
    "    denom = denom * np.sum(vectors**2, 1)\n",
    "    denom = np.sqrt(denom)\n",
    "    ranks = ranks / denom\n",
    "    mostSimilar = []\n",
    "    [mostSimilar.append(idx) for idx in ranks.argsort()[::-1]]\n",
    "    nearest_neighbors = mostSimilar[0:n_most_similar]\n",
    "    nearest_neighbors = [vocab[comp] for comp in nearest_neighbors]\n",
    "    return nearest_neighbors\n",
    "\n",
    "def get_most_similar_words(model = None, queries=[], vocabulary = None, n_most_similar=5) -> dict:\n",
    "        \"\"\"\n",
    "        Gets the nearest neighborhoring words for a list of tokens. By default, returns the 20 most similar words for each token in 'queries' array.\n",
    "        Parameters:\n",
    "        ===\n",
    "            queries (list of str): words to find similar ones\n",
    "            n_most_similar (int): number of most similar words to get for each word given in the input. By default is 20\n",
    "        Returns:\n",
    "        ===\n",
    "            dict of (str, list of str): dictionary containing the mapping between query words given and their respective similar words\n",
    "        \"\"\"\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # visualize word embeddings by using V to get nearest neighbors\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.rho.weight  # Vocab_size x E\n",
    "            \n",
    "\n",
    "            neighbors = {}\n",
    "            for word in queries:\n",
    "                neighbors[word] = nearest_neighbors(\n",
    "                    word,embeddings, vocabulary, n_most_similar)\n",
    "\n",
    "            return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4cdbbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb\n",
    "#wandb.init(entity=\"jlealtru\", project=\"ETM_runs_p\")\n",
    "#config = wandb.config          # Initialize config\n",
    "\n",
    "def train(model, batch_size=256,dictionary = None,\n",
    "          learning_rate=2e-3,test_data=None,\n",
    "          num_epochs=600,is_evaluate=False,log_every=40,ckpt=None):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    data_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4,\n",
    "                                        collate_fn=training_data.collate_fn1)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    if ckpt:\n",
    "        self.load_model(ckpt[\"net\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        start_epoch = ckpt[\"epoch\"] + 1\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "    \n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    trainloss_lst, valloss_lst = [], []\n",
    "    recloss_lst, klloss_lst = [],[]\n",
    "    c_v_lst, c_w2v_lst, c_uci_lst, c_npmi_lst, mimno_tc_lst, td_lst = [], [], [], [], [], []\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        epochloss_lst = []\n",
    "        model.train()\n",
    "        for iter_,data in enumerate(data_loader):\n",
    "            #optimizer.zero_grad()\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            \n",
    "            txts,bows = data\n",
    "            bows = bows.to(device)\n",
    "            normalized_bows = bows\n",
    "            normalized_bows.to(device)\n",
    "            \n",
    "            recon_loss, kld_theta = model.forward(bows, normalized_bows)\n",
    "            total_loss = recon_loss + kld_theta\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += torch.sum(recon_loss).item()\n",
    "            acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "            cnt += 1\n",
    "            if iter_ % 4 == 0:\n",
    "                cur_loss = round(acc_loss / cnt, 2) \n",
    "                cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "                cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "\n",
    "                print('Epoch: {} KL_theta: is {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                    epoch, cur_kl_theta, cur_loss, cur_real_loss))\n",
    "\n",
    "        cur_loss = round(acc_loss / cnt, 2) \n",
    "        cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "        cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "        print('*'*100)\n",
    "        print('Epoch: {} KL_theta: is {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                    epoch, cur_kl_theta, cur_loss, cur_real_loss))\n",
    "        if (epoch+1)%40==0:\n",
    "            topic_words = get_topic_words(model, dictionary)\n",
    "            topic_diversity = get_topic_diversity(model,topk=100)\n",
    "            #calc_topic_diversity(topic_words)\n",
    "            print(f'topic diversity is {topic_diversity}')\n",
    "            pprint(get_topics(model = model, num_topics = 25, top_n_words= 10, vocabulary = dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "939400a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 KL_theta: is 0.04 .. Rec_loss: 1501.15 .. NELBO: 1501.19\n",
      "Epoch: 0 KL_theta: is 6.03 .. Rec_loss: 1516.79 .. NELBO: 1522.82\n",
      "****************************************************************************************************\n",
      "Epoch: 0 KL_theta: is 5.2 .. Rec_loss: 1514.48 .. NELBO: 1519.68\n",
      "Epoch: 1 KL_theta: is 4.79 .. Rec_loss: 1514.88 .. NELBO: 1519.67\n",
      "Epoch: 1 KL_theta: is 3.74 .. Rec_loss: 1511.65 .. NELBO: 1515.39\n",
      "****************************************************************************************************\n",
      "Epoch: 1 KL_theta: is 3.58 .. Rec_loss: 1510.93 .. NELBO: 1514.51\n",
      "Epoch: 2 KL_theta: is 3.36 .. Rec_loss: 1510.68 .. NELBO: 1514.04\n",
      "Epoch: 2 KL_theta: is 2.79 .. Rec_loss: 1505.0 .. NELBO: 1507.79\n",
      "****************************************************************************************************\n",
      "Epoch: 2 KL_theta: is 2.67 .. Rec_loss: 1504.8 .. NELBO: 1507.47\n",
      "Epoch: 3 KL_theta: is 2.54 .. Rec_loss: 1504.37 .. NELBO: 1506.91\n",
      "Epoch: 3 KL_theta: is 2.17 .. Rec_loss: 1498.2 .. NELBO: 1500.37\n",
      "****************************************************************************************************\n",
      "Epoch: 3 KL_theta: is 2.09 .. Rec_loss: 1495.83 .. NELBO: 1497.92\n",
      "Epoch: 4 KL_theta: is 2.01 .. Rec_loss: 1493.49 .. NELBO: 1495.5\n",
      "Epoch: 4 KL_theta: is 1.78 .. Rec_loss: 1487.8 .. NELBO: 1489.58\n",
      "****************************************************************************************************\n",
      "Epoch: 4 KL_theta: is 1.72 .. Rec_loss: 1487.31 .. NELBO: 1489.03\n",
      "Epoch: 5 KL_theta: is 1.67 .. Rec_loss: 1485.33 .. NELBO: 1487.0\n",
      "Epoch: 5 KL_theta: is 1.49 .. Rec_loss: 1480.95 .. NELBO: 1482.44\n",
      "****************************************************************************************************\n",
      "Epoch: 5 KL_theta: is 1.46 .. Rec_loss: 1479.78 .. NELBO: 1481.24\n",
      "Epoch: 6 KL_theta: is 1.42 .. Rec_loss: 1478.62 .. NELBO: 1480.04\n",
      "Epoch: 6 KL_theta: is 1.29 .. Rec_loss: 1474.72 .. NELBO: 1476.01\n",
      "****************************************************************************************************\n",
      "Epoch: 6 KL_theta: is 1.26 .. Rec_loss: 1474.09 .. NELBO: 1475.35\n",
      "Epoch: 7 KL_theta: is 1.23 .. Rec_loss: 1473.36 .. NELBO: 1474.59\n",
      "Epoch: 7 KL_theta: is 1.14 .. Rec_loss: 1470.29 .. NELBO: 1471.43\n",
      "****************************************************************************************************\n",
      "Epoch: 7 KL_theta: is 1.11 .. Rec_loss: 1469.47 .. NELBO: 1470.58\n",
      "Epoch: 8 KL_theta: is 1.09 .. Rec_loss: 1468.73 .. NELBO: 1469.82\n",
      "Epoch: 8 KL_theta: is 1.01 .. Rec_loss: 1466.4 .. NELBO: 1467.41\n",
      "****************************************************************************************************\n",
      "Epoch: 8 KL_theta: is 1.0 .. Rec_loss: 1465.81 .. NELBO: 1466.81\n",
      "Epoch: 9 KL_theta: is 0.98 .. Rec_loss: 1465.14 .. NELBO: 1466.12\n",
      "Epoch: 9 KL_theta: is 0.92 .. Rec_loss: 1462.89 .. NELBO: 1463.81\n",
      "****************************************************************************************************\n",
      "Epoch: 9 KL_theta: is 0.9 .. Rec_loss: 1463.13 .. NELBO: 1464.03\n",
      "Epoch: 10 KL_theta: is 0.89 .. Rec_loss: 1462.91 .. NELBO: 1463.8\n",
      "Epoch: 10 KL_theta: is 0.84 .. Rec_loss: 1460.71 .. NELBO: 1461.55\n",
      "****************************************************************************************************\n",
      "Epoch: 10 KL_theta: is 0.83 .. Rec_loss: 1460.91 .. NELBO: 1461.74\n",
      "Epoch: 11 KL_theta: is 0.81 .. Rec_loss: 1460.56 .. NELBO: 1461.37\n",
      "Epoch: 11 KL_theta: is 0.77 .. Rec_loss: 1458.94 .. NELBO: 1459.71\n",
      "****************************************************************************************************\n",
      "Epoch: 11 KL_theta: is 0.76 .. Rec_loss: 1458.98 .. NELBO: 1459.74\n",
      "Epoch: 12 KL_theta: is 0.75 .. Rec_loss: 1458.8 .. NELBO: 1459.55\n",
      "Epoch: 12 KL_theta: is 0.72 .. Rec_loss: 1457.61 .. NELBO: 1458.33\n",
      "****************************************************************************************************\n",
      "Epoch: 12 KL_theta: is 0.71 .. Rec_loss: 1457.13 .. NELBO: 1457.84\n",
      "Epoch: 13 KL_theta: is 0.7 .. Rec_loss: 1457.19 .. NELBO: 1457.89\n",
      "Epoch: 13 KL_theta: is 0.67 .. Rec_loss: 1456.04 .. NELBO: 1456.71\n",
      "****************************************************************************************************\n",
      "Epoch: 13 KL_theta: is 0.66 .. Rec_loss: 1455.52 .. NELBO: 1456.18\n",
      "Epoch: 14 KL_theta: is 0.65 .. Rec_loss: 1454.91 .. NELBO: 1455.56\n",
      "Epoch: 14 KL_theta: is 0.63 .. Rec_loss: 1454.43 .. NELBO: 1455.06\n",
      "****************************************************************************************************\n",
      "Epoch: 14 KL_theta: is 0.62 .. Rec_loss: 1454.2 .. NELBO: 1454.82\n",
      "Epoch: 15 KL_theta: is 0.62 .. Rec_loss: 1453.86 .. NELBO: 1454.48\n",
      "Epoch: 15 KL_theta: is 0.59 .. Rec_loss: 1453.33 .. NELBO: 1453.92\n",
      "****************************************************************************************************\n",
      "Epoch: 15 KL_theta: is 0.59 .. Rec_loss: 1453.01 .. NELBO: 1453.6\n",
      "Epoch: 16 KL_theta: is 0.58 .. Rec_loss: 1452.86 .. NELBO: 1453.44\n",
      "Epoch: 16 KL_theta: is 0.56 .. Rec_loss: 1452.06 .. NELBO: 1452.62\n",
      "****************************************************************************************************\n",
      "Epoch: 16 KL_theta: is 0.56 .. Rec_loss: 1452.07 .. NELBO: 1452.63\n",
      "Epoch: 17 KL_theta: is 0.55 .. Rec_loss: 1451.92 .. NELBO: 1452.47\n",
      "Epoch: 17 KL_theta: is 0.54 .. Rec_loss: 1451.33 .. NELBO: 1451.87\n",
      "****************************************************************************************************\n",
      "Epoch: 17 KL_theta: is 0.53 .. Rec_loss: 1451.17 .. NELBO: 1451.7\n",
      "Epoch: 18 KL_theta: is 0.53 .. Rec_loss: 1451.08 .. NELBO: 1451.61\n",
      "Epoch: 18 KL_theta: is 0.51 .. Rec_loss: 1450.64 .. NELBO: 1451.15\n",
      "****************************************************************************************************\n",
      "Epoch: 18 KL_theta: is 0.51 .. Rec_loss: 1450.27 .. NELBO: 1450.78\n",
      "Epoch: 19 KL_theta: is 0.5 .. Rec_loss: 1450.02 .. NELBO: 1450.52\n",
      "Epoch: 19 KL_theta: is 0.49 .. Rec_loss: 1449.55 .. NELBO: 1450.04\n",
      "****************************************************************************************************\n",
      "Epoch: 19 KL_theta: is 0.49 .. Rec_loss: 1449.63 .. NELBO: 1450.12\n",
      "Epoch: 20 KL_theta: is 0.48 .. Rec_loss: 1449.57 .. NELBO: 1450.05\n",
      "Epoch: 20 KL_theta: is 0.47 .. Rec_loss: 1449.18 .. NELBO: 1449.65\n",
      "****************************************************************************************************\n",
      "Epoch: 20 KL_theta: is 0.47 .. Rec_loss: 1448.91 .. NELBO: 1449.38\n",
      "Epoch: 21 KL_theta: is 0.47 .. Rec_loss: 1448.68 .. NELBO: 1449.15\n",
      "Epoch: 21 KL_theta: is 0.46 .. Rec_loss: 1448.48 .. NELBO: 1448.94\n",
      "****************************************************************************************************\n",
      "Epoch: 21 KL_theta: is 0.45 .. Rec_loss: 1448.28 .. NELBO: 1448.73\n",
      "Epoch: 22 KL_theta: is 0.45 .. Rec_loss: 1448.1 .. NELBO: 1448.55\n",
      "Epoch: 22 KL_theta: is 0.44 .. Rec_loss: 1447.79 .. NELBO: 1448.23\n",
      "****************************************************************************************************\n",
      "Epoch: 22 KL_theta: is 0.44 .. Rec_loss: 1447.76 .. NELBO: 1448.2\n",
      "Epoch: 23 KL_theta: is 0.44 .. Rec_loss: 1447.46 .. NELBO: 1447.9\n",
      "Epoch: 23 KL_theta: is 0.43 .. Rec_loss: 1447.32 .. NELBO: 1447.75\n",
      "****************************************************************************************************\n",
      "Epoch: 23 KL_theta: is 0.43 .. Rec_loss: 1447.28 .. NELBO: 1447.71\n",
      "Epoch: 24 KL_theta: is 0.42 .. Rec_loss: 1447.2 .. NELBO: 1447.62\n",
      "Epoch: 24 KL_theta: is 0.42 .. Rec_loss: 1446.86 .. NELBO: 1447.28\n",
      "****************************************************************************************************\n",
      "Epoch: 24 KL_theta: is 0.42 .. Rec_loss: 1446.84 .. NELBO: 1447.26\n",
      "Epoch: 25 KL_theta: is 0.41 .. Rec_loss: 1446.75 .. NELBO: 1447.16\n",
      "Epoch: 25 KL_theta: is 0.41 .. Rec_loss: 1446.41 .. NELBO: 1446.82\n",
      "****************************************************************************************************\n",
      "Epoch: 25 KL_theta: is 0.41 .. Rec_loss: 1446.46 .. NELBO: 1446.87\n",
      "Epoch: 26 KL_theta: is 0.4 .. Rec_loss: 1446.44 .. NELBO: 1446.84\n",
      "Epoch: 26 KL_theta: is 0.4 .. Rec_loss: 1446.05 .. NELBO: 1446.45\n",
      "****************************************************************************************************\n",
      "Epoch: 26 KL_theta: is 0.4 .. Rec_loss: 1446.11 .. NELBO: 1446.51\n",
      "Epoch: 27 KL_theta: is 0.39 .. Rec_loss: 1446.06 .. NELBO: 1446.45\n",
      "Epoch: 27 KL_theta: is 0.39 .. Rec_loss: 1445.79 .. NELBO: 1446.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Epoch: 27 KL_theta: is 0.39 .. Rec_loss: 1445.75 .. NELBO: 1446.14\n",
      "Epoch: 28 KL_theta: is 0.39 .. Rec_loss: 1445.6 .. NELBO: 1445.99\n",
      "Epoch: 28 KL_theta: is 0.38 .. Rec_loss: 1445.39 .. NELBO: 1445.77\n",
      "****************************************************************************************************\n",
      "Epoch: 28 KL_theta: is 0.38 .. Rec_loss: 1445.45 .. NELBO: 1445.83\n",
      "Epoch: 29 KL_theta: is 0.38 .. Rec_loss: 1445.32 .. NELBO: 1445.7\n",
      "Epoch: 29 KL_theta: is 0.38 .. Rec_loss: 1445.21 .. NELBO: 1445.59\n",
      "****************************************************************************************************\n",
      "Epoch: 29 KL_theta: is 0.37 .. Rec_loss: 1445.11 .. NELBO: 1445.48\n",
      "Epoch: 30 KL_theta: is 0.37 .. Rec_loss: 1445.09 .. NELBO: 1445.46\n",
      "Epoch: 30 KL_theta: is 0.37 .. Rec_loss: 1444.87 .. NELBO: 1445.24\n",
      "****************************************************************************************************\n",
      "Epoch: 30 KL_theta: is 0.37 .. Rec_loss: 1444.8 .. NELBO: 1445.17\n",
      "Epoch: 31 KL_theta: is 0.37 .. Rec_loss: 1444.76 .. NELBO: 1445.13\n",
      "Epoch: 31 KL_theta: is 0.36 .. Rec_loss: 1444.7 .. NELBO: 1445.06\n",
      "****************************************************************************************************\n",
      "Epoch: 31 KL_theta: is 0.36 .. Rec_loss: 1444.43 .. NELBO: 1444.79\n",
      "Epoch: 32 KL_theta: is 0.36 .. Rec_loss: 1444.41 .. NELBO: 1444.77\n",
      "Epoch: 32 KL_theta: is 0.36 .. Rec_loss: 1444.29 .. NELBO: 1444.65\n",
      "****************************************************************************************************\n",
      "Epoch: 32 KL_theta: is 0.36 .. Rec_loss: 1444.12 .. NELBO: 1444.48\n",
      "Epoch: 33 KL_theta: is 0.36 .. Rec_loss: 1444.03 .. NELBO: 1444.39\n",
      "Epoch: 33 KL_theta: is 0.36 .. Rec_loss: 1443.92 .. NELBO: 1444.28\n",
      "****************************************************************************************************\n",
      "Epoch: 33 KL_theta: is 0.35 .. Rec_loss: 1443.86 .. NELBO: 1444.21\n",
      "Epoch: 34 KL_theta: is 0.35 .. Rec_loss: 1443.81 .. NELBO: 1444.16\n",
      "Epoch: 34 KL_theta: is 0.35 .. Rec_loss: 1443.64 .. NELBO: 1443.99\n",
      "****************************************************************************************************\n",
      "Epoch: 34 KL_theta: is 0.35 .. Rec_loss: 1443.65 .. NELBO: 1444.0\n",
      "Epoch: 35 KL_theta: is 0.35 .. Rec_loss: 1443.67 .. NELBO: 1444.02\n",
      "Epoch: 35 KL_theta: is 0.35 .. Rec_loss: 1443.48 .. NELBO: 1443.83\n",
      "****************************************************************************************************\n",
      "Epoch: 35 KL_theta: is 0.35 .. Rec_loss: 1443.41 .. NELBO: 1443.76\n",
      "Epoch: 36 KL_theta: is 0.35 .. Rec_loss: 1443.49 .. NELBO: 1443.84\n",
      "Epoch: 36 KL_theta: is 0.35 .. Rec_loss: 1443.22 .. NELBO: 1443.57\n",
      "****************************************************************************************************\n",
      "Epoch: 36 KL_theta: is 0.34 .. Rec_loss: 1443.21 .. NELBO: 1443.55\n",
      "Epoch: 37 KL_theta: is 0.34 .. Rec_loss: 1443.16 .. NELBO: 1443.5\n",
      "Epoch: 37 KL_theta: is 0.34 .. Rec_loss: 1442.98 .. NELBO: 1443.32\n",
      "****************************************************************************************************\n",
      "Epoch: 37 KL_theta: is 0.34 .. Rec_loss: 1443.05 .. NELBO: 1443.39\n",
      "Epoch: 38 KL_theta: is 0.34 .. Rec_loss: 1443.05 .. NELBO: 1443.39\n",
      "Epoch: 38 KL_theta: is 0.34 .. Rec_loss: 1442.92 .. NELBO: 1443.26\n",
      "****************************************************************************************************\n",
      "Epoch: 38 KL_theta: is 0.34 .. Rec_loss: 1442.85 .. NELBO: 1443.19\n",
      "Epoch: 39 KL_theta: is 0.34 .. Rec_loss: 1442.84 .. NELBO: 1443.18\n",
      "Epoch: 39 KL_theta: is 0.34 .. Rec_loss: 1442.69 .. NELBO: 1443.03\n",
      "****************************************************************************************************\n",
      "Epoch: 39 KL_theta: is 0.34 .. Rec_loss: 1442.68 .. NELBO: 1443.02\n",
      "torch.Size([20, 7291]) 20\n",
      "(20, 100)\n",
      "topic diversity is 0.0555\n",
      "[['group',\n",
      "  'life',\n",
      "  'sense',\n",
      "  'moments',\n",
      "  'set',\n",
      "  'style',\n",
      "  'noise',\n",
      "  'live',\n",
      "  'melody',\n",
      "  'past'],\n",
      " ['group',\n",
      "  'set',\n",
      "  'sense',\n",
      "  'life',\n",
      "  'moments',\n",
      "  'live',\n",
      "  'release',\n",
      "  'style',\n",
      "  'making',\n",
      "  'minutes'],\n",
      " ['group',\n",
      "  'life',\n",
      "  'sense',\n",
      "  'moments',\n",
      "  'set',\n",
      "  'place',\n",
      "  'live',\n",
      "  'release',\n",
      "  'minutes',\n",
      "  'piano'],\n",
      " ['group',\n",
      "  'life',\n",
      "  'sense',\n",
      "  'moments',\n",
      "  'place',\n",
      "  'release',\n",
      "  'set',\n",
      "  'live',\n",
      "  'style',\n",
      "  'making'],\n",
      " ['group',\n",
      "  'live',\n",
      "  'set',\n",
      "  'sense',\n",
      "  'life',\n",
      "  'release',\n",
      "  'moments',\n",
      "  'melody',\n",
      "  'style',\n",
      "  'place'],\n",
      " ['group',\n",
      "  'life',\n",
      "  'sense',\n",
      "  'moments',\n",
      "  'set',\n",
      "  'live',\n",
      "  'making',\n",
      "  'style',\n",
      "  'place',\n",
      "  'minutes'],\n",
      " ['group',\n",
      "  'sense',\n",
      "  'moments',\n",
      "  'set',\n",
      "  'life',\n",
      "  'live',\n",
      "  'release',\n",
      "  'style',\n",
      "  'melody',\n",
      "  'making'],\n",
      " ['group',\n",
      "  'life',\n",
      "  'set',\n",
      "  'moments',\n",
      "  'sense',\n",
      "  'live',\n",
      "  'style',\n",
      "  'place',\n",
      "  'release',\n",
      "  'minutes'],\n",
      " ['group',\n",
      "  'set',\n",
      "  'live',\n",
      "  'sense',\n",
      "  'life',\n",
      "  'moments',\n",
      "  'release',\n",
      "  'making',\n",
      "  'minutes',\n",
      "  'melody'],\n",
      " ['group',\n",
      "  'set',\n",
      "  'sense',\n",
      "  'live',\n",
      "  'moments',\n",
      "  'release',\n",
      "  'life',\n",
      "  'making',\n",
      "  'style',\n",
      "  'melody'],\n",
      " ['group',\n",
      "  'life',\n",
      "  'set',\n",
      "  'sense',\n",
      "  'moments',\n",
      "  'live',\n",
      "  'melody',\n",
      "  'release',\n",
      "  'style',\n",
      "  'minutes'],\n",
      " ['group',\n",
      "  'sense',\n",
      "  'set',\n",
      "  'life',\n",
      "  'live',\n",
      "  'moments',\n",
      "  'melody',\n",
      "  'place',\n",
      "  'style',\n",
      "  'minutes'],\n",
      " ['group',\n",
      "  'set',\n",
      "  'sense',\n",
      "  'live',\n",
      "  'life',\n",
      "  'release',\n",
      "  'moments',\n",
      "  'style',\n",
      "  'melody',\n",
      "  'place'],\n",
      " ['group',\n",
      "  'life',\n",
      "  'sense',\n",
      "  'set',\n",
      "  'moments',\n",
      "  'live',\n",
      "  'release',\n",
      "  'melody',\n",
      "  'style',\n",
      "  'piano'],\n",
      " ['group',\n",
      "  'sense',\n",
      "  'set',\n",
      "  'life',\n",
      "  'moments',\n",
      "  'live',\n",
      "  'release',\n",
      "  'place',\n",
      "  'style',\n",
      "  'minutes'],\n",
      " ['group',\n",
      "  'sense',\n",
      "  'life',\n",
      "  'moments',\n",
      "  'set',\n",
      "  'live',\n",
      "  'style',\n",
      "  'melody',\n",
      "  'place',\n",
      "  'minutes'],\n",
      " ['group',\n",
      "  'sense',\n",
      "  'life',\n",
      "  'set',\n",
      "  'live',\n",
      "  'moments',\n",
      "  'release',\n",
      "  'style',\n",
      "  'place',\n",
      "  'melody'],\n",
      " ['group',\n",
      "  'life',\n",
      "  'sense',\n",
      "  'moments',\n",
      "  'set',\n",
      "  'melody',\n",
      "  'release',\n",
      "  'live',\n",
      "  'minutes',\n",
      "  'piano'],\n",
      " ['group',\n",
      "  'set',\n",
      "  'live',\n",
      "  'sense',\n",
      "  'life',\n",
      "  'moments',\n",
      "  'release',\n",
      "  'style',\n",
      "  'making',\n",
      "  'melody'],\n",
      " ['group',\n",
      "  'life',\n",
      "  'sense',\n",
      "  'moments',\n",
      "  'live',\n",
      "  'style',\n",
      "  'set',\n",
      "  'piano',\n",
      "  'release',\n",
      "  'place']]\n",
      "Epoch: 40 KL_theta: is 0.34 .. Rec_loss: 1442.56 .. NELBO: 1442.9\n",
      "Epoch: 40 KL_theta: is 0.34 .. Rec_loss: 1442.51 .. NELBO: 1442.85\n",
      "****************************************************************************************************\n",
      "Epoch: 40 KL_theta: is 0.34 .. Rec_loss: 1442.52 .. NELBO: 1442.86\n",
      "Epoch: 41 KL_theta: is 0.34 .. Rec_loss: 1442.54 .. NELBO: 1442.88\n",
      "Epoch: 41 KL_theta: is 0.34 .. Rec_loss: 1442.4 .. NELBO: 1442.74\n",
      "****************************************************************************************************\n",
      "Epoch: 41 KL_theta: is 0.34 .. Rec_loss: 1442.35 .. NELBO: 1442.69\n",
      "Epoch: 42 KL_theta: is 0.34 .. Rec_loss: 1442.35 .. NELBO: 1442.69\n",
      "Epoch: 42 KL_theta: is 0.34 .. Rec_loss: 1442.23 .. NELBO: 1442.57\n",
      "****************************************************************************************************\n",
      "Epoch: 42 KL_theta: is 0.34 .. Rec_loss: 1442.18 .. NELBO: 1442.52\n",
      "Epoch: 43 KL_theta: is 0.34 .. Rec_loss: 1442.21 .. NELBO: 1442.55\n",
      "Epoch: 43 KL_theta: is 0.34 .. Rec_loss: 1442.02 .. NELBO: 1442.36\n",
      "****************************************************************************************************\n",
      "Epoch: 43 KL_theta: is 0.34 .. Rec_loss: 1442.05 .. NELBO: 1442.39\n",
      "Epoch: 44 KL_theta: is 0.34 .. Rec_loss: 1442.16 .. NELBO: 1442.5\n",
      "Epoch: 44 KL_theta: is 0.34 .. Rec_loss: 1441.9 .. NELBO: 1442.24\n",
      "****************************************************************************************************\n",
      "Epoch: 44 KL_theta: is 0.34 .. Rec_loss: 1441.92 .. NELBO: 1442.26\n",
      "Epoch: 45 KL_theta: is 0.34 .. Rec_loss: 1441.99 .. NELBO: 1442.33\n",
      "Epoch: 45 KL_theta: is 0.34 .. Rec_loss: 1441.76 .. NELBO: 1442.1\n",
      "****************************************************************************************************\n",
      "Epoch: 45 KL_theta: is 0.34 .. Rec_loss: 1441.8 .. NELBO: 1442.14\n",
      "Epoch: 46 KL_theta: is 0.34 .. Rec_loss: 1441.75 .. NELBO: 1442.09\n",
      "Epoch: 46 KL_theta: is 0.34 .. Rec_loss: 1441.68 .. NELBO: 1442.02\n",
      "****************************************************************************************************\n",
      "Epoch: 46 KL_theta: is 0.34 .. Rec_loss: 1441.66 .. NELBO: 1442.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 KL_theta: is 0.34 .. Rec_loss: 1441.65 .. NELBO: 1441.99\n",
      "Epoch: 47 KL_theta: is 0.34 .. Rec_loss: 1441.58 .. NELBO: 1441.92\n",
      "****************************************************************************************************\n",
      "Epoch: 47 KL_theta: is 0.34 .. Rec_loss: 1441.52 .. NELBO: 1441.86\n",
      "Epoch: 48 KL_theta: is 0.34 .. Rec_loss: 1441.5 .. NELBO: 1441.84\n",
      "Epoch: 48 KL_theta: is 0.34 .. Rec_loss: 1441.39 .. NELBO: 1441.73\n",
      "****************************************************************************************************\n",
      "Epoch: 48 KL_theta: is 0.34 .. Rec_loss: 1441.4 .. NELBO: 1441.74\n",
      "Epoch: 49 KL_theta: is 0.34 .. Rec_loss: 1441.33 .. NELBO: 1441.67\n",
      "Epoch: 49 KL_theta: is 0.35 .. Rec_loss: 1441.26 .. NELBO: 1441.61\n",
      "****************************************************************************************************\n",
      "Epoch: 49 KL_theta: is 0.35 .. Rec_loss: 1441.29 .. NELBO: 1441.64\n",
      "Epoch: 50 KL_theta: is 0.35 .. Rec_loss: 1441.2 .. NELBO: 1441.55\n",
      "Epoch: 50 KL_theta: is 0.35 .. Rec_loss: 1441.18 .. NELBO: 1441.53\n",
      "****************************************************************************************************\n",
      "Epoch: 50 KL_theta: is 0.35 .. Rec_loss: 1441.17 .. NELBO: 1441.52\n",
      "Epoch: 51 KL_theta: is 0.35 .. Rec_loss: 1441.15 .. NELBO: 1441.5\n",
      "Epoch: 51 KL_theta: is 0.36 .. Rec_loss: 1441.04 .. NELBO: 1441.4\n",
      "****************************************************************************************************\n",
      "Epoch: 51 KL_theta: is 0.36 .. Rec_loss: 1441.06 .. NELBO: 1441.42\n",
      "Epoch: 52 KL_theta: is 0.36 .. Rec_loss: 1441.0 .. NELBO: 1441.36\n",
      "Epoch: 52 KL_theta: is 0.36 .. Rec_loss: 1440.94 .. NELBO: 1441.3\n",
      "****************************************************************************************************\n",
      "Epoch: 52 KL_theta: is 0.37 .. Rec_loss: 1440.94 .. NELBO: 1441.31\n",
      "Epoch: 53 KL_theta: is 0.37 .. Rec_loss: 1440.91 .. NELBO: 1441.28\n",
      "Epoch: 53 KL_theta: is 0.37 .. Rec_loss: 1440.8 .. NELBO: 1441.17\n",
      "****************************************************************************************************\n",
      "Epoch: 53 KL_theta: is 0.37 .. Rec_loss: 1440.84 .. NELBO: 1441.21\n",
      "Epoch: 54 KL_theta: is 0.38 .. Rec_loss: 1440.82 .. NELBO: 1441.2\n",
      "Epoch: 54 KL_theta: is 0.38 .. Rec_loss: 1440.75 .. NELBO: 1441.13\n",
      "****************************************************************************************************\n",
      "Epoch: 54 KL_theta: is 0.38 .. Rec_loss: 1440.71 .. NELBO: 1441.09\n",
      "Epoch: 55 KL_theta: is 0.39 .. Rec_loss: 1440.65 .. NELBO: 1441.04\n",
      "Epoch: 55 KL_theta: is 0.39 .. Rec_loss: 1440.53 .. NELBO: 1440.92\n",
      "****************************************************************************************************\n",
      "Epoch: 55 KL_theta: is 0.4 .. Rec_loss: 1440.63 .. NELBO: 1441.03\n",
      "Epoch: 56 KL_theta: is 0.4 .. Rec_loss: 1440.6 .. NELBO: 1441.0\n",
      "Epoch: 56 KL_theta: is 0.41 .. Rec_loss: 1440.52 .. NELBO: 1440.93\n",
      "****************************************************************************************************\n",
      "Epoch: 56 KL_theta: is 0.41 .. Rec_loss: 1440.5 .. NELBO: 1440.91\n",
      "Epoch: 57 KL_theta: is 0.42 .. Rec_loss: 1440.47 .. NELBO: 1440.89\n",
      "Epoch: 57 KL_theta: is 0.43 .. Rec_loss: 1440.37 .. NELBO: 1440.8\n",
      "****************************************************************************************************\n",
      "Epoch: 57 KL_theta: is 0.43 .. Rec_loss: 1440.37 .. NELBO: 1440.8\n",
      "Epoch: 58 KL_theta: is 0.44 .. Rec_loss: 1440.36 .. NELBO: 1440.8\n",
      "Epoch: 58 KL_theta: is 0.45 .. Rec_loss: 1440.24 .. NELBO: 1440.69\n",
      "****************************************************************************************************\n",
      "Epoch: 58 KL_theta: is 0.46 .. Rec_loss: 1440.23 .. NELBO: 1440.69\n",
      "Epoch: 59 KL_theta: is 0.46 .. Rec_loss: 1440.2 .. NELBO: 1440.66\n",
      "Epoch: 59 KL_theta: is 0.48 .. Rec_loss: 1440.14 .. NELBO: 1440.62\n",
      "****************************************************************************************************\n",
      "Epoch: 59 KL_theta: is 0.48 .. Rec_loss: 1440.07 .. NELBO: 1440.55\n",
      "Epoch: 60 KL_theta: is 0.48 .. Rec_loss: 1440.04 .. NELBO: 1440.52\n",
      "Epoch: 60 KL_theta: is 0.5 .. Rec_loss: 1439.96 .. NELBO: 1440.46\n",
      "****************************************************************************************************\n",
      "Epoch: 60 KL_theta: is 0.51 .. Rec_loss: 1439.91 .. NELBO: 1440.42\n",
      "Epoch: 61 KL_theta: is 0.51 .. Rec_loss: 1439.89 .. NELBO: 1440.4\n",
      "Epoch: 61 KL_theta: is 0.53 .. Rec_loss: 1439.74 .. NELBO: 1440.27\n",
      "****************************************************************************************************\n",
      "Epoch: 61 KL_theta: is 0.53 .. Rec_loss: 1439.78 .. NELBO: 1440.31\n",
      "Epoch: 62 KL_theta: is 0.54 .. Rec_loss: 1439.8 .. NELBO: 1440.34\n",
      "Epoch: 62 KL_theta: is 0.55 .. Rec_loss: 1439.64 .. NELBO: 1440.19\n",
      "****************************************************************************************************\n",
      "Epoch: 62 KL_theta: is 0.56 .. Rec_loss: 1439.63 .. NELBO: 1440.19\n",
      "Epoch: 63 KL_theta: is 0.56 .. Rec_loss: 1439.56 .. NELBO: 1440.12\n",
      "Epoch: 63 KL_theta: is 0.58 .. Rec_loss: 1439.43 .. NELBO: 1440.01\n",
      "****************************************************************************************************\n",
      "Epoch: 63 KL_theta: is 0.58 .. Rec_loss: 1439.51 .. NELBO: 1440.09\n",
      "Epoch: 64 KL_theta: is 0.59 .. Rec_loss: 1439.52 .. NELBO: 1440.11\n",
      "Epoch: 64 KL_theta: is 0.61 .. Rec_loss: 1439.41 .. NELBO: 1440.02\n",
      "****************************************************************************************************\n",
      "Epoch: 64 KL_theta: is 0.61 .. Rec_loss: 1439.34 .. NELBO: 1439.95\n",
      "Epoch: 65 KL_theta: is 0.62 .. Rec_loss: 1439.31 .. NELBO: 1439.93\n",
      "Epoch: 65 KL_theta: is 0.63 .. Rec_loss: 1439.2 .. NELBO: 1439.83\n",
      "****************************************************************************************************\n",
      "Epoch: 65 KL_theta: is 0.64 .. Rec_loss: 1439.19 .. NELBO: 1439.83\n",
      "Epoch: 66 KL_theta: is 0.64 .. Rec_loss: 1439.18 .. NELBO: 1439.82\n",
      "Epoch: 66 KL_theta: is 0.66 .. Rec_loss: 1439.07 .. NELBO: 1439.73\n",
      "****************************************************************************************************\n",
      "Epoch: 66 KL_theta: is 0.66 .. Rec_loss: 1439.04 .. NELBO: 1439.7\n",
      "Epoch: 67 KL_theta: is 0.67 .. Rec_loss: 1438.99 .. NELBO: 1439.66\n",
      "Epoch: 67 KL_theta: is 0.69 .. Rec_loss: 1438.91 .. NELBO: 1439.6\n",
      "****************************************************************************************************\n",
      "Epoch: 67 KL_theta: is 0.69 .. Rec_loss: 1438.89 .. NELBO: 1439.58\n",
      "Epoch: 68 KL_theta: is 0.69 .. Rec_loss: 1438.88 .. NELBO: 1439.57\n",
      "Epoch: 68 KL_theta: is 0.71 .. Rec_loss: 1438.76 .. NELBO: 1439.47\n",
      "****************************************************************************************************\n",
      "Epoch: 68 KL_theta: is 0.72 .. Rec_loss: 1438.75 .. NELBO: 1439.47\n",
      "Epoch: 69 KL_theta: is 0.72 .. Rec_loss: 1438.73 .. NELBO: 1439.45\n",
      "Epoch: 69 KL_theta: is 0.74 .. Rec_loss: 1438.6 .. NELBO: 1439.34\n",
      "****************************************************************************************************\n",
      "Epoch: 69 KL_theta: is 0.74 .. Rec_loss: 1438.62 .. NELBO: 1439.36\n",
      "Epoch: 70 KL_theta: is 0.75 .. Rec_loss: 1438.6 .. NELBO: 1439.35\n",
      "Epoch: 70 KL_theta: is 0.76 .. Rec_loss: 1438.49 .. NELBO: 1439.25\n",
      "****************************************************************************************************\n",
      "Epoch: 70 KL_theta: is 0.77 .. Rec_loss: 1438.47 .. NELBO: 1439.24\n",
      "Epoch: 71 KL_theta: is 0.77 .. Rec_loss: 1438.42 .. NELBO: 1439.19\n",
      "Epoch: 71 KL_theta: is 0.79 .. Rec_loss: 1438.33 .. NELBO: 1439.12\n",
      "****************************************************************************************************\n",
      "Epoch: 71 KL_theta: is 0.79 .. Rec_loss: 1438.34 .. NELBO: 1439.13\n",
      "Epoch: 72 KL_theta: is 0.8 .. Rec_loss: 1438.27 .. NELBO: 1439.07\n",
      "Epoch: 72 KL_theta: is 0.81 .. Rec_loss: 1438.21 .. NELBO: 1439.02\n",
      "****************************************************************************************************\n",
      "Epoch: 72 KL_theta: is 0.82 .. Rec_loss: 1438.21 .. NELBO: 1439.03\n",
      "Epoch: 73 KL_theta: is 0.82 .. Rec_loss: 1438.17 .. NELBO: 1438.99\n",
      "Epoch: 73 KL_theta: is 0.84 .. Rec_loss: 1438.07 .. NELBO: 1438.91\n",
      "****************************************************************************************************\n",
      "Epoch: 73 KL_theta: is 0.84 .. Rec_loss: 1438.08 .. NELBO: 1438.92\n",
      "Epoch: 74 KL_theta: is 0.84 .. Rec_loss: 1438.06 .. NELBO: 1438.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74 KL_theta: is 0.86 .. Rec_loss: 1437.97 .. NELBO: 1438.83\n",
      "****************************************************************************************************\n",
      "Epoch: 74 KL_theta: is 0.86 .. Rec_loss: 1437.93 .. NELBO: 1438.79\n",
      "Epoch: 75 KL_theta: is 0.87 .. Rec_loss: 1437.87 .. NELBO: 1438.74\n",
      "Epoch: 75 KL_theta: is 0.88 .. Rec_loss: 1437.79 .. NELBO: 1438.67\n",
      "****************************************************************************************************\n",
      "Epoch: 75 KL_theta: is 0.89 .. Rec_loss: 1437.81 .. NELBO: 1438.7\n",
      "Epoch: 76 KL_theta: is 0.89 .. Rec_loss: 1437.79 .. NELBO: 1438.68\n",
      "Epoch: 76 KL_theta: is 0.91 .. Rec_loss: 1437.7 .. NELBO: 1438.61\n",
      "****************************************************************************************************\n",
      "Epoch: 76 KL_theta: is 0.91 .. Rec_loss: 1437.68 .. NELBO: 1438.59\n",
      "Epoch: 77 KL_theta: is 0.91 .. Rec_loss: 1437.65 .. NELBO: 1438.56\n",
      "Epoch: 77 KL_theta: is 0.93 .. Rec_loss: 1437.61 .. NELBO: 1438.54\n",
      "****************************************************************************************************\n",
      "Epoch: 77 KL_theta: is 0.93 .. Rec_loss: 1437.52 .. NELBO: 1438.45\n",
      "Epoch: 78 KL_theta: is 0.94 .. Rec_loss: 1437.49 .. NELBO: 1438.43\n",
      "Epoch: 78 KL_theta: is 0.95 .. Rec_loss: 1437.37 .. NELBO: 1438.32\n",
      "****************************************************************************************************\n",
      "Epoch: 78 KL_theta: is 0.96 .. Rec_loss: 1437.41 .. NELBO: 1438.37\n",
      "Epoch: 79 KL_theta: is 0.96 .. Rec_loss: 1437.4 .. NELBO: 1438.36\n",
      "Epoch: 79 KL_theta: is 0.98 .. Rec_loss: 1437.31 .. NELBO: 1438.29\n",
      "****************************************************************************************************\n",
      "Epoch: 79 KL_theta: is 0.98 .. Rec_loss: 1437.26 .. NELBO: 1438.24\n",
      "torch.Size([20, 7291]) 20\n",
      "(20, 100)\n",
      "topic diversity is 0.1505\n",
      "[['rap',\n",
      "  'life',\n",
      "  'death',\n",
      "  'sings',\n",
      "  'rapper',\n",
      "  'young',\n",
      "  'black',\n",
      "  'country',\n",
      "  'hip_hop',\n",
      "  'rappers'],\n",
      " ['group',\n",
      "  'set',\n",
      "  'minutes',\n",
      "  'melody',\n",
      "  'release',\n",
      "  'sense',\n",
      "  'ep',\n",
      "  'drums',\n",
      "  'moments',\n",
      "  'beat'],\n",
      " ['sings',\n",
      "  'life',\n",
      "  'indie',\n",
      "  'chorus',\n",
      "  'group',\n",
      "  'punk',\n",
      "  'cover',\n",
      "  'singing',\n",
      "  'big',\n",
      "  'words'],\n",
      " ['sings',\n",
      "  'life',\n",
      "  'indie',\n",
      "  'chorus',\n",
      "  'punk',\n",
      "  'country',\n",
      "  'words',\n",
      "  'singing',\n",
      "  'cover',\n",
      "  'black'],\n",
      " ['electronic',\n",
      "  'piece',\n",
      "  'ambient',\n",
      "  'noise',\n",
      "  'techno',\n",
      "  'dance',\n",
      "  'pieces',\n",
      "  'beats',\n",
      "  'label',\n",
      "  'house'],\n",
      " ['indie',\n",
      "  'chorus',\n",
      "  'group',\n",
      "  'sings',\n",
      "  'life',\n",
      "  'folk',\n",
      "  'cover',\n",
      "  'past',\n",
      "  'singing',\n",
      "  'punk'],\n",
      " ['group',\n",
      "  'indie',\n",
      "  'live',\n",
      "  'folk',\n",
      "  'moments',\n",
      "  'musical',\n",
      "  'past',\n",
      "  'chorus',\n",
      "  'hard',\n",
      "  'fact'],\n",
      " ['group',\n",
      "  'indie',\n",
      "  'live',\n",
      "  'moments',\n",
      "  'past',\n",
      "  'musical',\n",
      "  'folk',\n",
      "  'place',\n",
      "  'hard',\n",
      "  'half'],\n",
      " ['group',\n",
      "  'minutes',\n",
      "  'melody',\n",
      "  'set',\n",
      "  'instrumental',\n",
      "  'drums',\n",
      "  'beat',\n",
      "  'release',\n",
      "  'bass',\n",
      "  'sense'],\n",
      " ['group',\n",
      "  'minutes',\n",
      "  'melody',\n",
      "  'bass',\n",
      "  'set',\n",
      "  'beat',\n",
      "  'sense',\n",
      "  'drums',\n",
      "  'instrumental',\n",
      "  'piano'],\n",
      " ['group',\n",
      "  'live',\n",
      "  'moments',\n",
      "  'set',\n",
      "  'musical',\n",
      "  'release',\n",
      "  'half',\n",
      "  'ep',\n",
      "  'recorded',\n",
      "  'making'],\n",
      " ['group',\n",
      "  'minutes',\n",
      "  'melody',\n",
      "  'bass',\n",
      "  'sense',\n",
      "  'set',\n",
      "  'beat',\n",
      "  'drums',\n",
      "  'piano',\n",
      "  'ep'],\n",
      " ['minutes',\n",
      "  'bass',\n",
      "  'melody',\n",
      "  'group',\n",
      "  'drums',\n",
      "  'beat',\n",
      "  'noise',\n",
      "  'instrumental',\n",
      "  'sense',\n",
      "  'set'],\n",
      " ['group',\n",
      "  'indie',\n",
      "  'past',\n",
      "  'live',\n",
      "  'folk',\n",
      "  'musical',\n",
      "  'moments',\n",
      "  'chorus',\n",
      "  'place',\n",
      "  'hard'],\n",
      " ['group',\n",
      "  'set',\n",
      "  'moments',\n",
      "  'release',\n",
      "  'sense',\n",
      "  'live',\n",
      "  'style',\n",
      "  'ep',\n",
      "  'recorded',\n",
      "  'minutes'],\n",
      " ['sings',\n",
      "  'life',\n",
      "  'indie',\n",
      "  'chorus',\n",
      "  'punk',\n",
      "  'singing',\n",
      "  'cover',\n",
      "  'group',\n",
      "  'black',\n",
      "  'words'],\n",
      " ['group',\n",
      "  'set',\n",
      "  'release',\n",
      "  'moments',\n",
      "  'live',\n",
      "  'sense',\n",
      "  'ep',\n",
      "  'style',\n",
      "  'melody',\n",
      "  'recorded'],\n",
      " ['group',\n",
      "  'indie',\n",
      "  'chorus',\n",
      "  'life',\n",
      "  'folk',\n",
      "  'sings',\n",
      "  'live',\n",
      "  'cover',\n",
      "  'hard',\n",
      "  'past'],\n",
      " ['group',\n",
      "  'minutes',\n",
      "  'bass',\n",
      "  'melody',\n",
      "  'beat',\n",
      "  'set',\n",
      "  'sense',\n",
      "  'drums',\n",
      "  'noise',\n",
      "  'minute'],\n",
      " ['life',\n",
      "  'sings',\n",
      "  'indie',\n",
      "  'chorus',\n",
      "  'group',\n",
      "  'singing',\n",
      "  'punk',\n",
      "  'cover',\n",
      "  'words',\n",
      "  'black']]\n",
      "Epoch: 80 KL_theta: is 0.98 .. Rec_loss: 1437.26 .. NELBO: 1438.24\n",
      "Epoch: 80 KL_theta: is 1.0 .. Rec_loss: 1437.15 .. NELBO: 1438.15\n",
      "****************************************************************************************************\n",
      "Epoch: 80 KL_theta: is 1.0 .. Rec_loss: 1437.14 .. NELBO: 1438.14\n",
      "Epoch: 81 KL_theta: is 1.01 .. Rec_loss: 1437.13 .. NELBO: 1438.14\n",
      "Epoch: 81 KL_theta: is 1.02 .. Rec_loss: 1437.04 .. NELBO: 1438.06\n",
      "****************************************************************************************************\n",
      "Epoch: 81 KL_theta: is 1.03 .. Rec_loss: 1437.01 .. NELBO: 1438.04\n",
      "Epoch: 82 KL_theta: is 1.03 .. Rec_loss: 1436.97 .. NELBO: 1438.0\n",
      "Epoch: 82 KL_theta: is 1.04 .. Rec_loss: 1436.87 .. NELBO: 1437.91\n",
      "****************************************************************************************************\n",
      "Epoch: 82 KL_theta: is 1.05 .. Rec_loss: 1436.89 .. NELBO: 1437.94\n",
      "Epoch: 83 KL_theta: is 1.05 .. Rec_loss: 1436.9 .. NELBO: 1437.95\n",
      "Epoch: 83 KL_theta: is 1.07 .. Rec_loss: 1436.77 .. NELBO: 1437.84\n",
      "****************************************************************************************************\n",
      "Epoch: 83 KL_theta: is 1.07 .. Rec_loss: 1436.78 .. NELBO: 1437.85\n",
      "Epoch: 84 KL_theta: is 1.07 .. Rec_loss: 1436.74 .. NELBO: 1437.81\n",
      "Epoch: 84 KL_theta: is 1.09 .. Rec_loss: 1436.66 .. NELBO: 1437.75\n",
      "****************************************************************************************************\n",
      "Epoch: 84 KL_theta: is 1.09 .. Rec_loss: 1436.67 .. NELBO: 1437.76\n",
      "Epoch: 85 KL_theta: is 1.1 .. Rec_loss: 1436.62 .. NELBO: 1437.72\n",
      "Epoch: 85 KL_theta: is 1.11 .. Rec_loss: 1436.56 .. NELBO: 1437.67\n",
      "****************************************************************************************************\n",
      "Epoch: 85 KL_theta: is 1.11 .. Rec_loss: 1436.55 .. NELBO: 1437.66\n",
      "Epoch: 86 KL_theta: is 1.12 .. Rec_loss: 1436.52 .. NELBO: 1437.64\n",
      "Epoch: 86 KL_theta: is 1.13 .. Rec_loss: 1436.45 .. NELBO: 1437.58\n",
      "****************************************************************************************************\n",
      "Epoch: 86 KL_theta: is 1.14 .. Rec_loss: 1436.42 .. NELBO: 1437.56\n",
      "Epoch: 87 KL_theta: is 1.14 .. Rec_loss: 1436.39 .. NELBO: 1437.53\n",
      "Epoch: 87 KL_theta: is 1.16 .. Rec_loss: 1436.31 .. NELBO: 1437.47\n",
      "****************************************************************************************************\n",
      "Epoch: 87 KL_theta: is 1.16 .. Rec_loss: 1436.31 .. NELBO: 1437.47\n",
      "Epoch: 88 KL_theta: is 1.16 .. Rec_loss: 1436.26 .. NELBO: 1437.42\n",
      "Epoch: 88 KL_theta: is 1.18 .. Rec_loss: 1436.2 .. NELBO: 1437.38\n",
      "****************************************************************************************************\n",
      "Epoch: 88 KL_theta: is 1.18 .. Rec_loss: 1436.19 .. NELBO: 1437.37\n",
      "Epoch: 89 KL_theta: is 1.18 .. Rec_loss: 1436.19 .. NELBO: 1437.37\n",
      "Epoch: 89 KL_theta: is 1.2 .. Rec_loss: 1436.11 .. NELBO: 1437.31\n",
      "****************************************************************************************************\n",
      "Epoch: 89 KL_theta: is 1.2 .. Rec_loss: 1436.07 .. NELBO: 1437.27\n",
      "Epoch: 90 KL_theta: is 1.21 .. Rec_loss: 1436.05 .. NELBO: 1437.26\n",
      "Epoch: 90 KL_theta: is 1.22 .. Rec_loss: 1435.98 .. NELBO: 1437.2\n",
      "****************************************************************************************************\n",
      "Epoch: 90 KL_theta: is 1.22 .. Rec_loss: 1435.95 .. NELBO: 1437.17\n",
      "Epoch: 91 KL_theta: is 1.23 .. Rec_loss: 1435.9 .. NELBO: 1437.13\n",
      "Epoch: 91 KL_theta: is 1.24 .. Rec_loss: 1435.84 .. NELBO: 1437.08\n",
      "****************************************************************************************************\n",
      "Epoch: 91 KL_theta: is 1.25 .. Rec_loss: 1435.84 .. NELBO: 1437.09\n",
      "Epoch: 92 KL_theta: is 1.25 .. Rec_loss: 1435.82 .. NELBO: 1437.07\n",
      "Epoch: 92 KL_theta: is 1.26 .. Rec_loss: 1435.76 .. NELBO: 1437.02\n",
      "****************************************************************************************************\n",
      "Epoch: 92 KL_theta: is 1.27 .. Rec_loss: 1435.71 .. NELBO: 1436.98\n",
      "Epoch: 93 KL_theta: is 1.27 .. Rec_loss: 1435.68 .. NELBO: 1436.95\n",
      "Epoch: 93 KL_theta: is 1.29 .. Rec_loss: 1435.64 .. NELBO: 1436.93\n",
      "****************************************************************************************************\n",
      "Epoch: 93 KL_theta: is 1.29 .. Rec_loss: 1435.59 .. NELBO: 1436.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 KL_theta: is 1.29 .. Rec_loss: 1435.53 .. NELBO: 1436.82\n",
      "Epoch: 94 KL_theta: is 1.31 .. Rec_loss: 1435.52 .. NELBO: 1436.83\n",
      "****************************************************************************************************\n",
      "Epoch: 94 KL_theta: is 1.31 .. Rec_loss: 1435.46 .. NELBO: 1436.77\n",
      "Epoch: 95 KL_theta: is 1.31 .. Rec_loss: 1435.43 .. NELBO: 1436.74\n",
      "Epoch: 95 KL_theta: is 1.33 .. Rec_loss: 1435.4 .. NELBO: 1436.73\n",
      "****************************************************************************************************\n",
      "Epoch: 95 KL_theta: is 1.33 .. Rec_loss: 1435.32 .. NELBO: 1436.65\n",
      "Epoch: 96 KL_theta: is 1.34 .. Rec_loss: 1435.3 .. NELBO: 1436.64\n",
      "Epoch: 96 KL_theta: is 1.35 .. Rec_loss: 1435.22 .. NELBO: 1436.57\n",
      "****************************************************************************************************\n",
      "Epoch: 96 KL_theta: is 1.35 .. Rec_loss: 1435.21 .. NELBO: 1436.56\n",
      "Epoch: 97 KL_theta: is 1.36 .. Rec_loss: 1435.17 .. NELBO: 1436.53\n",
      "Epoch: 97 KL_theta: is 1.37 .. Rec_loss: 1435.13 .. NELBO: 1436.5\n",
      "****************************************************************************************************\n",
      "Epoch: 97 KL_theta: is 1.38 .. Rec_loss: 1435.1 .. NELBO: 1436.48\n",
      "Epoch: 98 KL_theta: is 1.38 .. Rec_loss: 1435.08 .. NELBO: 1436.46\n",
      "Epoch: 98 KL_theta: is 1.39 .. Rec_loss: 1435.0 .. NELBO: 1436.39\n",
      "****************************************************************************************************\n",
      "Epoch: 98 KL_theta: is 1.4 .. Rec_loss: 1435.0 .. NELBO: 1436.4\n",
      "Epoch: 99 KL_theta: is 1.4 .. Rec_loss: 1435.01 .. NELBO: 1436.41\n",
      "Epoch: 99 KL_theta: is 1.42 .. Rec_loss: 1434.92 .. NELBO: 1436.34\n",
      "****************************************************************************************************\n",
      "Epoch: 99 KL_theta: is 1.42 .. Rec_loss: 1434.89 .. NELBO: 1436.31\n",
      "Epoch: 100 KL_theta: is 1.42 .. Rec_loss: 1434.87 .. NELBO: 1436.29\n",
      "Epoch: 100 KL_theta: is 1.44 .. Rec_loss: 1434.81 .. NELBO: 1436.25\n",
      "****************************************************************************************************\n",
      "Epoch: 100 KL_theta: is 1.44 .. Rec_loss: 1434.78 .. NELBO: 1436.22\n",
      "Epoch: 101 KL_theta: is 1.45 .. Rec_loss: 1434.78 .. NELBO: 1436.23\n",
      "Epoch: 101 KL_theta: is 1.46 .. Rec_loss: 1434.7 .. NELBO: 1436.16\n",
      "****************************************************************************************************\n",
      "Epoch: 101 KL_theta: is 1.46 .. Rec_loss: 1434.68 .. NELBO: 1436.14\n",
      "Epoch: 102 KL_theta: is 1.47 .. Rec_loss: 1434.65 .. NELBO: 1436.12\n",
      "Epoch: 102 KL_theta: is 1.48 .. Rec_loss: 1434.61 .. NELBO: 1436.09\n",
      "****************************************************************************************************\n",
      "Epoch: 102 KL_theta: is 1.49 .. Rec_loss: 1434.56 .. NELBO: 1436.05\n",
      "Epoch: 103 KL_theta: is 1.49 .. Rec_loss: 1434.53 .. NELBO: 1436.02\n",
      "Epoch: 103 KL_theta: is 1.51 .. Rec_loss: 1434.45 .. NELBO: 1435.96\n",
      "****************************************************************************************************\n",
      "Epoch: 103 KL_theta: is 1.51 .. Rec_loss: 1434.47 .. NELBO: 1435.98\n",
      "Epoch: 104 KL_theta: is 1.51 .. Rec_loss: 1434.46 .. NELBO: 1435.97\n",
      "Epoch: 104 KL_theta: is 1.53 .. Rec_loss: 1434.35 .. NELBO: 1435.88\n",
      "****************************************************************************************************\n",
      "Epoch: 104 KL_theta: is 1.53 .. Rec_loss: 1434.37 .. NELBO: 1435.9\n",
      "Epoch: 105 KL_theta: is 1.54 .. Rec_loss: 1434.36 .. NELBO: 1435.9\n",
      "Epoch: 105 KL_theta: is 1.55 .. Rec_loss: 1434.29 .. NELBO: 1435.84\n",
      "****************************************************************************************************\n",
      "Epoch: 105 KL_theta: is 1.56 .. Rec_loss: 1434.27 .. NELBO: 1435.83\n",
      "Epoch: 106 KL_theta: is 1.56 .. Rec_loss: 1434.25 .. NELBO: 1435.81\n",
      "Epoch: 106 KL_theta: is 1.57 .. Rec_loss: 1434.19 .. NELBO: 1435.76\n",
      "****************************************************************************************************\n",
      "Epoch: 106 KL_theta: is 1.58 .. Rec_loss: 1434.15 .. NELBO: 1435.73\n",
      "Epoch: 107 KL_theta: is 1.58 .. Rec_loss: 1434.15 .. NELBO: 1435.73\n",
      "Epoch: 107 KL_theta: is 1.6 .. Rec_loss: 1434.08 .. NELBO: 1435.68\n",
      "****************************************************************************************************\n",
      "Epoch: 107 KL_theta: is 1.6 .. Rec_loss: 1434.04 .. NELBO: 1435.64\n",
      "Epoch: 108 KL_theta: is 1.6 .. Rec_loss: 1434.01 .. NELBO: 1435.61\n",
      "Epoch: 108 KL_theta: is 1.62 .. Rec_loss: 1433.94 .. NELBO: 1435.56\n",
      "****************************************************************************************************\n",
      "Epoch: 108 KL_theta: is 1.62 .. Rec_loss: 1433.95 .. NELBO: 1435.57\n",
      "Epoch: 109 KL_theta: is 1.63 .. Rec_loss: 1433.96 .. NELBO: 1435.59\n",
      "Epoch: 109 KL_theta: is 1.64 .. Rec_loss: 1433.89 .. NELBO: 1435.53\n",
      "****************************************************************************************************\n",
      "Epoch: 109 KL_theta: is 1.65 .. Rec_loss: 1433.82 .. NELBO: 1435.47\n",
      "Epoch: 110 KL_theta: is 1.65 .. Rec_loss: 1433.82 .. NELBO: 1435.47\n",
      "Epoch: 110 KL_theta: is 1.67 .. Rec_loss: 1433.74 .. NELBO: 1435.41\n",
      "****************************************************************************************************\n",
      "Epoch: 110 KL_theta: is 1.67 .. Rec_loss: 1433.71 .. NELBO: 1435.38\n",
      "Epoch: 111 KL_theta: is 1.67 .. Rec_loss: 1433.68 .. NELBO: 1435.35\n",
      "Epoch: 111 KL_theta: is 1.69 .. Rec_loss: 1433.64 .. NELBO: 1435.33\n",
      "****************************************************************************************************\n",
      "Epoch: 111 KL_theta: is 1.69 .. Rec_loss: 1433.6 .. NELBO: 1435.29\n",
      "Epoch: 112 KL_theta: is 1.7 .. Rec_loss: 1433.59 .. NELBO: 1435.29\n",
      "Epoch: 112 KL_theta: is 1.71 .. Rec_loss: 1433.49 .. NELBO: 1435.2\n",
      "****************************************************************************************************\n",
      "Epoch: 112 KL_theta: is 1.71 .. Rec_loss: 1433.51 .. NELBO: 1435.22\n",
      "Epoch: 113 KL_theta: is 1.72 .. Rec_loss: 1433.53 .. NELBO: 1435.25\n",
      "Epoch: 113 KL_theta: is 1.73 .. Rec_loss: 1433.42 .. NELBO: 1435.15\n",
      "****************************************************************************************************\n",
      "Epoch: 113 KL_theta: is 1.74 .. Rec_loss: 1433.41 .. NELBO: 1435.15\n",
      "Epoch: 114 KL_theta: is 1.74 .. Rec_loss: 1433.38 .. NELBO: 1435.12\n",
      "Epoch: 114 KL_theta: is 1.76 .. Rec_loss: 1433.34 .. NELBO: 1435.1\n",
      "****************************************************************************************************\n",
      "Epoch: 114 KL_theta: is 1.76 .. Rec_loss: 1433.3 .. NELBO: 1435.06\n",
      "Epoch: 115 KL_theta: is 1.77 .. Rec_loss: 1433.28 .. NELBO: 1435.05\n",
      "Epoch: 115 KL_theta: is 1.78 .. Rec_loss: 1433.19 .. NELBO: 1434.97\n",
      "****************************************************************************************************\n",
      "Epoch: 115 KL_theta: is 1.79 .. Rec_loss: 1433.2 .. NELBO: 1434.99\n",
      "Epoch: 116 KL_theta: is 1.79 .. Rec_loss: 1433.17 .. NELBO: 1434.96\n",
      "Epoch: 116 KL_theta: is 1.8 .. Rec_loss: 1433.11 .. NELBO: 1434.91\n",
      "****************************************************************************************************\n",
      "Epoch: 116 KL_theta: is 1.81 .. Rec_loss: 1433.11 .. NELBO: 1434.92\n",
      "Epoch: 117 KL_theta: is 1.81 .. Rec_loss: 1433.07 .. NELBO: 1434.88\n",
      "Epoch: 117 KL_theta: is 1.83 .. Rec_loss: 1433.01 .. NELBO: 1434.84\n",
      "****************************************************************************************************\n",
      "Epoch: 117 KL_theta: is 1.83 .. Rec_loss: 1433.01 .. NELBO: 1434.84\n",
      "Epoch: 118 KL_theta: is 1.84 .. Rec_loss: 1432.99 .. NELBO: 1434.83\n",
      "Epoch: 118 KL_theta: is 1.85 .. Rec_loss: 1432.9 .. NELBO: 1434.75\n",
      "****************************************************************************************************\n",
      "Epoch: 118 KL_theta: is 1.86 .. Rec_loss: 1432.91 .. NELBO: 1434.77\n",
      "Epoch: 119 KL_theta: is 1.86 .. Rec_loss: 1432.9 .. NELBO: 1434.76\n",
      "Epoch: 119 KL_theta: is 1.88 .. Rec_loss: 1432.84 .. NELBO: 1434.72\n",
      "****************************************************************************************************\n",
      "Epoch: 119 KL_theta: is 1.88 .. Rec_loss: 1432.79 .. NELBO: 1434.67\n",
      "torch.Size([20, 7291]) 20\n",
      "(20, 100)\n",
      "topic diversity is 0.2515\n",
      "[['rap',\n",
      "  'hip_hop',\n",
      "  'life',\n",
      "  'beats',\n",
      "  'production',\n",
      "  'beat',\n",
      "  'rapper',\n",
      "  'black',\n",
      "  'soul',\n",
      "  'style'],\n",
      " ['group',\n",
      "  'moments',\n",
      "  'set',\n",
      "  'instrumental',\n",
      "  'recorded',\n",
      "  'release',\n",
      "  'hear',\n",
      "  'playing',\n",
      "  'ep',\n",
      "  'melody'],\n",
      " ['life',\n",
      "  'sings',\n",
      "  'big',\n",
      "  'group',\n",
      "  'chorus',\n",
      "  'live',\n",
      "  'singing',\n",
      "  'set',\n",
      "  'fact',\n",
      "  'words'],\n",
      " ['punk',\n",
      "  'sings',\n",
      "  'kids',\n",
      "  'girls',\n",
      "  'boy',\n",
      "  'life',\n",
      "  'indie',\n",
      "  'country',\n",
      "  'boys',\n",
      "  'heart'],\n",
      " ['dance',\n",
      "  'electronic',\n",
      "  'beats',\n",
      "  'label',\n",
      "  'house',\n",
      "  'techno',\n",
      "  'mix',\n",
      "  'beat',\n",
      "  'disco',\n",
      "  'synths'],\n",
      " ['indie',\n",
      "  'folk',\n",
      "  'chorus',\n",
      "  'opener',\n",
      "  'harmonies',\n",
      "  'melodies',\n",
      "  'songwriting',\n",
      "  'riffs',\n",
      "  'arrangements',\n",
      "  'post_punk'],\n",
      " ['folk',\n",
      "  'indie',\n",
      "  'chorus',\n",
      "  'opener',\n",
      "  'melodies',\n",
      "  'past',\n",
      "  'vocal',\n",
      "  'musical',\n",
      "  'singing',\n",
      "  'fact'],\n",
      " ['folk',\n",
      "  'opener',\n",
      "  'indie',\n",
      "  'group',\n",
      "  'chorus',\n",
      "  'past',\n",
      "  'melodies',\n",
      "  'musical',\n",
      "  'vocal',\n",
      "  'singing'],\n",
      " ['instrumental',\n",
      "  'group',\n",
      "  'moments',\n",
      "  'minutes',\n",
      "  'piano',\n",
      "  'melody',\n",
      "  'playing',\n",
      "  'drums',\n",
      "  'bass',\n",
      "  'sense'],\n",
      " ['group',\n",
      "  'minutes',\n",
      "  'instrumental',\n",
      "  'melody',\n",
      "  'piano',\n",
      "  'moments',\n",
      "  'drums',\n",
      "  'bass',\n",
      "  'minute',\n",
      "  'sense'],\n",
      " ['group',\n",
      "  'set',\n",
      "  'release',\n",
      "  'ep',\n",
      "  'musical',\n",
      "  'moments',\n",
      "  'recorded',\n",
      "  'place',\n",
      "  'making',\n",
      "  'sense'],\n",
      " ['group',\n",
      "  'instrumental',\n",
      "  'minutes',\n",
      "  'melody',\n",
      "  'piano',\n",
      "  'moments',\n",
      "  'drums',\n",
      "  'bass',\n",
      "  'minute',\n",
      "  'sense'],\n",
      " ['piece',\n",
      "  'noise',\n",
      "  'jazz',\n",
      "  'pieces',\n",
      "  'drone',\n",
      "  'minutes',\n",
      "  'metal',\n",
      "  'piano',\n",
      "  'minute',\n",
      "  'percussion'],\n",
      " ['folk',\n",
      "  'opener',\n",
      "  'chorus',\n",
      "  'indie',\n",
      "  'past',\n",
      "  'group',\n",
      "  'melodies',\n",
      "  'musical',\n",
      "  'fact',\n",
      "  'vocal'],\n",
      " ['group',\n",
      "  'set',\n",
      "  'release',\n",
      "  'moments',\n",
      "  'sense',\n",
      "  'ep',\n",
      "  'place',\n",
      "  'recorded',\n",
      "  'hear',\n",
      "  'making'],\n",
      " ['sings',\n",
      "  'life',\n",
      "  'chorus',\n",
      "  'indie',\n",
      "  'big',\n",
      "  'singing',\n",
      "  'words',\n",
      "  'past',\n",
      "  'home',\n",
      "  'line'],\n",
      " ['group',\n",
      "  'moments',\n",
      "  'melody',\n",
      "  'sense',\n",
      "  'set',\n",
      "  'musical',\n",
      "  'release',\n",
      "  'ep',\n",
      "  'place',\n",
      "  'drums'],\n",
      " ['chorus',\n",
      "  'indie',\n",
      "  'folk',\n",
      "  'opener',\n",
      "  'sings',\n",
      "  'group',\n",
      "  'past',\n",
      "  'life',\n",
      "  'fact',\n",
      "  'singing'],\n",
      " ['minutes',\n",
      "  'group',\n",
      "  'piano',\n",
      "  'melody',\n",
      "  'drums',\n",
      "  'moments',\n",
      "  'bass',\n",
      "  'minute',\n",
      "  'instrumental',\n",
      "  'noise'],\n",
      " ['live',\n",
      "  'version',\n",
      "  'disc',\n",
      "  'cover',\n",
      "  'covers',\n",
      "  'country',\n",
      "  'studio',\n",
      "  'fans',\n",
      "  'material',\n",
      "  'recorded']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120 KL_theta: is 1.89 .. Rec_loss: 1432.78 .. NELBO: 1434.67\n",
      "Epoch: 120 KL_theta: is 1.9 .. Rec_loss: 1432.73 .. NELBO: 1434.63\n",
      "****************************************************************************************************\n",
      "Epoch: 120 KL_theta: is 1.91 .. Rec_loss: 1432.68 .. NELBO: 1434.59\n",
      "Epoch: 121 KL_theta: is 1.91 .. Rec_loss: 1432.65 .. NELBO: 1434.56\n",
      "Epoch: 121 KL_theta: is 1.93 .. Rec_loss: 1432.58 .. NELBO: 1434.51\n",
      "****************************************************************************************************\n",
      "Epoch: 121 KL_theta: is 1.93 .. Rec_loss: 1432.58 .. NELBO: 1434.51\n",
      "Epoch: 122 KL_theta: is 1.93 .. Rec_loss: 1432.54 .. NELBO: 1434.47\n",
      "Epoch: 122 KL_theta: is 1.95 .. Rec_loss: 1432.48 .. NELBO: 1434.43\n",
      "****************************************************************************************************\n",
      "Epoch: 122 KL_theta: is 1.96 .. Rec_loss: 1432.47 .. NELBO: 1434.43\n",
      "Epoch: 123 KL_theta: is 1.96 .. Rec_loss: 1432.43 .. NELBO: 1434.39\n",
      "Epoch: 123 KL_theta: is 1.98 .. Rec_loss: 1432.38 .. NELBO: 1434.36\n",
      "****************************************************************************************************\n",
      "Epoch: 123 KL_theta: is 1.98 .. Rec_loss: 1432.36 .. NELBO: 1434.34\n",
      "Epoch: 124 KL_theta: is 1.98 .. Rec_loss: 1432.34 .. NELBO: 1434.32\n",
      "Epoch: 124 KL_theta: is 2.0 .. Rec_loss: 1432.28 .. NELBO: 1434.28\n",
      "****************************************************************************************************\n",
      "Epoch: 124 KL_theta: is 2.01 .. Rec_loss: 1432.25 .. NELBO: 1434.26\n",
      "Epoch: 125 KL_theta: is 2.01 .. Rec_loss: 1432.25 .. NELBO: 1434.26\n",
      "Epoch: 125 KL_theta: is 2.03 .. Rec_loss: 1432.15 .. NELBO: 1434.18\n",
      "****************************************************************************************************\n",
      "Epoch: 125 KL_theta: is 2.03 .. Rec_loss: 1432.15 .. NELBO: 1434.18\n",
      "Epoch: 126 KL_theta: is 2.03 .. Rec_loss: 1432.15 .. NELBO: 1434.18\n",
      "Epoch: 126 KL_theta: is 2.05 .. Rec_loss: 1432.06 .. NELBO: 1434.11\n",
      "****************************************************************************************************\n",
      "Epoch: 126 KL_theta: is 2.06 .. Rec_loss: 1432.05 .. NELBO: 1434.11\n",
      "Epoch: 127 KL_theta: is 2.06 .. Rec_loss: 1432.06 .. NELBO: 1434.12\n",
      "Epoch: 127 KL_theta: is 2.08 .. Rec_loss: 1431.96 .. NELBO: 1434.04\n",
      "****************************************************************************************************\n",
      "Epoch: 127 KL_theta: is 2.08 .. Rec_loss: 1431.94 .. NELBO: 1434.02\n",
      "Epoch: 128 KL_theta: is 2.08 .. Rec_loss: 1431.89 .. NELBO: 1433.97\n",
      "Epoch: 128 KL_theta: is 2.1 .. Rec_loss: 1431.83 .. NELBO: 1433.93\n",
      "****************************************************************************************************\n",
      "Epoch: 128 KL_theta: is 2.11 .. Rec_loss: 1431.84 .. NELBO: 1433.95\n",
      "Epoch: 129 KL_theta: is 2.11 .. Rec_loss: 1431.84 .. NELBO: 1433.95\n",
      "Epoch: 129 KL_theta: is 2.13 .. Rec_loss: 1431.74 .. NELBO: 1433.87\n",
      "****************************************************************************************************\n",
      "Epoch: 129 KL_theta: is 2.13 .. Rec_loss: 1431.73 .. NELBO: 1433.86\n",
      "Epoch: 130 KL_theta: is 2.14 .. Rec_loss: 1431.72 .. NELBO: 1433.86\n",
      "Epoch: 130 KL_theta: is 2.15 .. Rec_loss: 1431.64 .. NELBO: 1433.79\n",
      "****************************************************************************************************\n",
      "Epoch: 130 KL_theta: is 2.16 .. Rec_loss: 1431.62 .. NELBO: 1433.78\n",
      "Epoch: 131 KL_theta: is 2.16 .. Rec_loss: 1431.6 .. NELBO: 1433.76\n",
      "Epoch: 131 KL_theta: is 2.18 .. Rec_loss: 1431.52 .. NELBO: 1433.7\n",
      "****************************************************************************************************\n",
      "Epoch: 131 KL_theta: is 2.18 .. Rec_loss: 1431.52 .. NELBO: 1433.7\n",
      "Epoch: 132 KL_theta: is 2.19 .. Rec_loss: 1431.51 .. NELBO: 1433.7\n",
      "Epoch: 132 KL_theta: is 2.2 .. Rec_loss: 1431.42 .. NELBO: 1433.62\n",
      "****************************************************************************************************\n",
      "Epoch: 132 KL_theta: is 2.21 .. Rec_loss: 1431.42 .. NELBO: 1433.63\n",
      "Epoch: 133 KL_theta: is 2.21 .. Rec_loss: 1431.37 .. NELBO: 1433.58\n",
      "Epoch: 133 KL_theta: is 2.23 .. Rec_loss: 1431.32 .. NELBO: 1433.55\n",
      "****************************************************************************************************\n",
      "Epoch: 133 KL_theta: is 2.23 .. Rec_loss: 1431.32 .. NELBO: 1433.55\n",
      "Epoch: 134 KL_theta: is 2.23 .. Rec_loss: 1431.3 .. NELBO: 1433.53\n",
      "Epoch: 134 KL_theta: is 2.25 .. Rec_loss: 1431.23 .. NELBO: 1433.48\n",
      "****************************************************************************************************\n",
      "Epoch: 134 KL_theta: is 2.25 .. Rec_loss: 1431.22 .. NELBO: 1433.47\n",
      "Epoch: 135 KL_theta: is 2.26 .. Rec_loss: 1431.18 .. NELBO: 1433.44\n",
      "Epoch: 135 KL_theta: is 2.27 .. Rec_loss: 1431.13 .. NELBO: 1433.4\n",
      "****************************************************************************************************\n",
      "Epoch: 135 KL_theta: is 2.28 .. Rec_loss: 1431.11 .. NELBO: 1433.39\n",
      "Epoch: 136 KL_theta: is 2.28 .. Rec_loss: 1431.1 .. NELBO: 1433.38\n",
      "Epoch: 136 KL_theta: is 2.3 .. Rec_loss: 1431.01 .. NELBO: 1433.31\n",
      "****************************************************************************************************\n",
      "Epoch: 136 KL_theta: is 2.3 .. Rec_loss: 1431.01 .. NELBO: 1433.31\n",
      "Epoch: 137 KL_theta: is 2.31 .. Rec_loss: 1430.99 .. NELBO: 1433.3\n",
      "Epoch: 137 KL_theta: is 2.32 .. Rec_loss: 1430.93 .. NELBO: 1433.25\n",
      "****************************************************************************************************\n",
      "Epoch: 137 KL_theta: is 2.33 .. Rec_loss: 1430.9 .. NELBO: 1433.23\n",
      "Epoch: 138 KL_theta: is 2.33 .. Rec_loss: 1430.87 .. NELBO: 1433.2\n",
      "Epoch: 138 KL_theta: is 2.35 .. Rec_loss: 1430.82 .. NELBO: 1433.17\n",
      "****************************************************************************************************\n",
      "Epoch: 138 KL_theta: is 2.35 .. Rec_loss: 1430.79 .. NELBO: 1433.14\n",
      "Epoch: 139 KL_theta: is 2.36 .. Rec_loss: 1430.76 .. NELBO: 1433.12\n",
      "Epoch: 139 KL_theta: is 2.37 .. Rec_loss: 1430.69 .. NELBO: 1433.06\n",
      "****************************************************************************************************\n",
      "Epoch: 139 KL_theta: is 2.38 .. Rec_loss: 1430.69 .. NELBO: 1433.07\n",
      "Epoch: 140 KL_theta: is 2.38 .. Rec_loss: 1430.69 .. NELBO: 1433.07\n",
      "Epoch: 140 KL_theta: is 2.4 .. Rec_loss: 1430.62 .. NELBO: 1433.02\n",
      "****************************************************************************************************\n",
      "Epoch: 140 KL_theta: is 2.4 .. Rec_loss: 1430.59 .. NELBO: 1432.99\n",
      "Epoch: 141 KL_theta: is 2.4 .. Rec_loss: 1430.61 .. NELBO: 1433.01\n",
      "Epoch: 141 KL_theta: is 2.42 .. Rec_loss: 1430.52 .. NELBO: 1432.94\n",
      "****************************************************************************************************\n",
      "Epoch: 141 KL_theta: is 2.42 .. Rec_loss: 1430.48 .. NELBO: 1432.9\n",
      "Epoch: 142 KL_theta: is 2.43 .. Rec_loss: 1430.46 .. NELBO: 1432.89\n",
      "Epoch: 142 KL_theta: is 2.44 .. Rec_loss: 1430.39 .. NELBO: 1432.83\n",
      "****************************************************************************************************\n",
      "Epoch: 142 KL_theta: is 2.45 .. Rec_loss: 1430.37 .. NELBO: 1432.82\n",
      "Epoch: 143 KL_theta: is 2.45 .. Rec_loss: 1430.38 .. NELBO: 1432.83\n",
      "Epoch: 143 KL_theta: is 2.47 .. Rec_loss: 1430.29 .. NELBO: 1432.76\n",
      "****************************************************************************************************\n",
      "Epoch: 143 KL_theta: is 2.47 .. Rec_loss: 1430.28 .. NELBO: 1432.75\n",
      "Epoch: 144 KL_theta: is 2.47 .. Rec_loss: 1430.28 .. NELBO: 1432.75\n",
      "Epoch: 144 KL_theta: is 2.49 .. Rec_loss: 1430.2 .. NELBO: 1432.69\n",
      "****************************************************************************************************\n",
      "Epoch: 144 KL_theta: is 2.49 .. Rec_loss: 1430.18 .. NELBO: 1432.67\n",
      "Epoch: 145 KL_theta: is 2.5 .. Rec_loss: 1430.16 .. NELBO: 1432.66\n",
      "Epoch: 145 KL_theta: is 2.51 .. Rec_loss: 1430.09 .. NELBO: 1432.6\n",
      "****************************************************************************************************\n",
      "Epoch: 145 KL_theta: is 2.52 .. Rec_loss: 1430.08 .. NELBO: 1432.6\n",
      "Epoch: 146 KL_theta: is 2.52 .. Rec_loss: 1430.06 .. NELBO: 1432.58\n",
      "Epoch: 146 KL_theta: is 2.54 .. Rec_loss: 1429.99 .. NELBO: 1432.53\n",
      "****************************************************************************************************\n",
      "Epoch: 146 KL_theta: is 2.54 .. Rec_loss: 1429.99 .. NELBO: 1432.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 147 KL_theta: is 2.54 .. Rec_loss: 1429.98 .. NELBO: 1432.52\n",
      "Epoch: 147 KL_theta: is 2.56 .. Rec_loss: 1429.91 .. NELBO: 1432.47\n",
      "****************************************************************************************************\n",
      "Epoch: 147 KL_theta: is 2.56 .. Rec_loss: 1429.89 .. NELBO: 1432.45\n",
      "Epoch: 148 KL_theta: is 2.57 .. Rec_loss: 1429.87 .. NELBO: 1432.44\n",
      "Epoch: 148 KL_theta: is 2.58 .. Rec_loss: 1429.81 .. NELBO: 1432.39\n",
      "****************************************************************************************************\n",
      "Epoch: 148 KL_theta: is 2.58 .. Rec_loss: 1429.79 .. NELBO: 1432.37\n",
      "Epoch: 149 KL_theta: is 2.59 .. Rec_loss: 1429.78 .. NELBO: 1432.37\n",
      "Epoch: 149 KL_theta: is 2.6 .. Rec_loss: 1429.71 .. NELBO: 1432.31\n",
      "****************************************************************************************************\n",
      "Epoch: 149 KL_theta: is 2.61 .. Rec_loss: 1429.69 .. NELBO: 1432.3\n",
      "Epoch: 150 KL_theta: is 2.61 .. Rec_loss: 1429.68 .. NELBO: 1432.29\n",
      "Epoch: 150 KL_theta: is 2.63 .. Rec_loss: 1429.6 .. NELBO: 1432.23\n",
      "****************************************************************************************************\n",
      "Epoch: 150 KL_theta: is 2.63 .. Rec_loss: 1429.6 .. NELBO: 1432.23\n",
      "Epoch: 151 KL_theta: is 2.63 .. Rec_loss: 1429.59 .. NELBO: 1432.22\n",
      "Epoch: 151 KL_theta: is 2.65 .. Rec_loss: 1429.51 .. NELBO: 1432.16\n",
      "****************************************************************************************************\n",
      "Epoch: 151 KL_theta: is 2.65 .. Rec_loss: 1429.52 .. NELBO: 1432.17\n",
      "Epoch: 152 KL_theta: is 2.66 .. Rec_loss: 1429.5 .. NELBO: 1432.16\n",
      "Epoch: 152 KL_theta: is 2.67 .. Rec_loss: 1429.45 .. NELBO: 1432.12\n",
      "****************************************************************************************************\n",
      "Epoch: 152 KL_theta: is 2.68 .. Rec_loss: 1429.42 .. NELBO: 1432.1\n",
      "Epoch: 153 KL_theta: is 2.68 .. Rec_loss: 1429.39 .. NELBO: 1432.07\n",
      "Epoch: 153 KL_theta: is 2.7 .. Rec_loss: 1429.32 .. NELBO: 1432.02\n",
      "****************************************************************************************************\n",
      "Epoch: 153 KL_theta: is 2.7 .. Rec_loss: 1429.33 .. NELBO: 1432.03\n",
      "Epoch: 154 KL_theta: is 2.7 .. Rec_loss: 1429.31 .. NELBO: 1432.01\n",
      "Epoch: 154 KL_theta: is 2.72 .. Rec_loss: 1429.25 .. NELBO: 1431.97\n",
      "****************************************************************************************************\n",
      "Epoch: 154 KL_theta: is 2.72 .. Rec_loss: 1429.23 .. NELBO: 1431.95\n",
      "Epoch: 155 KL_theta: is 2.73 .. Rec_loss: 1429.22 .. NELBO: 1431.95\n",
      "Epoch: 155 KL_theta: is 2.74 .. Rec_loss: 1429.14 .. NELBO: 1431.88\n",
      "****************************************************************************************************\n",
      "Epoch: 155 KL_theta: is 2.74 .. Rec_loss: 1429.14 .. NELBO: 1431.88\n",
      "Epoch: 156 KL_theta: is 2.75 .. Rec_loss: 1429.11 .. NELBO: 1431.86\n",
      "Epoch: 156 KL_theta: is 2.76 .. Rec_loss: 1429.07 .. NELBO: 1431.83\n",
      "****************************************************************************************************\n",
      "Epoch: 156 KL_theta: is 2.77 .. Rec_loss: 1429.05 .. NELBO: 1431.82\n",
      "Epoch: 157 KL_theta: is 2.77 .. Rec_loss: 1429.05 .. NELBO: 1431.82\n",
      "Epoch: 157 KL_theta: is 2.79 .. Rec_loss: 1428.97 .. NELBO: 1431.76\n",
      "****************************************************************************************************\n",
      "Epoch: 157 KL_theta: is 2.79 .. Rec_loss: 1428.95 .. NELBO: 1431.74\n",
      "Epoch: 158 KL_theta: is 2.79 .. Rec_loss: 1428.94 .. NELBO: 1431.73\n",
      "Epoch: 158 KL_theta: is 2.81 .. Rec_loss: 1428.87 .. NELBO: 1431.68\n",
      "****************************************************************************************************\n",
      "Epoch: 158 KL_theta: is 2.81 .. Rec_loss: 1428.86 .. NELBO: 1431.67\n",
      "Epoch: 159 KL_theta: is 2.82 .. Rec_loss: 1428.84 .. NELBO: 1431.66\n",
      "Epoch: 159 KL_theta: is 2.83 .. Rec_loss: 1428.78 .. NELBO: 1431.61\n",
      "****************************************************************************************************\n",
      "Epoch: 159 KL_theta: is 2.83 .. Rec_loss: 1428.77 .. NELBO: 1431.6\n",
      "torch.Size([20, 7291]) 20\n",
      "(20, 100)\n",
      "topic diversity is 0.3075\n",
      "[['rap',\n",
      "  'hip_hop',\n",
      "  'beats',\n",
      "  'production',\n",
      "  'beat',\n",
      "  'rapper',\n",
      "  'life',\n",
      "  'black',\n",
      "  'soul',\n",
      "  'style'],\n",
      " ['group',\n",
      "  'release',\n",
      "  'playing',\n",
      "  'interesting',\n",
      "  'point',\n",
      "  'instrumental',\n",
      "  'ep',\n",
      "  'hear',\n",
      "  'musical',\n",
      "  'recorded'],\n",
      " ['life',\n",
      "  'sings',\n",
      "  'singing',\n",
      "  'death',\n",
      "  'singer',\n",
      "  'home',\n",
      "  'days',\n",
      "  'words',\n",
      "  'group',\n",
      "  'past'],\n",
      " ['punk',\n",
      "  'indie',\n",
      "  'smith',\n",
      "  'kids',\n",
      "  'girls',\n",
      "  'garage',\n",
      "  'fun',\n",
      "  'guys',\n",
      "  'chorus',\n",
      "  'boys'],\n",
      " ['dance',\n",
      "  'electronic',\n",
      "  'house',\n",
      "  'beats',\n",
      "  'label',\n",
      "  'techno',\n",
      "  'disco',\n",
      "  'mix',\n",
      "  'beat',\n",
      "  'synth'],\n",
      " ['folk',\n",
      "  'acoustic',\n",
      "  'acoustic_guitar',\n",
      "  'strings',\n",
      "  'piano',\n",
      "  'percussion',\n",
      "  'melodies',\n",
      "  'gentle',\n",
      "  'indie',\n",
      "  'harmonies'],\n",
      " ['light',\n",
      "  'moments',\n",
      "  'words',\n",
      "  'sense',\n",
      "  'opener',\n",
      "  'past',\n",
      "  'life',\n",
      "  'sings',\n",
      "  'place',\n",
      "  'dark'],\n",
      " ['moments',\n",
      "  'past',\n",
      "  'light',\n",
      "  'sense',\n",
      "  'place',\n",
      "  'life',\n",
      "  'opener',\n",
      "  'words',\n",
      "  'moment',\n",
      "  'dark'],\n",
      " ['instrumental',\n",
      "  'bass',\n",
      "  'melody',\n",
      "  'minutes',\n",
      "  'group',\n",
      "  'rhythm',\n",
      "  'playing',\n",
      "  'interesting',\n",
      "  'post',\n",
      "  'drums'],\n",
      " ['melody',\n",
      "  'instrumental',\n",
      "  'drums',\n",
      "  'bass',\n",
      "  'minutes',\n",
      "  'moments',\n",
      "  'melodic',\n",
      "  'rhythm',\n",
      "  'noise',\n",
      "  'simple'],\n",
      " ['ep',\n",
      "  'musical',\n",
      "  'making',\n",
      "  'half',\n",
      "  'point',\n",
      "  'find',\n",
      "  'place',\n",
      "  'fact',\n",
      "  'moments',\n",
      "  'past'],\n",
      " ['instrumental',\n",
      "  'melody',\n",
      "  'minutes',\n",
      "  'bass',\n",
      "  'drums',\n",
      "  'moments',\n",
      "  'rhythm',\n",
      "  'simple',\n",
      "  'minute',\n",
      "  'group'],\n",
      " ['piece',\n",
      "  'noise',\n",
      "  'pieces',\n",
      "  'drone',\n",
      "  'minutes',\n",
      "  'jazz',\n",
      "  'electronic',\n",
      "  'minute',\n",
      "  'piano',\n",
      "  'metal'],\n",
      " ['words',\n",
      "  'life',\n",
      "  'sings',\n",
      "  'light',\n",
      "  'moments',\n",
      "  'past',\n",
      "  'sense',\n",
      "  'opener',\n",
      "  'place',\n",
      "  'chorus'],\n",
      " ['ep',\n",
      "  'release',\n",
      "  'point',\n",
      "  'making',\n",
      "  'moments',\n",
      "  'musical',\n",
      "  'group',\n",
      "  'place',\n",
      "  'half',\n",
      "  'find'],\n",
      " ['sings',\n",
      "  'life',\n",
      "  'death',\n",
      "  'words',\n",
      "  'country',\n",
      "  'singing',\n",
      "  'singer',\n",
      "  'woman',\n",
      "  'home',\n",
      "  'story'],\n",
      " ['moments',\n",
      "  'ep',\n",
      "  'sense',\n",
      "  'vocal',\n",
      "  'melody',\n",
      "  'musical',\n",
      "  'drums',\n",
      "  'half',\n",
      "  'place',\n",
      "  'past'],\n",
      " ['life',\n",
      "  'sings',\n",
      "  'words',\n",
      "  'chorus',\n",
      "  'past',\n",
      "  'indie',\n",
      "  'place',\n",
      "  'singing',\n",
      "  'opener',\n",
      "  'home'],\n",
      " ['melody',\n",
      "  'drums',\n",
      "  'bass',\n",
      "  'minutes',\n",
      "  'instrumental',\n",
      "  'noise',\n",
      "  'moments',\n",
      "  'percussion',\n",
      "  'melodic',\n",
      "  'melodies'],\n",
      " ['live',\n",
      "  'version',\n",
      "  'disc',\n",
      "  'set',\n",
      "  'recorded',\n",
      "  'original',\n",
      "  'material',\n",
      "  'collection',\n",
      "  'group',\n",
      "  'release']]\n",
      "Epoch: 160 KL_theta: is 2.84 .. Rec_loss: 1428.77 .. NELBO: 1431.61\n",
      "Epoch: 160 KL_theta: is 2.85 .. Rec_loss: 1428.69 .. NELBO: 1431.54\n",
      "****************************************************************************************************\n",
      "Epoch: 160 KL_theta: is 2.86 .. Rec_loss: 1428.68 .. NELBO: 1431.54\n",
      "Epoch: 161 KL_theta: is 2.86 .. Rec_loss: 1428.65 .. NELBO: 1431.51\n",
      "Epoch: 161 KL_theta: is 2.88 .. Rec_loss: 1428.61 .. NELBO: 1431.49\n",
      "****************************************************************************************************\n",
      "Epoch: 161 KL_theta: is 2.88 .. Rec_loss: 1428.59 .. NELBO: 1431.47\n",
      "Epoch: 162 KL_theta: is 2.88 .. Rec_loss: 1428.57 .. NELBO: 1431.45\n",
      "Epoch: 162 KL_theta: is 2.9 .. Rec_loss: 1428.52 .. NELBO: 1431.42\n",
      "****************************************************************************************************\n",
      "Epoch: 162 KL_theta: is 2.9 .. Rec_loss: 1428.48 .. NELBO: 1431.38\n",
      "Epoch: 163 KL_theta: is 2.91 .. Rec_loss: 1428.49 .. NELBO: 1431.4\n",
      "Epoch: 163 KL_theta: is 2.92 .. Rec_loss: 1428.42 .. NELBO: 1431.34\n",
      "****************************************************************************************************\n",
      "Epoch: 163 KL_theta: is 2.92 .. Rec_loss: 1428.38 .. NELBO: 1431.3\n",
      "Epoch: 164 KL_theta: is 2.93 .. Rec_loss: 1428.37 .. NELBO: 1431.3\n",
      "Epoch: 164 KL_theta: is 2.94 .. Rec_loss: 1428.31 .. NELBO: 1431.25\n",
      "****************************************************************************************************\n",
      "Epoch: 164 KL_theta: is 2.95 .. Rec_loss: 1428.29 .. NELBO: 1431.24\n",
      "Epoch: 165 KL_theta: is 2.95 .. Rec_loss: 1428.3 .. NELBO: 1431.25\n",
      "Epoch: 165 KL_theta: is 2.97 .. Rec_loss: 1428.22 .. NELBO: 1431.19\n",
      "****************************************************************************************************\n",
      "Epoch: 165 KL_theta: is 2.97 .. Rec_loss: 1428.19 .. NELBO: 1431.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166 KL_theta: is 2.97 .. Rec_loss: 1428.19 .. NELBO: 1431.16\n",
      "Epoch: 166 KL_theta: is 2.99 .. Rec_loss: 1428.1 .. NELBO: 1431.09\n",
      "****************************************************************************************************\n",
      "Epoch: 166 KL_theta: is 2.99 .. Rec_loss: 1428.11 .. NELBO: 1431.1\n",
      "Epoch: 167 KL_theta: is 3.0 .. Rec_loss: 1428.07 .. NELBO: 1431.07\n",
      "Epoch: 167 KL_theta: is 3.01 .. Rec_loss: 1428.03 .. NELBO: 1431.04\n",
      "****************************************************************************************************\n",
      "Epoch: 167 KL_theta: is 3.01 .. Rec_loss: 1428.01 .. NELBO: 1431.02\n",
      "Epoch: 168 KL_theta: is 3.02 .. Rec_loss: 1427.98 .. NELBO: 1431.0\n",
      "Epoch: 168 KL_theta: is 3.03 .. Rec_loss: 1427.92 .. NELBO: 1430.95\n",
      "****************************************************************************************************\n",
      "Epoch: 168 KL_theta: is 3.04 .. Rec_loss: 1427.93 .. NELBO: 1430.97\n",
      "Epoch: 169 KL_theta: is 3.04 .. Rec_loss: 1427.91 .. NELBO: 1430.95\n",
      "Epoch: 169 KL_theta: is 3.06 .. Rec_loss: 1427.86 .. NELBO: 1430.92\n",
      "****************************************************************************************************\n",
      "Epoch: 169 KL_theta: is 3.06 .. Rec_loss: 1427.84 .. NELBO: 1430.9\n",
      "Epoch: 170 KL_theta: is 3.06 .. Rec_loss: 1427.83 .. NELBO: 1430.89\n",
      "Epoch: 170 KL_theta: is 3.08 .. Rec_loss: 1427.77 .. NELBO: 1430.85\n",
      "****************************************************************************************************\n",
      "Epoch: 170 KL_theta: is 3.08 .. Rec_loss: 1427.75 .. NELBO: 1430.83\n",
      "Epoch: 171 KL_theta: is 3.09 .. Rec_loss: 1427.73 .. NELBO: 1430.82\n",
      "Epoch: 171 KL_theta: is 3.1 .. Rec_loss: 1427.67 .. NELBO: 1430.77\n",
      "****************************************************************************************************\n",
      "Epoch: 171 KL_theta: is 3.1 .. Rec_loss: 1427.66 .. NELBO: 1430.76\n",
      "Epoch: 172 KL_theta: is 3.11 .. Rec_loss: 1427.66 .. NELBO: 1430.77\n",
      "Epoch: 172 KL_theta: is 3.12 .. Rec_loss: 1427.59 .. NELBO: 1430.71\n",
      "****************************************************************************************************\n",
      "Epoch: 172 KL_theta: is 3.13 .. Rec_loss: 1427.57 .. NELBO: 1430.7\n",
      "Epoch: 173 KL_theta: is 3.13 .. Rec_loss: 1427.54 .. NELBO: 1430.67\n",
      "Epoch: 173 KL_theta: is 3.15 .. Rec_loss: 1427.49 .. NELBO: 1430.64\n",
      "****************************************************************************************************\n",
      "Epoch: 173 KL_theta: is 3.15 .. Rec_loss: 1427.48 .. NELBO: 1430.63\n",
      "Epoch: 174 KL_theta: is 3.15 .. Rec_loss: 1427.46 .. NELBO: 1430.61\n",
      "Epoch: 174 KL_theta: is 3.17 .. Rec_loss: 1427.41 .. NELBO: 1430.58\n",
      "****************************************************************************************************\n",
      "Epoch: 174 KL_theta: is 3.17 .. Rec_loss: 1427.39 .. NELBO: 1430.56\n",
      "Epoch: 175 KL_theta: is 3.18 .. Rec_loss: 1427.37 .. NELBO: 1430.55\n",
      "Epoch: 175 KL_theta: is 3.19 .. Rec_loss: 1427.31 .. NELBO: 1430.5\n",
      "****************************************************************************************************\n",
      "Epoch: 175 KL_theta: is 3.19 .. Rec_loss: 1427.31 .. NELBO: 1430.5\n",
      "Epoch: 176 KL_theta: is 3.2 .. Rec_loss: 1427.29 .. NELBO: 1430.49\n",
      "Epoch: 176 KL_theta: is 3.21 .. Rec_loss: 1427.23 .. NELBO: 1430.44\n",
      "****************************************************************************************************\n",
      "Epoch: 176 KL_theta: is 3.22 .. Rec_loss: 1427.22 .. NELBO: 1430.44\n",
      "Epoch: 177 KL_theta: is 3.22 .. Rec_loss: 1427.21 .. NELBO: 1430.43\n",
      "Epoch: 177 KL_theta: is 3.24 .. Rec_loss: 1427.14 .. NELBO: 1430.38\n",
      "****************************************************************************************************\n",
      "Epoch: 177 KL_theta: is 3.24 .. Rec_loss: 1427.13 .. NELBO: 1430.37\n",
      "Epoch: 178 KL_theta: is 3.24 .. Rec_loss: 1427.12 .. NELBO: 1430.36\n",
      "Epoch: 178 KL_theta: is 3.26 .. Rec_loss: 1427.06 .. NELBO: 1430.32\n",
      "****************************************************************************************************\n",
      "Epoch: 178 KL_theta: is 3.26 .. Rec_loss: 1427.05 .. NELBO: 1430.31\n",
      "Epoch: 179 KL_theta: is 3.27 .. Rec_loss: 1427.05 .. NELBO: 1430.32\n",
      "Epoch: 179 KL_theta: is 3.28 .. Rec_loss: 1426.98 .. NELBO: 1430.26\n",
      "****************************************************************************************************\n",
      "Epoch: 179 KL_theta: is 3.28 .. Rec_loss: 1426.95 .. NELBO: 1430.23\n",
      "Epoch: 180 KL_theta: is 3.29 .. Rec_loss: 1426.97 .. NELBO: 1430.26\n",
      "Epoch: 180 KL_theta: is 3.3 .. Rec_loss: 1426.87 .. NELBO: 1430.17\n",
      "****************************************************************************************************\n",
      "Epoch: 180 KL_theta: is 3.31 .. Rec_loss: 1426.87 .. NELBO: 1430.18\n",
      "Epoch: 181 KL_theta: is 3.31 .. Rec_loss: 1426.84 .. NELBO: 1430.15\n",
      "Epoch: 181 KL_theta: is 3.33 .. Rec_loss: 1426.79 .. NELBO: 1430.12\n",
      "****************************************************************************************************\n",
      "Epoch: 181 KL_theta: is 3.33 .. Rec_loss: 1426.78 .. NELBO: 1430.11\n",
      "Epoch: 182 KL_theta: is 3.33 .. Rec_loss: 1426.77 .. NELBO: 1430.1\n",
      "Epoch: 182 KL_theta: is 3.35 .. Rec_loss: 1426.71 .. NELBO: 1430.06\n",
      "****************************************************************************************************\n",
      "Epoch: 182 KL_theta: is 3.35 .. Rec_loss: 1426.69 .. NELBO: 1430.04\n",
      "Epoch: 183 KL_theta: is 3.36 .. Rec_loss: 1426.67 .. NELBO: 1430.03\n",
      "Epoch: 183 KL_theta: is 3.37 .. Rec_loss: 1426.62 .. NELBO: 1429.99\n",
      "****************************************************************************************************\n",
      "Epoch: 183 KL_theta: is 3.37 .. Rec_loss: 1426.6 .. NELBO: 1429.97\n",
      "Epoch: 184 KL_theta: is 3.38 .. Rec_loss: 1426.6 .. NELBO: 1429.98\n",
      "Epoch: 184 KL_theta: is 3.39 .. Rec_loss: 1426.53 .. NELBO: 1429.92\n",
      "****************************************************************************************************\n",
      "Epoch: 184 KL_theta: is 3.4 .. Rec_loss: 1426.51 .. NELBO: 1429.91\n",
      "Epoch: 185 KL_theta: is 3.4 .. Rec_loss: 1426.49 .. NELBO: 1429.89\n",
      "Epoch: 185 KL_theta: is 3.41 .. Rec_loss: 1426.42 .. NELBO: 1429.83\n",
      "****************************************************************************************************\n",
      "Epoch: 185 KL_theta: is 3.42 .. Rec_loss: 1426.44 .. NELBO: 1429.86\n",
      "Epoch: 186 KL_theta: is 3.42 .. Rec_loss: 1426.42 .. NELBO: 1429.84\n",
      "Epoch: 186 KL_theta: is 3.44 .. Rec_loss: 1426.36 .. NELBO: 1429.8\n",
      "****************************************************************************************************\n",
      "Epoch: 186 KL_theta: is 3.44 .. Rec_loss: 1426.35 .. NELBO: 1429.79\n",
      "Epoch: 187 KL_theta: is 3.44 .. Rec_loss: 1426.34 .. NELBO: 1429.78\n",
      "Epoch: 187 KL_theta: is 3.46 .. Rec_loss: 1426.3 .. NELBO: 1429.76\n",
      "****************************************************************************************************\n",
      "Epoch: 187 KL_theta: is 3.46 .. Rec_loss: 1426.26 .. NELBO: 1429.72\n",
      "Epoch: 188 KL_theta: is 3.46 .. Rec_loss: 1426.24 .. NELBO: 1429.7\n",
      "Epoch: 188 KL_theta: is 3.48 .. Rec_loss: 1426.19 .. NELBO: 1429.67\n",
      "****************************************************************************************************\n",
      "Epoch: 188 KL_theta: is 3.48 .. Rec_loss: 1426.17 .. NELBO: 1429.65\n",
      "Epoch: 189 KL_theta: is 3.49 .. Rec_loss: 1426.16 .. NELBO: 1429.65\n",
      "Epoch: 189 KL_theta: is 3.5 .. Rec_loss: 1426.09 .. NELBO: 1429.59\n",
      "****************************************************************************************************\n",
      "Epoch: 189 KL_theta: is 3.5 .. Rec_loss: 1426.09 .. NELBO: 1429.59\n",
      "Epoch: 190 KL_theta: is 3.51 .. Rec_loss: 1426.07 .. NELBO: 1429.58\n",
      "Epoch: 190 KL_theta: is 3.52 .. Rec_loss: 1426.01 .. NELBO: 1429.53\n",
      "****************************************************************************************************\n",
      "Epoch: 190 KL_theta: is 3.53 .. Rec_loss: 1426.01 .. NELBO: 1429.54\n",
      "Epoch: 191 KL_theta: is 3.53 .. Rec_loss: 1425.99 .. NELBO: 1429.52\n",
      "Epoch: 191 KL_theta: is 3.54 .. Rec_loss: 1425.94 .. NELBO: 1429.48\n",
      "****************************************************************************************************\n",
      "Epoch: 191 KL_theta: is 3.55 .. Rec_loss: 1425.92 .. NELBO: 1429.47\n",
      "Epoch: 192 KL_theta: is 3.55 .. Rec_loss: 1425.92 .. NELBO: 1429.47\n",
      "Epoch: 192 KL_theta: is 3.56 .. Rec_loss: 1425.85 .. NELBO: 1429.41\n",
      "****************************************************************************************************\n",
      "Epoch: 192 KL_theta: is 3.57 .. Rec_loss: 1425.84 .. NELBO: 1429.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 193 KL_theta: is 3.57 .. Rec_loss: 1425.81 .. NELBO: 1429.38\n",
      "Epoch: 193 KL_theta: is 3.59 .. Rec_loss: 1425.75 .. NELBO: 1429.34\n",
      "****************************************************************************************************\n",
      "Epoch: 193 KL_theta: is 3.59 .. Rec_loss: 1425.76 .. NELBO: 1429.35\n",
      "Epoch: 194 KL_theta: is 3.59 .. Rec_loss: 1425.76 .. NELBO: 1429.35\n",
      "Epoch: 194 KL_theta: is 3.61 .. Rec_loss: 1425.69 .. NELBO: 1429.3\n",
      "****************************************************************************************************\n",
      "Epoch: 194 KL_theta: is 3.61 .. Rec_loss: 1425.68 .. NELBO: 1429.29\n",
      "Epoch: 195 KL_theta: is 3.62 .. Rec_loss: 1425.69 .. NELBO: 1429.31\n",
      "Epoch: 195 KL_theta: is 3.63 .. Rec_loss: 1425.61 .. NELBO: 1429.24\n",
      "****************************************************************************************************\n",
      "Epoch: 195 KL_theta: is 3.63 .. Rec_loss: 1425.6 .. NELBO: 1429.23\n",
      "Epoch: 196 KL_theta: is 3.64 .. Rec_loss: 1425.57 .. NELBO: 1429.21\n",
      "Epoch: 196 KL_theta: is 3.65 .. Rec_loss: 1425.52 .. NELBO: 1429.17\n",
      "****************************************************************************************************\n",
      "Epoch: 196 KL_theta: is 3.65 .. Rec_loss: 1425.52 .. NELBO: 1429.17\n",
      "Epoch: 197 KL_theta: is 3.66 .. Rec_loss: 1425.51 .. NELBO: 1429.17\n",
      "Epoch: 197 KL_theta: is 3.67 .. Rec_loss: 1425.46 .. NELBO: 1429.13\n",
      "****************************************************************************************************\n",
      "Epoch: 197 KL_theta: is 3.67 .. Rec_loss: 1425.43 .. NELBO: 1429.1\n",
      "Epoch: 198 KL_theta: is 3.68 .. Rec_loss: 1425.42 .. NELBO: 1429.1\n",
      "Epoch: 198 KL_theta: is 3.69 .. Rec_loss: 1425.37 .. NELBO: 1429.06\n",
      "****************************************************************************************************\n",
      "Epoch: 198 KL_theta: is 3.7 .. Rec_loss: 1425.34 .. NELBO: 1429.04\n",
      "Epoch: 199 KL_theta: is 3.7 .. Rec_loss: 1425.33 .. NELBO: 1429.03\n",
      "Epoch: 199 KL_theta: is 3.71 .. Rec_loss: 1425.28 .. NELBO: 1428.99\n",
      "****************************************************************************************************\n",
      "Epoch: 199 KL_theta: is 3.72 .. Rec_loss: 1425.26 .. NELBO: 1428.98\n",
      "torch.Size([20, 7291]) 20\n",
      "(20, 100)\n",
      "topic diversity is 0.3715\n",
      "[['rap',\n",
      "  'hip_hop',\n",
      "  'beats',\n",
      "  'beat',\n",
      "  'production',\n",
      "  'rapper',\n",
      "  'soul',\n",
      "  'black',\n",
      "  'style',\n",
      "  'life'],\n",
      " ['group',\n",
      "  'release',\n",
      "  'ep',\n",
      "  'interesting',\n",
      "  'hear',\n",
      "  'material',\n",
      "  'fact',\n",
      "  'point',\n",
      "  'studio',\n",
      "  'instrumental'],\n",
      " ['big',\n",
      "  'days',\n",
      "  'indie',\n",
      "  'fact',\n",
      "  'singer',\n",
      "  'chorus',\n",
      "  'cover',\n",
      "  'past',\n",
      "  'songwriting',\n",
      "  'singing'],\n",
      " ['punk',\n",
      "  'smith',\n",
      "  'girls',\n",
      "  'garage',\n",
      "  'kids',\n",
      "  'guys',\n",
      "  'indie',\n",
      "  'fun',\n",
      "  'boys',\n",
      "  'wave'],\n",
      " ['dance',\n",
      "  'house',\n",
      "  'techno',\n",
      "  'label',\n",
      "  'electronic',\n",
      "  'disco',\n",
      "  'beats',\n",
      "  'mix',\n",
      "  'remix',\n",
      "  'dub'],\n",
      " ['folk',\n",
      "  'acoustic',\n",
      "  'strings',\n",
      "  'acoustic_guitar',\n",
      "  'piano',\n",
      "  'country',\n",
      "  'gentle',\n",
      "  'violin',\n",
      "  'sings',\n",
      "  'johnson'],\n",
      " ['black',\n",
      "  'sense',\n",
      "  'words',\n",
      "  'light',\n",
      "  'power',\n",
      "  'dark',\n",
      "  'life',\n",
      "  'body',\n",
      "  'human',\n",
      "  'space'],\n",
      " ['moments',\n",
      "  'past',\n",
      "  'opener',\n",
      "  'place',\n",
      "  'sense',\n",
      "  'vocal',\n",
      "  'melodies',\n",
      "  'moment',\n",
      "  'light',\n",
      "  'indie'],\n",
      " ['jazz',\n",
      "  'metal',\n",
      "  'drummer',\n",
      "  'playing',\n",
      "  'group',\n",
      "  'trio',\n",
      "  'bass',\n",
      "  'post',\n",
      "  'guitarist',\n",
      "  'drums'],\n",
      " ['moments',\n",
      "  'melody',\n",
      "  'minutes',\n",
      "  'instrumental',\n",
      "  'drums',\n",
      "  'simple',\n",
      "  'minute',\n",
      "  'bass',\n",
      "  'approach',\n",
      "  'musical'],\n",
      " ['ep',\n",
      "  'indie',\n",
      "  'production',\n",
      "  'chorus',\n",
      "  'vocal',\n",
      "  'fact',\n",
      "  'big',\n",
      "  'past',\n",
      "  'release',\n",
      "  'point'],\n",
      " ['instrumental',\n",
      "  'minutes',\n",
      "  'moments',\n",
      "  'melody',\n",
      "  'minute',\n",
      "  'group',\n",
      "  'drums',\n",
      "  'point',\n",
      "  'interesting',\n",
      "  'approach'],\n",
      " ['piece',\n",
      "  'pieces',\n",
      "  'noise',\n",
      "  'piano',\n",
      "  'drone',\n",
      "  'ambient',\n",
      "  'composer',\n",
      "  'minute',\n",
      "  'electronic',\n",
      "  'minutes'],\n",
      " ['moments',\n",
      "  'sense',\n",
      "  'place',\n",
      "  'past',\n",
      "  'words',\n",
      "  'light',\n",
      "  'opener',\n",
      "  'moment',\n",
      "  'life',\n",
      "  'musical'],\n",
      " ['ep',\n",
      "  'release',\n",
      "  'point',\n",
      "  'fact',\n",
      "  'production',\n",
      "  'moments',\n",
      "  'group',\n",
      "  'making',\n",
      "  'vocal',\n",
      "  'sort'],\n",
      " ['sings',\n",
      "  'life',\n",
      "  'death',\n",
      "  'words',\n",
      "  'woman',\n",
      "  'story',\n",
      "  'personal',\n",
      "  'singing',\n",
      "  'young',\n",
      "  'country'],\n",
      " ['ep',\n",
      "  'moments',\n",
      "  'vocal',\n",
      "  'production',\n",
      "  'melody',\n",
      "  'melodies',\n",
      "  'half',\n",
      "  'point',\n",
      "  'release',\n",
      "  'musical'],\n",
      " ['indie',\n",
      "  'chorus',\n",
      "  'opener',\n",
      "  'past',\n",
      "  'vocal',\n",
      "  'melodies',\n",
      "  'singer',\n",
      "  'songwriting',\n",
      "  'singing',\n",
      "  'moments'],\n",
      " ['noise',\n",
      "  'melody',\n",
      "  'percussion',\n",
      "  'drums',\n",
      "  'rhythm',\n",
      "  'electronic',\n",
      "  'drone',\n",
      "  'bass',\n",
      "  'simple',\n",
      "  'tones'],\n",
      " ['live',\n",
      "  'disc',\n",
      "  'version',\n",
      "  'set',\n",
      "  'group',\n",
      "  'collection',\n",
      "  'original',\n",
      "  'recorded',\n",
      "  'material',\n",
      "  'release']]\n"
     ]
    }
   ],
   "source": [
    "#model.to(device)\n",
    "#wandb.watch(model, log=\"all\")\n",
    "train(model,batch_size=1024, learning_rate=2e-3,test_data=None,dictionary=dictionary,\n",
    "      num_epochs=200,is_evaluate=False,log_every=40,\n",
    "      ckpt=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e99fa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interpol': ['interpol', 'spoon', 'nme', 'devo', 'ramones']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get topic proportion\n",
    "#with model.eval():\n",
    "#    model.get_theta(x_bows_train[0])\n",
    "get_most_similar_words(model = model, queries=['interpol'], vocabulary = dictionary, n_most_similar=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c83b8f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rap',\n",
       "  'hip_hop',\n",
       "  'beats',\n",
       "  'beat',\n",
       "  'production',\n",
       "  'rapper',\n",
       "  'soul',\n",
       "  'black',\n",
       "  'style',\n",
       "  'life',\n",
       "  'flow',\n",
       "  'producer',\n",
       "  'rappers',\n",
       "  'r&b',\n",
       "  'raps'],\n",
       " ['group',\n",
       "  'release',\n",
       "  'ep',\n",
       "  'interesting',\n",
       "  'hear',\n",
       "  'material',\n",
       "  'fact',\n",
       "  'point',\n",
       "  'studio',\n",
       "  'instrumental',\n",
       "  'style',\n",
       "  'minutes',\n",
       "  'version',\n",
       "  'musical',\n",
       "  'play'],\n",
       " ['big',\n",
       "  'days',\n",
       "  'indie',\n",
       "  'fact',\n",
       "  'singer',\n",
       "  'chorus',\n",
       "  'cover',\n",
       "  'past',\n",
       "  'songwriting',\n",
       "  'singing',\n",
       "  'life',\n",
       "  'real',\n",
       "  'hear',\n",
       "  'home',\n",
       "  'friends'],\n",
       " ['punk',\n",
       "  'smith',\n",
       "  'girls',\n",
       "  'garage',\n",
       "  'kids',\n",
       "  'guys',\n",
       "  'indie',\n",
       "  'fun',\n",
       "  'boys',\n",
       "  'wave',\n",
       "  'fucking',\n",
       "  'post_punk',\n",
       "  'hardcore',\n",
       "  'group',\n",
       "  'riffs'],\n",
       " ['dance',\n",
       "  'house',\n",
       "  'techno',\n",
       "  'label',\n",
       "  'electronic',\n",
       "  'disco',\n",
       "  'beats',\n",
       "  'mix',\n",
       "  'remix',\n",
       "  'dub',\n",
       "  'beat',\n",
       "  'bass',\n",
       "  'synths',\n",
       "  'club',\n",
       "  'electro'],\n",
       " ['folk',\n",
       "  'acoustic',\n",
       "  'strings',\n",
       "  'acoustic_guitar',\n",
       "  'piano',\n",
       "  'country',\n",
       "  'gentle',\n",
       "  'violin',\n",
       "  'sings',\n",
       "  'johnson',\n",
       "  'percussion',\n",
       "  'harmonies',\n",
       "  'arrangements',\n",
       "  'singing',\n",
       "  'bird'],\n",
       " ['black',\n",
       "  'sense',\n",
       "  'words',\n",
       "  'light',\n",
       "  'power',\n",
       "  'dark',\n",
       "  'life',\n",
       "  'body',\n",
       "  'human',\n",
       "  'space',\n",
       "  'place',\n",
       "  'dream',\n",
       "  'sings',\n",
       "  'felt',\n",
       "  'moment'],\n",
       " ['moments',\n",
       "  'past',\n",
       "  'opener',\n",
       "  'place',\n",
       "  'sense',\n",
       "  'vocal',\n",
       "  'melodies',\n",
       "  'moment',\n",
       "  'light',\n",
       "  'indie',\n",
       "  'musical',\n",
       "  'half',\n",
       "  'making',\n",
       "  'find',\n",
       "  'words'],\n",
       " ['jazz',\n",
       "  'metal',\n",
       "  'drummer',\n",
       "  'playing',\n",
       "  'group',\n",
       "  'trio',\n",
       "  'bass',\n",
       "  'post',\n",
       "  'guitarist',\n",
       "  'drums',\n",
       "  'bassist',\n",
       "  'musicians',\n",
       "  'riffs',\n",
       "  'members',\n",
       "  'groove'],\n",
       " ['moments',\n",
       "  'melody',\n",
       "  'minutes',\n",
       "  'instrumental',\n",
       "  'drums',\n",
       "  'simple',\n",
       "  'minute',\n",
       "  'bass',\n",
       "  'approach',\n",
       "  'musical',\n",
       "  'point',\n",
       "  'beat',\n",
       "  'piano',\n",
       "  'melodic',\n",
       "  'sense'],\n",
       " ['ep',\n",
       "  'indie',\n",
       "  'production',\n",
       "  'chorus',\n",
       "  'vocal',\n",
       "  'fact',\n",
       "  'big',\n",
       "  'past',\n",
       "  'release',\n",
       "  'point',\n",
       "  'sort',\n",
       "  'half',\n",
       "  'find',\n",
       "  'moments',\n",
       "  'melodies'],\n",
       " ['instrumental',\n",
       "  'minutes',\n",
       "  'moments',\n",
       "  'melody',\n",
       "  'minute',\n",
       "  'group',\n",
       "  'drums',\n",
       "  'point',\n",
       "  'interesting',\n",
       "  'approach',\n",
       "  'musical',\n",
       "  'ep',\n",
       "  'release',\n",
       "  'style',\n",
       "  'bass'],\n",
       " ['piece',\n",
       "  'pieces',\n",
       "  'noise',\n",
       "  'piano',\n",
       "  'drone',\n",
       "  'ambient',\n",
       "  'composer',\n",
       "  'minute',\n",
       "  'electronic',\n",
       "  'minutes',\n",
       "  'space',\n",
       "  'notes',\n",
       "  'drones',\n",
       "  'film',\n",
       "  'works'],\n",
       " ['moments',\n",
       "  'sense',\n",
       "  'place',\n",
       "  'past',\n",
       "  'words',\n",
       "  'light',\n",
       "  'opener',\n",
       "  'moment',\n",
       "  'life',\n",
       "  'musical',\n",
       "  'lost',\n",
       "  'making',\n",
       "  'vocal',\n",
       "  'dark',\n",
       "  'find'],\n",
       " ['ep',\n",
       "  'release',\n",
       "  'point',\n",
       "  'fact',\n",
       "  'production',\n",
       "  'moments',\n",
       "  'group',\n",
       "  'making',\n",
       "  'vocal',\n",
       "  'sort',\n",
       "  'find',\n",
       "  'big',\n",
       "  'half',\n",
       "  'past',\n",
       "  'hard'],\n",
       " ['sings',\n",
       "  'life',\n",
       "  'death',\n",
       "  'words',\n",
       "  'woman',\n",
       "  'story',\n",
       "  'personal',\n",
       "  'singing',\n",
       "  'young',\n",
       "  'country',\n",
       "  'pain',\n",
       "  'relationship',\n",
       "  'god',\n",
       "  'mother',\n",
       "  'emotional'],\n",
       " ['ep',\n",
       "  'moments',\n",
       "  'vocal',\n",
       "  'production',\n",
       "  'melody',\n",
       "  'melodies',\n",
       "  'half',\n",
       "  'point',\n",
       "  'release',\n",
       "  'musical',\n",
       "  'past',\n",
       "  'length',\n",
       "  'sort',\n",
       "  'approach',\n",
       "  'making'],\n",
       " ['indie',\n",
       "  'chorus',\n",
       "  'opener',\n",
       "  'past',\n",
       "  'vocal',\n",
       "  'melodies',\n",
       "  'singer',\n",
       "  'songwriting',\n",
       "  'singing',\n",
       "  'moments',\n",
       "  'big',\n",
       "  'ep',\n",
       "  'production',\n",
       "  'heart',\n",
       "  'half'],\n",
       " ['noise',\n",
       "  'melody',\n",
       "  'percussion',\n",
       "  'drums',\n",
       "  'rhythm',\n",
       "  'electronic',\n",
       "  'drone',\n",
       "  'bass',\n",
       "  'simple',\n",
       "  'tones',\n",
       "  'melodies',\n",
       "  'slowly',\n",
       "  'sonic',\n",
       "  'subtle',\n",
       "  'elements'],\n",
       " ['live',\n",
       "  'disc',\n",
       "  'version',\n",
       "  'set',\n",
       "  'group',\n",
       "  'collection',\n",
       "  'original',\n",
       "  'recorded',\n",
       "  'material',\n",
       "  'release',\n",
       "  'studio',\n",
       "  'compilation',\n",
       "  'early',\n",
       "  'cover',\n",
       "  'released']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get topic words\n",
    "get_topics(model = model, num_topics = 25, top_n_words= 15, vocabulary = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "538aff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic is: 19 and text is ['half', 'manner', 'david_bowie', 'inauspicious', 'kick', 'sentimental', 'start', 'life', 'mars', 'rest', 'performance', 'vh1', 'storytellers', 'series', 'poses', 'question', 'accurately', 'suggests', 'audience', 'privy', 'inside', 'information', 'answers', 'tepid', 'set', 'broadcast', 'august', 'quarter_century', 'hung', 'ziggy_stardust', 'kimonos', 'episode', 'teased', 'fans', 'rare', 'chance_hear', 'shape_shifting', 'discuss', 'deconstruct', 'mythology', 'bowie', 'running', 'fumes', 'disappointing', 'decade--', 'earthling', 'forthcoming', 'hours', 'iconic', 'catalog', 'colorful', 'personal', 'narrative', 'narratives', 'amazing', 'supporting_cast', 'draw', 'stories', 'bowie', 'sadly', 'flashes', 'wit', 'gregarious', 'storytelling', 'cd_dvd', 'release', 'television', 'performance', 'standing', 'center', 'community', 'theater', 'stage', 'set', 'entertaining', 'overly', 'polite', 'audience', 'donning', 'gray', 'hoodie', 'bowie', 'likable', 'cuff', 'introduction', 'china', 'hushed', 'retelling', 'story', 'story--', 'iggy_pop', 'coffee', 'recalling', '1970s', 'berlin', 'punk', 'german', 'artists', 'built', 'destroyed', 'replica', 'berlin', 'wall--', 'wrote', 'jim', 'guess', 'invasion', 'exploitation', 'mike', 'mention', 'insight', 'nile_rodgers', 'nazi', 'reference', 'controversial', 'award_winning', 'video--', 'interesting', 'tale', 'piano', 'intro', 'suited', 'low_budget', 'tv_movie', 'storytelling', 'continues_vein', 'jumping', 'paul', 'anka', 'connection', 'life', 'mars', 'eartha', 'kitt', 'obsession', 'shocking', 'admission', 'mid-70s', 'dark', 'drug_fueled', 'period', 'impervious', 'recall', 'frustrating', 'charming', 'deprecating', 'tossing', 'casual', 'references', 'meeting', 'abbie', 'hoffmann', 'amusing', 'marc_bolan', 'impression', 'suggesting', 'dozens', 'interesting', 'relevant', 'revealing', 'stories', 'felt', 'musically', 'occasionally', 'sliding', 'cheesy', 'cabaret', 'lite', 'renditions', 'casting', 'session_player', 'stereotypes', 'tracks', 'original', 'broadcast', 'bonus_tracks', 'dvd', 'hours', 'fans', 'choice', 'extra', 'tracks', 'crashing', 'car', 'read', 'provide', 'slightly', 'rounded', 'career', 'additional', 'commentary', 'making', 'modest', 'live_performances', 'puzzling', 'dvd', 'cd', 'element', 'release', 'dvd', 'companion', 'cd', 'bother', 'separate', 'storytelling', 'include', 'bonus_tracks', 'makes', 'audio', 'recording', 'television', 'episode', 'major_labels', 'problems', 'venerable', 'archaic', 'media', 'formats', 'working', 'hard', 'invent', 'substance', 'reissue', 'indebted', 'famous', 'adding', 'significantly', 'insight', 'famous', 'intriguing', 'career', 'hard_imagine', 'fan', 'outcry', 'dvd', 'filled', 'decent', 'muddled', 'performances']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlealtru/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    bow = torch.zeros(len(dictionary))\n",
    "    item = list(zip(*x_bows_train[1])) # bow = [[token_id1,token_id2,...],[freq1,freq2,...]]\n",
    "    bow[list(item[0])] = torch.tensor(list(item[1])).float()\n",
    "    topics = model.get_theta(torch.tensor(bow).float().to(device))\n",
    "    print(f'topic is: {torch.argmax(topics[0])} and text is {x_tokens_train[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd087786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0196, 0.0201, 0.0202, 0.0298, 0.5075, 0.0374, 0.0338, 0.0219, 0.0173,\n",
       "         0.0222, 0.0285, 0.0242, 0.0195, 0.0256, 0.0253, 0.0391, 0.0240, 0.0252,\n",
       "         0.0148, 0.0439], device='cuda:0'),\n",
       " tensor(7.2107, device='cuda:0'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#topics\n",
    "npp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff90b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_words(model = model, queries=['pop','cash'], vocabulary = vocab, n_most_similar=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
